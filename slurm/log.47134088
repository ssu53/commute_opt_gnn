JobID: 47134088
======
Time: Fri Mar  1 01:04:56 GMT 2024
Running on master node: cpu-q-44
Loading data...
Number of training graphs: 1500
Train graph sizes: Counter({101: 46, 114: 45, 79: 44, 87: 40, 94: 39, 122: 39, 88: 37, 92: 37, 107: 36, 119: 36, 80: 34, 105: 34, 75: 33, 99: 33, 112: 33, 116: 33, 121: 33, 78: 32, 84: 32, 98: 32, 100: 32, 120: 32, 96: 31, 111: 31, 118: 31, 81: 29, 83: 28, 95: 28, 103: 28, 108: 28, 82: 27, 89: 27, 106: 27, 93: 26, 90: 26, 97: 26, 109: 25, 115: 25, 117: 25, 85: 24, 124: 24, 76: 22, 86: 22, 91: 22, 102: 22, 104: 22, 110: 22, 123: 22, 77: 19, 113: 19})
{25: [], 26: [], 27: [], 28: [], 29: [], 30: [], 31: [], 32: [], 33: [], 34: []}
Number of validation graphs: 1500
Val graph sizes: Counter({159: 12, 168: 11, 172: 11, 130: 9, 164: 9, 174: 9, 126: 8, 142: 8, 141: 8, 149: 8, 125: 7, 129: 7, 133: 7, 134: 7, 139: 7, 154: 7, 153: 7, 160: 7, 128: 6, 135: 6, 137: 6, 136: 6, 143: 6, 146: 6, 147: 6, 152: 6, 150: 6, 158: 6, 157: 6, 165: 6, 138: 5, 144: 5, 148: 5, 145: 5, 156: 5, 163: 5, 162: 5, 166: 5, 132: 4, 151: 4, 161: 4, 169: 4, 167: 4, 171: 4, 173: 4, 131: 3, 140: 3, 127: 2, 170: 2, 155: 1})
{'wandb': {'experiment_name': '', 'project': 'ColourInteract Sweeps Friday', 'entity': 'commute_opt_gnn'}, 'data': {'name': 'ColourInteract', 'dataset': 'LRGB', 'c1': 1.0, 'c2s': [0.01, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 100.0], 'num_colours': 4, 'train_size': 1500, 'val_size': 300, 'min_train_nodes': 75, 'max_train_nodes': 125, 'min_val_nodes': 125, 'max_val_nodes': 175, 'rewirers': ['cayley_clusters', 'cayley'], 'normalise': True, 'seed': 93, 'c2': 0.01}, 'model': {'hidden_channels': 8, 'num_layers': 5, 'drop_prob': 0.1, 'global_pool_aggr': 'global_mean_pool', 'approaches': ['only_original', 'interleave'], 'seeds': [17, 47, 23], 'approach': 'only_original'}, 'train': {'lr': 0.0001, 'num_epochs': 400, 'print_every': 5, 'train_batch_size': 32, 'val_batch_size': 32}, 'run': {'silent': False}}
Device: cpu
No rewiring.
train targets: 3.38 +/- 0.220
val targets: 3.55 +/- 0.193
Rewirer None with seed 17
Using MSE Loss...
Minimum val loss at epoch 305: 0.10284598991274833
Final val loss at epoch 400: 0.5137036152184009
Train / Val loss by epoch
5: 24.915 / 113.326
10: 20.990 / 63.414
15: 14.019 / 45.032
20: 8.885 / 34.646
25: 8.447 / 28.921
30: 7.345 / 23.275
35: 5.003 / 19.100
40: 8.282 / 17.164
45: 4.791 / 12.197
50: 4.916 / 12.575
55: 5.301 / 11.124
60: 9.251 / 10.657
65: 7.509 / 8.341
70: 3.277 / 9.382
75: 4.230 / 8.511
80: 3.364 / 8.204
85: 3.312 / 8.039
90: 2.838 / 6.422
95: 3.050 / 6.263
100: 5.019 / 7.277
105: 2.989 / 5.579
110: 2.907 / 7.220
115: 2.032 / 5.490
120: 2.101 / 5.320
125: 2.168 / 6.339
130: 3.646 / 5.819
135: 2.032 / 5.333
140: 3.945 / 5.650
145: 2.281 / 4.134
150: 1.530 / 5.199
155: 1.930 / 3.977
160: 1.833 / 3.941
165: 2.536 / 4.886
170: 4.005 / 4.888
175: 3.101 / 3.130
180: 1.184 / 3.052
185: 1.217 / 3.132
190: 1.084 / 2.885
195: 1.248 / 2.189
200: 1.274 / 2.238
205: 0.769 / 1.935
210: 0.766 / 1.568
215: 1.013 / 2.167
220: 0.720 / 1.149
225: 0.878 / 1.459
230: 1.454 / 0.987
235: 2.600 / 1.248
240: 0.443 / 0.750
245: 0.419 / 0.529
250: 1.068 / 0.602
255: 0.593 / 0.301
260: 0.899 / 0.476
265: 0.802 / 0.436
270: 0.426 / 0.256
275: 0.706 / 0.259
280: 0.467 / 0.171
285: 1.611 / 0.167
290: 0.668 / 0.126
295: 0.603 / 0.126
300: 0.197 / 0.105
305: 0.779 / 0.103
310: 0.360 / 0.113
315: 1.020 / 0.107
320: 0.353 / 0.137
325: 0.252 / 0.199
330: 0.329 / 0.202
335: 0.188 / 0.251
340: 0.685 / 0.172
345: 0.191 / 0.270
350: 0.084 / 0.274
355: 0.237 / 0.231
360: 0.104 / 0.319
365: 0.177 / 0.276
370: 0.080 / 0.354
375: 0.068 / 0.358
380: 0.097 / 0.411
385: 0.101 / 0.364
390: 0.053 / 0.473
395: 0.109 / 0.394
400: 0.091 / 0.514
Rewirer None with seed 47
Using MSE Loss...
Minimum val loss at epoch 320: 0.13997287228703498
Final val loss at epoch 400: 0.44113091509789226
Train / Val loss by epoch
5: 16.853 / 13.909
10: 18.310 / 16.099
15: 14.074 / 17.801
20: 8.565 / 17.120
25: 9.751 / 20.042
30: 7.109 / 16.099
35: 8.158 / 19.788
40: 11.056 / 19.545
45: 6.624 / 20.068
50: 8.811 / 18.832
55: 8.131 / 19.481
60: 6.436 / 20.076
65: 5.177 / 21.237
70: 6.831 / 16.395
75: 4.381 / 18.780
80: 3.255 / 17.070
85: 3.762 / 17.108
90: 2.975 / 17.868
95: 1.999 / 15.733
100: 1.843 / 13.930
105: 3.542 / 12.297
110: 3.260 / 14.016
115: 1.991 / 13.397
120: 2.085 / 11.942
125: 1.896 / 10.356
130: 2.060 / 9.318
135: 1.983 / 9.688
140: 1.561 / 7.907
145: 1.910 / 8.409
150: 1.878 / 8.536
155: 1.078 / 7.021
160: 1.814 / 7.398
165: 0.759 / 6.702
170: 0.967 / 4.882
175: 0.822 / 4.852
180: 0.769 / 4.106
185: 0.843 / 4.074
190: 1.002 / 3.504
195: 1.702 / 2.457
200: 1.115 / 2.227
205: 0.671 / 2.563
210: 1.887 / 2.893
215: 0.977 / 2.134
220: 0.737 / 1.768
225: 0.533 / 1.060
230: 0.690 / 1.500
235: 0.904 / 0.871
240: 0.559 / 0.912
245: 0.517 / 0.656
250: 0.309 / 0.500
255: 0.404 / 0.532
260: 0.932 / 0.474
265: 0.473 / 0.346
270: 0.319 / 0.341
275: 0.435 / 0.297
280: 0.203 / 0.298
285: 0.226 / 0.197
290: 0.644 / 0.309
295: 0.530 / 0.179
300: 0.391 / 0.146
305: 0.311 / 0.174
310: 0.356 / 0.140
315: 0.269 / 0.145
320: 0.326 / 0.140
325: 0.426 / 0.169
330: 0.260 / 0.145
335: 0.128 / 0.172
340: 0.288 / 0.187
345: 0.179 / 0.153
350: 0.171 / 0.189
355: 0.195 / 0.218
360: 0.226 / 0.217
365: 0.111 / 0.213
370: 0.295 / 0.314
375: 0.394 / 0.268
380: 0.165 / 0.357
385: 0.244 / 0.339
390: 0.147 / 0.331
395: 0.113 / 0.396
400: 0.178 / 0.441
Rewirer None with seed 23
Using MSE Loss...
Minimum val loss at epoch 395: 0.8187058568000793
Final val loss at epoch 400: 0.848777225613594
Train / Val loss by epoch
5: 33.587 / 22.471
10: 18.569 / 14.201
15: 15.806 / 12.595
20: 23.907 / 11.456
25: 12.197 / 9.854
30: 6.990 / 9.039
35: 6.942 / 8.625
40: 7.114 / 8.893
45: 5.616 / 10.115
50: 6.573 / 9.618
55: 6.460 / 11.104
60: 4.090 / 13.181
65: 6.187 / 12.160
70: 4.044 / 14.336
75: 4.698 / 12.948
80: 5.108 / 12.458
85: 1.709 / 11.119
90: 3.843 / 10.884
95: 2.086 / 10.753
100: 3.479 / 10.345
105: 2.938 / 10.267
110: 3.129 / 8.825
115: 2.144 / 10.286
120: 3.561 / 8.857
125: 1.893 / 7.644
130: 1.703 / 7.436
135: 1.467 / 6.329
140: 1.589 / 6.292
145: 1.398 / 5.317
150: 1.699 / 4.882
155: 2.012 / 3.825
160: 2.175 / 3.238
165: 1.496 / 4.133
170: 0.671 / 3.899
175: 1.333 / 2.882
180: 1.766 / 2.394
185: 1.153 / 1.990
190: 2.458 / 2.179
195: 0.739 / 2.161
200: 0.855 / 2.297
205: 0.561 / 2.189
210: 0.619 / 2.038
215: 1.977 / 1.525
220: 0.712 / 1.364
225: 1.206 / 1.682
230: 0.366 / 1.916
235: 0.362 / 1.582
240: 0.506 / 1.305
245: 0.651 / 1.281
250: 0.746 / 1.186
255: 0.673 / 1.001
260: 0.454 / 1.156
265: 1.048 / 1.170
270: 0.996 / 1.066
275: 0.419 / 1.177
280: 0.232 / 1.296
285: 0.207 / 1.236
290: 0.812 / 1.023
295: 0.474 / 1.003
300: 0.091 / 0.985
305: 0.458 / 0.952
310: 0.169 / 1.102
315: 0.306 / 0.907
320: 0.357 / 1.363
325: 0.158 / 1.054
330: 0.223 / 1.219
335: 0.079 / 1.169
340: 0.436 / 1.006
345: 0.125 / 1.021
350: 0.198 / 1.019
355: 0.123 / 0.981
360: 0.062 / 1.010
365: 0.091 / 1.016
370: 0.128 / 0.979
375: 0.091 / 0.983
380: 0.151 / 0.988
385: 0.211 / 0.959
390: 0.170 / 0.970
395: 0.051 / 0.819
400: 0.048 / 0.849
{None: [{'best': 0.10284598991274833, 'end': 0.5137036152184009},
        {'best': 0.13997287228703498, 'end': 0.44113091509789226},
        {'best': 0.8187058568000793, 'end': 0.848777225613594}]}
{'wandb': {'experiment_name': '', 'project': 'ColourInteract Sweeps Friday', 'entity': 'commute_opt_gnn'}, 'data': {'name': 'ColourInteract', 'dataset': 'LRGB', 'c1': 1.0, 'c2s': [0.01, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 100.0], 'num_colours': 4, 'train_size': 1500, 'val_size': 300, 'min_train_nodes': 75, 'max_train_nodes': 125, 'min_val_nodes': 125, 'max_val_nodes': 175, 'rewirers': ['cayley_clusters', 'cayley'], 'normalise': True, 'seed': 93, 'c2': 0.01}, 'model': {'hidden_channels': 8, 'num_layers': 5, 'drop_prob': 0.1, 'global_pool_aggr': 'global_mean_pool', 'approaches': ['only_original', 'interleave'], 'seeds': [17, 47, 23], 'approach': 'interleave', 'seed': 23}, 'train': {'lr': 0.0001, 'num_epochs': 400, 'print_every': 5, 'train_batch_size': 32, 'val_batch_size': 32}, 'run': {'silent': False}}
Device: cpu
train targets: 3.38 +/- 0.220
val targets: 3.55 +/- 0.193
Rewirer cayley_clusters with seed 17
Using MSE Loss...
Minimum val loss at epoch 275: 0.19051432758569717
Final val loss at epoch 400: 0.7021546296775341
Train / Val loss by epoch
5: 32.116 / 88.803
10: 28.191 / 52.700
15: 13.819 / 44.888
20: 15.368 / 35.597
25: 6.546 / 29.172
30: 8.039 / 25.199
35: 4.509 / 19.880
40: 8.865 / 20.724
45: 6.645 / 14.869
50: 8.268 / 13.291
55: 8.053 / 13.806
60: 7.304 / 13.105
65: 4.678 / 11.424
70: 4.006 / 11.773
75: 5.422 / 10.073
80: 4.369 / 9.216
85: 4.573 / 8.530
90: 3.035 / 7.516
95: 2.336 / 6.488
100: 3.144 / 6.333
105: 3.610 / 4.927
110: 1.591 / 4.618
115: 2.154 / 3.927
120: 1.301 / 4.025
125: 3.128 / 3.999
130: 3.384 / 3.283
135: 1.348 / 2.799
140: 2.271 / 2.741
145: 2.080 / 2.541
150: 0.950 / 2.867
155: 2.460 / 1.764
160: 2.549 / 1.877
165: 0.764 / 1.870
170: 3.347 / 2.294
175: 2.154 / 1.016
180: 1.119 / 1.562
185: 0.776 / 0.958
190: 0.860 / 0.874
195: 1.442 / 1.165
200: 1.128 / 0.779
205: 0.725 / 0.551
210: 0.751 / 0.564
215: 0.602 / 0.638
220: 1.003 / 0.536
225: 0.833 / 0.397
230: 1.506 / 0.429
235: 1.296 / 0.562
240: 0.767 / 0.283
245: 0.492 / 0.251
250: 0.681 / 0.275
255: 0.659 / 0.220
260: 0.799 / 0.215
265: 0.573 / 0.205
270: 1.284 / 0.203
275: 0.546 / 0.191
280: 0.831 / 0.217
285: 0.781 / 0.282
290: 0.797 / 0.226
295: 0.531 / 0.213
300: 0.295 / 0.293
305: 0.471 / 0.232
310: 0.357 / 0.291
315: 0.732 / 0.286
320: 0.369 / 0.303
325: 0.189 / 0.406
330: 0.251 / 0.423
335: 0.501 / 0.381
340: 0.643 / 0.415
345: 0.444 / 0.426
350: 0.122 / 0.465
355: 0.353 / 0.390
360: 0.185 / 0.442
365: 0.155 / 0.562
370: 0.073 / 0.630
375: 0.106 / 0.518
380: 0.091 / 0.600
385: 0.111 / 0.547
390: 0.079 / 0.605
395: 0.081 / 0.648
400: 0.096 / 0.702
Rewirer cayley_clusters with seed 47
Using MSE Loss...
Minimum val loss at epoch 345: 0.16260807886719703
Final val loss at epoch 400: 0.3414535790681839
Train / Val loss by epoch
5: 12.228 / 5.970
10: 24.107 / 5.038
15: 15.247 / 4.850
20: 14.204 / 4.501
25: 10.130 / 6.045
30: 7.873 / 4.025
35: 9.398 / 6.585
40: 7.245 / 5.827
45: 6.840 / 6.739
50: 11.150 / 6.923
55: 5.144 / 8.598
60: 7.455 / 8.978
65: 6.913 / 10.555
70: 6.034 / 7.949
75: 3.447 / 8.488
80: 4.006 / 8.509
85: 2.032 / 9.650
90: 3.673 / 9.761
95: 2.265 / 8.294
100: 2.987 / 7.974
105: 3.272 / 6.458
110: 2.328 / 7.622
115: 1.782 / 7.236
120: 1.635 / 7.220
125: 1.977 / 6.328
130: 2.115 / 5.347
135: 1.931 / 5.068
140: 1.070 / 3.840
145: 2.000 / 4.306
150: 2.067 / 4.186
155: 1.435 / 3.498
160: 1.694 / 3.126
165: 1.270 / 3.359
170: 0.934 / 2.334
175: 1.182 / 1.999
180: 1.113 / 1.806
185: 0.931 / 1.736
190: 0.971 / 1.730
195: 1.061 / 1.162
200: 0.678 / 1.326
205: 0.812 / 1.140
210: 1.088 / 1.434
215: 0.455 / 1.089
220: 0.756 / 1.114
225: 0.744 / 0.707
230: 0.762 / 1.044
235: 0.677 / 0.803
240: 0.574 / 0.499
245: 0.468 / 0.499
250: 0.387 / 0.426
255: 0.571 / 0.611
260: 0.599 / 0.432
265: 0.320 / 0.429
270: 0.441 / 0.300
275: 0.337 / 0.341
280: 0.351 / 0.364
285: 0.215 / 0.275
290: 0.527 / 0.299
295: 0.401 / 0.261
300: 0.232 / 0.213
305: 0.252 / 0.248
310: 0.353 / 0.200
315: 0.431 / 0.201
320: 0.169 / 0.164
325: 0.269 / 0.175
330: 0.520 / 0.165
335: 0.229 / 0.167
340: 0.216 / 0.164
345: 0.276 / 0.163
350: 0.218 / 0.166
355: 0.229 / 0.172
360: 0.451 / 0.185
365: 0.283 / 0.163
370: 0.299 / 0.207
375: 0.369 / 0.202
380: 0.152 / 0.260
385: 0.165 / 0.206
390: 0.104 / 0.260
395: 0.091 / 0.294
400: 0.099 / 0.341
Rewirer cayley_clusters with seed 23
Using MSE Loss...
Minimum val loss at epoch 315: 1.1835321471095086
Final val loss at epoch 400: 1.2655536592006684
Train / Val loss by epoch
5: 25.380 / 27.434
10: 28.862 / 18.674
15: 18.315 / 17.739
20: 17.784 / 16.250
25: 8.612 / 14.571
30: 10.145 / 13.555
35: 5.970 / 12.166
40: 7.176 / 11.585
45: 5.166 / 13.756
50: 9.881 / 11.583
55: 3.639 / 12.645
60: 4.957 / 13.933
65: 6.797 / 12.845
70: 4.585 / 16.066
75: 4.406 / 13.326
80: 4.651 / 13.015
85: 2.668 / 10.424
90: 3.443 / 9.457
95: 1.565 / 9.727
100: 3.986 / 8.786
105: 3.939 / 7.575
110: 2.492 / 6.433
115: 1.443 / 7.663
120: 3.322 / 6.362
125: 2.730 / 4.577
130: 1.542 / 4.619
135: 3.198 / 4.493
140: 1.368 / 5.040
145: 1.091 / 3.529
150: 1.468 / 3.048
155: 2.542 / 2.953
160: 3.370 / 2.351
165: 1.322 / 3.618
170: 0.809 / 3.024
175: 1.152 / 2.980
180: 1.172 / 2.593
185: 0.639 / 2.236
190: 3.218 / 2.026
195: 1.049 / 2.171
200: 0.867 / 2.517
205: 0.729 / 3.076
210: 0.736 / 2.548
215: 1.492 / 2.676
220: 0.692 / 2.182
225: 1.232 / 2.350
230: 1.887 / 2.605
235: 1.119 / 2.113
240: 1.180 / 1.992
245: 0.777 / 2.044
250: 1.394 / 1.769
255: 1.722 / 1.718
260: 0.423 / 1.839
265: 1.130 / 1.663
270: 1.316 / 1.750
275: 0.589 / 1.758
280: 0.234 / 2.140
285: 0.163 / 1.850
290: 1.141 / 1.300
295: 0.283 / 1.795
300: 0.212 / 1.664
305: 0.744 / 1.331
310: 0.377 / 1.390
315: 0.267 / 1.184
320: 0.231 / 1.460
325: 0.372 / 1.263
330: 0.173 / 1.487
335: 0.137 / 1.416
340: 0.249 / 1.338
345: 0.140 / 1.544
350: 0.265 / 1.419
355: 0.111 / 1.486
360: 0.099 / 1.472
365: 0.084 / 1.426
370: 0.103 / 1.433
375: 0.084 / 1.352
380: 0.095 / 1.334
385: 0.058 / 1.245
390: 0.083 / 1.233
395: 0.053 / 1.225
400: 0.045 / 1.266
Rewirer cayley with seed 17
Using MSE Loss...
Minimum val loss at epoch 295: 0.126902362331748
Final val loss at epoch 400: 0.449420739710331
Train / Val loss by epoch
5: 25.805 / 180.514
10: 21.462 / 102.335
15: 17.003 / 73.481
20: 13.277 / 56.269
25: 8.501 / 45.259
30: 10.980 / 37.211
35: 7.145 / 31.362
40: 7.114 / 29.191
45: 6.588 / 22.389
50: 7.098 / 22.070
55: 4.997 / 19.883
60: 8.794 / 17.211
65: 7.163 / 14.832
70: 4.831 / 14.142
75: 3.753 / 12.220
80: 3.124 / 11.617
85: 4.154 / 10.275
90: 4.228 / 8.190
95: 4.581 / 8.145
100: 3.160 / 7.999
105: 2.576 / 6.239
110: 2.848 / 6.772
115: 2.422 / 5.021
120: 2.407 / 4.460
125: 3.391 / 4.886
130: 4.826 / 3.872
135: 1.681 / 3.699
140: 2.949 / 2.889
145: 1.525 / 2.526
150: 1.330 / 3.236
155: 3.188 / 2.143
160: 2.533 / 2.180
165: 0.723 / 2.552
170: 2.622 / 2.495
175: 2.076 / 1.522
180: 0.660 / 1.687
185: 0.904 / 1.490
190: 0.702 / 1.319
195: 1.477 / 1.573
200: 1.416 / 0.971
205: 1.319 / 0.902
210: 0.969 / 0.709
215: 0.736 / 1.111
220: 0.710 / 0.640
225: 0.683 / 0.726
230: 2.147 / 0.579
235: 1.745 / 0.544
240: 0.817 / 0.259
245: 0.703 / 0.238
250: 0.800 / 0.241
255: 0.586 / 0.211
260: 0.863 / 0.190
265: 0.505 / 0.159
270: 1.063 / 0.176
275: 0.762 / 0.142
280: 0.654 / 0.135
285: 1.941 / 0.133
290: 0.741 / 0.134
295: 0.520 / 0.127
300: 0.498 / 0.159
305: 0.914 / 0.179
310: 0.306 / 0.177
315: 0.828 / 0.242
320: 0.183 / 0.274
325: 0.245 / 0.292
330: 0.278 / 0.254
335: 0.416 / 0.194
340: 0.503 / 0.237
345: 0.619 / 0.221
350: 0.252 / 0.329
355: 0.546 / 0.275
360: 0.315 / 0.323
365: 0.112 / 0.326
370: 0.126 / 0.484
375: 0.101 / 0.376
380: 0.087 / 0.406
385: 0.286 / 0.259
390: 0.101 / 0.394
395: 0.068 / 0.349
400: 0.062 / 0.449
Rewirer cayley with seed 47
Using MSE Loss...
Minimum val loss at epoch 350: 0.11893371418118477
Final val loss at epoch 400: 0.24906593449413778
Train / Val loss by epoch
5: 20.882 / 56.322
10: 18.854 / 59.150
15: 17.104 / 58.068
20: 11.601 / 51.866
25: 9.133 / 50.138
30: 7.819 / 42.443
35: 8.085 / 41.966
40: 10.285 / 34.591
45: 8.138 / 33.561
50: 9.657 / 31.210
55: 8.600 / 29.486
60: 7.120 / 26.857
65: 4.894 / 27.276
70: 6.790 / 21.518
75: 4.799 / 20.728
80: 4.659 / 18.159
85: 5.105 / 20.071
90: 4.209 / 18.609
95: 3.804 / 17.607
100: 4.099 / 16.937
105: 4.176 / 14.417
110: 3.107 / 15.621
115: 1.461 / 14.032
120: 1.493 / 12.802
125: 2.017 / 11.367
130: 2.924 / 9.822
135: 1.947 / 10.288
140: 2.049 / 7.785
145: 2.403 / 8.646
150: 1.330 / 8.235
155: 1.017 / 6.879
160: 1.584 / 6.611
165: 0.985 / 7.092
170: 0.754 / 4.783
175: 0.585 / 3.844
180: 1.254 / 3.747
185: 1.428 / 3.746
190: 0.764 / 3.504
195: 1.683 / 2.601
200: 0.973 / 2.637
205: 0.641 / 3.171
210: 0.774 / 2.810
215: 0.657 / 2.094
220: 0.678 / 2.008
225: 0.604 / 1.481
230: 0.746 / 1.678
235: 0.905 / 1.377
240: 0.638 / 1.114
245: 0.575 / 0.944
250: 0.481 / 0.837
255: 0.780 / 0.810
260: 0.832 / 0.721
265: 0.366 / 0.662
270: 0.665 / 0.593
275: 0.535 / 0.381
280: 0.254 / 0.564
285: 0.342 / 0.291
290: 0.345 / 0.417
295: 0.378 / 0.353
300: 0.279 / 0.271
305: 0.274 / 0.247
310: 0.252 / 0.235
315: 0.281 / 0.199
320: 0.322 / 0.169
325: 0.227 / 0.164
330: 0.413 / 0.138
335: 0.241 / 0.126
340: 0.240 / 0.122
345: 0.200 / 0.122
350: 0.286 / 0.119
355: 0.236 / 0.124
360: 0.196 / 0.125
365: 0.233 / 0.132
370: 0.212 / 0.147
375: 0.145 / 0.139
380: 0.168 / 0.193
385: 0.207 / 0.145
390: 0.140 / 0.151
395: 0.111 / 0.243
400: 0.229 / 0.249
Rewirer cayley with seed 23
Using MSE Loss...
Minimum val loss at epoch 325: 1.3164148420095443
Final val loss at epoch 400: 1.402043554186821
Train / Val loss by epoch
5: 27.446 / 18.920
10: 24.424 / 9.554
15: 16.558 / 6.903
20: 19.786 / 6.141
25: 6.533 / 6.017
30: 7.872 / 5.749
35: 8.455 / 5.131
40: 7.420 / 5.333
45: 9.058 / 7.095
50: 7.490 / 5.253
55: 4.745 / 7.223
60: 3.867 / 7.372
65: 5.312 / 7.436
70: 6.171 / 9.985
75: 4.530 / 7.425
80: 5.429 / 7.889
85: 1.977 / 5.403
90: 2.902 / 5.791
95: 1.523 / 5.514
100: 3.303 / 4.805
105: 2.915 / 5.048
110: 2.217 / 4.399
115: 1.413 / 4.686
120: 3.071 / 4.338
125: 3.490 / 3.126
130: 1.560 / 3.261
135: 3.446 / 2.875
140: 1.034 / 3.304
145: 0.995 / 3.019
150: 1.840 / 2.930
155: 2.684 / 2.183
160: 1.973 / 2.153
165: 2.074 / 2.783
170: 1.152 / 2.725
175: 1.114 / 2.603
180: 1.115 / 2.119
185: 0.741 / 2.123
190: 2.097 / 1.796
195: 0.495 / 1.960
200: 1.167 / 2.442
205: 0.448 / 2.768
210: 0.662 / 2.727
215: 1.274 / 2.044
220: 0.440 / 1.907
225: 1.556 / 2.293
230: 1.199 / 2.430
235: 1.046 / 1.959
240: 0.687 / 1.661
245: 1.083 / 1.691
250: 1.155 / 1.659
255: 0.507 / 1.811
260: 0.342 / 1.826
265: 0.912 / 1.666
270: 0.889 / 1.851
275: 0.449 / 1.929
280: 0.148 / 2.089
285: 0.104 / 1.936
290: 0.506 / 1.544
295: 0.473 / 1.683
300: 0.268 / 1.709
305: 0.697 / 1.434
310: 0.206 / 1.634
315: 0.179 / 1.428
320: 0.172 / 1.673
325: 0.389 / 1.316
330: 0.235 / 1.696
335: 0.167 / 1.586
340: 0.345 / 1.639
345: 0.332 / 1.727
350: 0.156 / 1.741
355: 0.079 / 1.746
360: 0.074 / 1.796
365: 0.113 / 1.614
370: 0.091 / 1.612
375: 0.069 / 1.666
380: 0.062 / 1.594
385: 0.104 / 1.589
390: 0.110 / 1.591
395: 0.031 / 1.465
400: 0.043 / 1.402
{'cayley': [{'best': 0.126902362331748, 'end': 0.449420739710331},
            {'best': 0.11893371418118477, 'end': 0.24906593449413778},
            {'best': 1.3164148420095443, 'end': 1.402043554186821}],
 'cayley_clusters': [{'best': 0.19051432758569717, 'end': 0.7021546296775341},
                     {'best': 0.16260807886719703, 'end': 0.3414535790681839},
                     {'best': 1.1835321471095086, 'end': 1.2655536592006684}]}
Loading data...
Number of training graphs: 1500
Train graph sizes: Counter({101: 46, 114: 45, 79: 44, 87: 40, 94: 39, 122: 39, 88: 37, 92: 37, 107: 36, 119: 36, 80: 34, 105: 34, 75: 33, 99: 33, 112: 33, 116: 33, 121: 33, 78: 32, 84: 32, 98: 32, 100: 32, 120: 32, 96: 31, 111: 31, 118: 31, 81: 29, 83: 28, 95: 28, 103: 28, 108: 28, 82: 27, 89: 27, 106: 27, 93: 26, 90: 26, 97: 26, 109: 25, 115: 25, 117: 25, 85: 24, 124: 24, 76: 22, 86: 22, 91: 22, 102: 22, 104: 22, 110: 22, 123: 22, 77: 19, 113: 19})
{25: [], 26: [], 27: [], 28: [], 29: [], 30: [], 31: [], 32: [], 33: [], 34: []}
Number of validation graphs: 1500
Val graph sizes: Counter({159: 12, 168: 11, 172: 11, 130: 9, 164: 9, 174: 9, 126: 8, 142: 8, 141: 8, 149: 8, 125: 7, 129: 7, 133: 7, 134: 7, 139: 7, 154: 7, 153: 7, 160: 7, 128: 6, 135: 6, 137: 6, 136: 6, 143: 6, 146: 6, 147: 6, 152: 6, 150: 6, 158: 6, 157: 6, 165: 6, 138: 5, 144: 5, 148: 5, 145: 5, 156: 5, 163: 5, 162: 5, 166: 5, 132: 4, 151: 4, 161: 4, 169: 4, 167: 4, 171: 4, 173: 4, 131: 3, 140: 3, 127: 2, 170: 2, 155: 1})
{'wandb': {'experiment_name': '', 'project': 'ColourInteract Sweeps Friday', 'entity': 'commute_opt_gnn'}, 'data': {'name': 'ColourInteract', 'dataset': 'LRGB', 'c1': 1.0, 'c2s': [0.01, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 100.0], 'num_colours': 4, 'train_size': 1500, 'val_size': 300, 'min_train_nodes': 75, 'max_train_nodes': 125, 'min_val_nodes': 125, 'max_val_nodes': 175, 'rewirers': ['cayley_clusters', 'cayley'], 'normalise': True, 'seed': 93, 'c2': 0.1}, 'model': {'hidden_channels': 8, 'num_layers': 5, 'drop_prob': 0.1, 'global_pool_aggr': 'global_mean_pool', 'approaches': ['only_original', 'interleave'], 'seeds': [17, 47, 23], 'approach': 'only_original', 'seed': 23}, 'train': {'lr': 0.0001, 'num_epochs': 400, 'print_every': 5, 'train_batch_size': 32, 'val_batch_size': 32}, 'run': {'silent': False}}
Device: cpu
No rewiring.
train targets: 6.66 +/- 0.654
val targets: 8.49 +/- 0.669
Rewirer None with seed 17
Using MSE Loss...
Minimum val loss at epoch 395: 0.11162590626627207
Final val loss at epoch 400: 0.13380964435636997
Train / Val loss by epoch
5: 27.945 / 140.801
10: 21.589 / 70.713
15: 14.315 / 50.690
20: 9.159 / 39.504
25: 8.734 / 32.948
30: 7.656 / 26.471
35: 5.097 / 22.006
40: 8.341 / 19.979
45: 5.014 / 14.651
50: 5.102 / 15.031
55: 5.221 / 13.248
60: 9.454 / 12.654
65: 7.589 / 10.431
70: 3.418 / 11.316
75: 4.264 / 10.534
80: 3.591 / 10.160
85: 3.262 / 10.137
90: 2.821 / 8.466
95: 2.971 / 8.403
100: 5.163 / 9.185
105: 3.389 / 7.316
110: 2.797 / 9.212
115: 2.194 / 7.179
120: 2.481 / 6.765
125: 2.482 / 8.142
130: 3.768 / 7.284
135: 2.057 / 6.697
140: 3.950 / 6.893
145: 2.393 / 5.397
150: 1.512 / 6.697
155: 1.877 / 5.275
160: 2.061 / 5.161
165: 2.483 / 6.229
170: 4.074 / 6.395
175: 3.225 / 4.061
180: 1.344 / 4.061
185: 1.297 / 4.242
190: 1.111 / 3.865
195: 1.259 / 3.123
200: 1.414 / 2.970
205: 0.924 / 2.843
210: 0.870 / 2.300
215: 1.117 / 3.083
220: 0.821 / 1.854
225: 0.846 / 2.203
230: 1.621 / 1.643
235: 2.685 / 1.904
240: 0.624 / 1.275
245: 0.473 / 1.011
250: 1.228 / 1.099
255: 0.602 / 0.652
260: 0.970 / 0.944
265: 0.641 / 0.879
270: 0.514 / 0.650
275: 0.712 / 0.700
280: 0.508 / 0.483
285: 1.396 / 0.454
290: 0.735 / 0.365
295: 0.509 / 0.403
300: 0.311 / 0.362
305: 0.911 / 0.360
310: 0.342 / 0.262
315: 0.835 / 0.308
320: 0.244 / 0.185
325: 0.205 / 0.162
330: 0.416 / 0.161
335: 0.237 / 0.142
340: 0.602 / 0.173
345: 0.190 / 0.134
350: 0.109 / 0.133
355: 0.258 / 0.152
360: 0.197 / 0.126
365: 0.214 / 0.135
370: 0.140 / 0.120
375: 0.121 / 0.118
380: 0.121 / 0.114
385: 0.159 / 0.115
390: 0.097 / 0.118
395: 0.131 / 0.112
400: 0.295 / 0.134
Rewirer None with seed 47
Using MSE Loss...
Minimum val loss at epoch 345: 0.21226273477077484
Final val loss at epoch 400: 0.34035507291555406
Train / Val loss by epoch
5: 16.718 / 16.326
10: 17.888 / 19.160
15: 13.728 / 21.377
20: 8.422 / 21.033
25: 9.351 / 24.391
30: 6.555 / 19.732
35: 7.764 / 23.558
40: 10.952 / 22.922
45: 6.352 / 23.655
50: 8.487 / 22.588
55: 7.279 / 22.892
60: 6.340 / 23.550
65: 4.943 / 24.244
70: 6.262 / 19.594
75: 4.019 / 21.304
80: 3.288 / 19.805
85: 3.814 / 19.471
90: 2.834 / 19.810
95: 1.939 / 17.692
100: 1.863 / 15.795
105: 3.608 / 13.964
110: 3.017 / 14.920
115: 1.791 / 14.508
120: 1.916 / 12.711
125: 1.875 / 10.625
130: 1.905 / 9.975
135: 1.877 / 10.046
140: 1.478 / 8.494
145: 1.770 / 8.995
150: 1.679 / 8.933
155: 1.233 / 8.013
160: 1.769 / 7.853
165: 0.819 / 7.132
170: 0.800 / 5.357
175: 0.743 / 5.178
180: 0.703 / 4.519
185: 0.927 / 4.586
190: 1.137 / 4.017
195: 1.356 / 2.802
200: 1.113 / 2.720
205: 0.845 / 3.063
210: 1.885 / 3.215
215: 0.846 / 2.533
220: 0.873 / 2.295
225: 0.500 / 1.552
230: 0.682 / 2.045
235: 0.774 / 1.366
240: 0.645 / 1.308
245: 0.493 / 1.029
250: 0.241 / 0.946
255: 0.482 / 0.973
260: 0.972 / 0.883
265: 0.412 / 0.701
270: 0.368 / 0.644
275: 0.481 / 0.601
280: 0.291 / 0.640
285: 0.255 / 0.474
290: 0.823 / 0.607
295: 0.550 / 0.390
300: 0.445 / 0.300
305: 0.299 / 0.375
310: 0.292 / 0.267
315: 0.301 / 0.293
320: 0.367 / 0.244
325: 0.324 / 0.222
330: 0.315 / 0.237
335: 0.133 / 0.216
340: 0.293 / 0.217
345: 0.154 / 0.212
350: 0.185 / 0.220
355: 0.252 / 0.231
360: 0.330 / 0.244
365: 0.171 / 0.227
370: 0.292 / 0.277
375: 0.339 / 0.258
380: 0.219 / 0.287
385: 0.225 / 0.282
390: 0.150 / 0.271
395: 0.143 / 0.333
400: 0.237 / 0.340
Rewirer None with seed 23
Using MSE Loss...
Minimum val loss at epoch 305: 1.0087224140763282
Final val loss at epoch 400: 1.443769133090973
Train / Val loss by epoch
5: 32.425 / 19.942
10: 18.419 / 11.795
15: 14.866 / 10.273
20: 23.744 / 9.080
25: 11.824 / 7.679
30: 6.805 / 7.405
35: 6.915 / 7.037
40: 6.717 / 7.316
45: 5.298 / 8.466
50: 6.589 / 7.947
55: 6.152 / 9.238
60: 3.916 / 10.914
65: 5.727 / 9.968
70: 4.092 / 11.903
75: 4.545 / 10.767
80: 4.731 / 10.286
85: 1.725 / 9.287
90: 3.432 / 8.746
95: 1.945 / 8.496
100: 3.655 / 7.700
105: 2.875 / 7.857
110: 2.947 / 6.634
115: 1.796 / 7.762
120: 3.391 / 6.509
125: 1.565 / 5.658
130: 1.724 / 5.487
135: 1.439 / 4.643
140: 1.702 / 4.775
145: 1.414 / 4.110
150: 1.795 / 3.643
155: 2.265 / 2.491
160: 2.321 / 2.438
165: 1.874 / 2.883
170: 0.486 / 2.846
175: 1.375 / 2.101
180: 2.042 / 1.847
185: 0.928 / 1.589
190: 2.974 / 1.646
195: 0.971 / 1.868
200: 0.984 / 2.011
205: 0.737 / 1.857
210: 0.667 / 1.910
215: 1.876 / 1.381
220: 0.827 / 1.341
225: 1.211 / 1.473
230: 0.738 / 1.811
235: 0.340 / 1.687
240: 0.508 / 1.465
245: 0.678 / 1.274
250: 0.825 / 1.259
255: 0.648 / 1.193
260: 0.542 / 1.375
265: 1.496 / 1.346
270: 1.038 / 1.120
275: 0.571 / 1.337
280: 0.289 / 1.532
285: 0.272 / 1.361
290: 0.914 / 1.203
295: 0.414 / 1.322
300: 0.268 / 1.266
305: 0.534 / 1.009
310: 0.188 / 1.398
315: 0.285 / 1.067
320: 0.239 / 1.646
325: 0.337 / 1.304
330: 0.505 / 1.356
335: 0.176 / 1.522
340: 0.465 / 1.305
345: 0.232 / 1.407
350: 0.213 / 1.282
355: 0.209 / 1.313
360: 0.182 / 1.362
365: 0.259 / 1.404
370: 0.155 / 1.460
375: 0.179 / 1.314
380: 0.420 / 1.328
385: 0.288 / 1.437
390: 0.319 / 1.554
395: 0.096 / 1.299
400: 0.182 / 1.444
{None: [{'best': 0.11162590626627207, 'end': 0.13380964435636997},
        {'best': 0.21226273477077484, 'end': 0.34035507291555406},
        {'best': 1.0087224140763282, 'end': 1.443769133090973}]}
{'wandb': {'experiment_name': '', 'project': 'ColourInteract Sweeps Friday', 'entity': 'commute_opt_gnn'}, 'data': {'name': 'ColourInteract', 'dataset': 'LRGB', 'c1': 1.0, 'c2s': [0.01, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 100.0], 'num_colours': 4, 'train_size': 1500, 'val_size': 300, 'min_train_nodes': 75, 'max_train_nodes': 125, 'min_val_nodes': 125, 'max_val_nodes': 175, 'rewirers': ['cayley_clusters', 'cayley'], 'normalise': True, 'seed': 93, 'c2': 0.1}, 'model': {'hidden_channels': 8, 'num_layers': 5, 'drop_prob': 0.1, 'global_pool_aggr': 'global_mean_pool', 'approaches': ['only_original', 'interleave'], 'seeds': [17, 47, 23], 'approach': 'interleave', 'seed': 23}, 'train': {'lr': 0.0001, 'num_epochs': 400, 'print_every': 5, 'train_batch_size': 32, 'val_batch_size': 32}, 'run': {'silent': False}}
Device: cpu
train targets: 6.66 +/- 0.654
val targets: 8.49 +/- 0.669
Rewirer cayley_clusters with seed 17
Using MSE Loss...
Minimum val loss at epoch 305: 0.1683390349149704
Final val loss at epoch 400: 0.31281586512923243
Train / Val loss by epoch
5: 34.381 / 111.445
10: 29.432 / 57.544
15: 14.129 / 49.044
20: 15.587 / 39.142
25: 6.703 / 32.238
30: 8.403 / 27.813
35: 4.698 / 22.186
40: 9.094 / 22.912
45: 6.742 / 16.678
50: 8.469 / 14.949
55: 8.222 / 15.169
60: 7.345 / 14.580
65: 4.759 / 12.738
70: 3.994 / 13.090
75: 5.539 / 11.134
80: 4.543 / 10.437
85: 4.619 / 9.719
90: 3.150 / 8.897
95: 2.442 / 7.886
100: 3.262 / 7.425
105: 3.763 / 5.974
110: 1.674 / 5.609
115: 2.267 / 4.855
120: 1.381 / 4.880
125: 3.407 / 4.849
130: 3.538 / 4.100
135: 1.474 / 3.386
140: 2.430 / 3.378
145: 1.940 / 3.194
150: 1.116 / 3.568
155: 2.430 / 2.307
160: 2.692 / 2.535
165: 0.712 / 2.487
170: 3.426 / 3.138
175: 2.432 / 1.579
180: 1.180 / 2.210
185: 0.846 / 1.491
190: 0.849 / 1.386
195: 1.553 / 1.746
200: 1.444 / 1.233
205: 0.872 / 0.894
210: 0.850 / 0.944
215: 0.843 / 1.120
220: 1.176 / 0.886
225: 0.947 / 0.632
230: 1.691 / 0.658
235: 1.323 / 0.887
240: 0.735 / 0.415
245: 0.554 / 0.430
250: 0.748 / 0.447
255: 0.648 / 0.299
260: 0.830 / 0.335
265: 0.627 / 0.266
270: 1.415 / 0.342
275: 0.564 / 0.249
280: 0.931 / 0.219
285: 0.856 / 0.214
290: 0.790 / 0.204
295: 0.638 / 0.189
300: 0.322 / 0.201
305: 0.423 / 0.168
310: 0.351 / 0.185
315: 0.600 / 0.177
320: 0.239 / 0.197
325: 0.181 / 0.217
330: 0.315 / 0.215
335: 0.556 / 0.209
340: 0.825 / 0.199
345: 0.399 / 0.207
350: 0.126 / 0.237
355: 0.401 / 0.187
360: 0.220 / 0.212
365: 0.123 / 0.281
370: 0.104 / 0.276
375: 0.081 / 0.252
380: 0.116 / 0.266
385: 0.089 / 0.239
390: 0.084 / 0.239
395: 0.096 / 0.243
400: 0.179 / 0.313
Rewirer cayley_clusters with seed 47
Using MSE Loss...
Minimum val loss at epoch 360: 0.21372386813163757
Final val loss at epoch 400: 0.2796403743326664
Train / Val loss by epoch
5: 12.304 / 6.343
10: 23.483 / 5.690
15: 14.839 / 5.831
20: 13.729 / 5.547
25: 9.693 / 7.780
30: 7.435 / 5.166
35: 8.809 / 8.233
40: 7.374 / 6.954
45: 6.434 / 8.086
50: 10.779 / 8.443
55: 4.496 / 10.058
60: 7.105 / 10.846
65: 6.738 / 12.538
70: 5.916 / 9.970
75: 3.158 / 10.300
80: 3.805 / 10.644
85: 2.109 / 11.707
90: 3.523 / 11.925
95: 2.304 / 10.327
100: 2.825 / 9.413
105: 3.159 / 7.916
110: 2.199 / 9.082
115: 1.509 / 8.361
120: 1.539 / 8.403
125: 1.974 / 7.022
130: 1.933 / 6.068
135: 1.920 / 5.877
140: 0.956 / 4.496
145: 1.909 / 4.891
150: 1.740 / 4.711
155: 1.412 / 4.106
160: 1.610 / 3.575
165: 1.285 / 3.620
170: 0.977 / 2.646
175: 1.239 / 2.384
180: 1.140 / 2.247
185: 0.898 / 2.295
190: 0.866 / 2.099
195: 0.991 / 1.414
200: 0.803 / 1.682
205: 0.835 / 1.628
210: 1.147 / 1.942
215: 0.514 / 1.482
220: 0.775 / 1.495
225: 0.807 / 1.123
230: 0.773 / 1.424
235: 0.719 / 1.238
240: 0.533 / 0.914
245: 0.633 / 0.786
250: 0.377 / 0.742
255: 0.620 / 0.945
260: 0.658 / 0.737
265: 0.395 / 0.796
270: 0.467 / 0.577
275: 0.410 / 0.644
280: 0.451 / 0.626
285: 0.264 / 0.488
290: 0.677 / 0.517
295: 0.461 / 0.463
300: 0.301 / 0.402
305: 0.350 / 0.399
310: 0.466 / 0.335
315: 0.435 / 0.335
320: 0.227 / 0.258
325: 0.281 / 0.288
330: 0.570 / 0.271
335: 0.271 / 0.234
340: 0.261 / 0.236
345: 0.347 / 0.235
350: 0.246 / 0.228
355: 0.248 / 0.218
360: 0.474 / 0.214
365: 0.386 / 0.218
370: 0.353 / 0.217
375: 0.281 / 0.215
380: 0.161 / 0.235
385: 0.173 / 0.221
390: 0.094 / 0.246
395: 0.119 / 0.254
400: 0.150 / 0.280
Rewirer cayley_clusters with seed 23
Using MSE Loss...
Minimum val loss at epoch 290: 1.1256159782409667
Final val loss at epoch 400: 1.214662206172943
Train / Val loss by epoch
5: 24.705 / 23.848
10: 28.741 / 15.413
15: 18.005 / 14.472
20: 17.522 / 12.973
25: 8.216 / 11.607
30: 9.819 / 11.117
35: 5.691 / 9.994
40: 6.761 / 9.951
45: 4.857 / 11.585
50: 9.883 / 9.968
55: 3.343 / 10.943
60: 4.595 / 11.971
65: 6.443 / 10.930
70: 4.478 / 13.284
75: 4.121 / 11.283
80: 4.512 / 10.618
85: 2.458 / 8.425
90: 3.338 / 7.616
95: 1.489 / 7.905
100: 3.686 / 7.235
105: 3.709 / 6.062
110: 2.515 / 5.007
115: 1.462 / 6.123
120: 3.181 / 4.988
125: 2.402 / 3.505
130: 1.673 / 3.340
135: 3.149 / 3.351
140: 1.352 / 4.043
145: 1.072 / 2.794
150: 1.698 / 2.533
155: 2.621 / 2.377
160: 3.191 / 1.959
165: 1.009 / 3.096
170: 0.670 / 2.452
175: 1.000 / 2.504
180: 1.415 / 2.002
185: 0.682 / 1.786
190: 3.199 / 1.617
195: 1.113 / 1.750
200: 0.971 / 2.100
205: 0.862 / 2.279
210: 0.855 / 1.903
215: 1.540 / 2.050
220: 0.652 / 1.551
225: 1.263 / 1.586
230: 1.873 / 1.748
235: 1.063 / 1.482
240: 1.059 / 1.181
245: 0.756 / 1.430
250: 1.110 / 1.213
255: 1.363 / 1.293
260: 0.417 / 1.174
265: 1.042 / 1.204
270: 1.252 / 1.267
275: 0.659 / 1.384
280: 0.493 / 1.812
285: 0.274 / 1.724
290: 1.630 / 1.126
295: 0.361 / 1.726
300: 0.293 / 1.706
305: 0.664 / 1.459
310: 0.408 / 1.565
315: 0.445 / 1.239
320: 0.300 / 1.739
325: 0.428 / 1.448
330: 0.540 / 1.604
335: 0.351 / 1.423
340: 0.422 / 1.511
345: 0.191 / 1.635
350: 0.475 / 1.420
355: 0.260 / 1.536
360: 0.224 / 1.575
365: 0.211 / 1.445
370: 0.205 / 1.564
375: 0.164 / 1.381
380: 0.167 / 1.415
385: 0.139 / 1.213
390: 0.225 / 1.288
395: 0.085 / 1.145
400: 0.122 / 1.215
Rewirer cayley with seed 17
Using MSE Loss...
Minimum val loss at epoch 385: 0.07928715422749519
Final val loss at epoch 400: 0.13088543713092804
Train / Val loss by epoch
5: 27.854 / 218.904
10: 22.366 / 113.681
15: 17.151 / 82.741
20: 13.697 / 63.614
25: 8.512 / 51.523
30: 11.247 / 42.637
35: 7.463 / 36.059
40: 7.222 / 33.556
45: 6.741 / 25.933
50: 7.426 / 25.620
55: 5.069 / 22.981
60: 8.963 / 20.070
65: 7.092 / 17.822
70: 4.822 / 16.874
75: 3.859 / 14.576
80: 3.378 / 14.079
85: 4.267 / 12.851
90: 4.522 / 10.492
95: 4.711 / 10.618
100: 3.173 / 10.181
105: 2.810 / 8.292
110: 3.069 / 8.742
115: 2.532 / 6.837
120: 2.620 / 5.985
125: 3.947 / 6.643
130: 5.228 / 5.279
135: 1.801 / 5.057
140: 3.102 / 4.141
145: 1.788 / 3.641
150: 1.378 / 4.619
155: 3.566 / 3.174
160: 2.576 / 3.153
165: 0.825 / 3.723
170: 2.925 / 3.765
175: 2.382 / 2.394
180: 0.727 / 2.743
185: 1.139 / 2.589
190: 0.744 / 2.194
195: 1.665 / 2.550
200: 1.473 / 1.757
205: 1.182 / 1.659
210: 1.048 / 1.427
215: 0.792 / 2.000
220: 0.923 / 1.362
225: 0.782 / 1.268
230: 2.301 / 1.168
235: 1.782 / 1.070
240: 0.859 / 0.605
245: 0.922 / 0.652
250: 0.757 / 0.644
255: 0.734 / 0.527
260: 0.949 / 0.531
265: 0.520 / 0.400
270: 1.219 / 0.485
275: 0.902 / 0.357
280: 0.755 / 0.336
285: 1.904 / 0.257
290: 0.669 / 0.179
295: 0.493 / 0.233
300: 0.507 / 0.195
305: 0.809 / 0.183
310: 0.298 / 0.155
315: 0.831 / 0.137
320: 0.235 / 0.131
325: 0.279 / 0.117
330: 0.324 / 0.116
335: 0.382 / 0.113
340: 0.443 / 0.110
345: 0.670 / 0.111
350: 0.244 / 0.106
355: 0.475 / 0.095
360: 0.409 / 0.093
365: 0.169 / 0.094
370: 0.125 / 0.118
375: 0.109 / 0.092
380: 0.133 / 0.092
385: 0.196 / 0.079
390: 0.133 / 0.096
395: 0.101 / 0.090
400: 0.087 / 0.131
Rewirer cayley with seed 47
Using MSE Loss...
Minimum val loss at epoch 375: 0.16115628331899642
Final val loss at epoch 400: 0.2464330591261387
Train / Val loss by epoch
5: 21.479 / 59.433
10: 18.668 / 61.404
15: 16.967 / 60.438
20: 11.607 / 54.365
25: 8.826 / 53.077
30: 7.604 / 44.600
35: 7.832 / 44.272
40: 10.378 / 36.551
45: 7.761 / 35.287
50: 9.698 / 32.979
55: 7.810 / 30.879
60: 7.061 / 28.619
65: 4.539 / 29.021
70: 6.610 / 23.601
75: 4.336 / 22.484
80: 4.408 / 20.259
85: 4.971 / 22.163
90: 4.174 / 20.741
95: 3.733 / 19.683
100: 4.134 / 18.814
105: 3.946 / 15.717
110: 2.892 / 17.345
115: 1.417 / 15.395
120: 1.441 / 13.759
125: 1.935 / 12.437
130: 2.640 / 10.525
135: 2.063 / 10.848
140: 2.188 / 8.238
145: 2.302 / 9.179
150: 1.185 / 8.702
155: 1.036 / 7.669
160: 1.519 / 6.799
165: 0.949 / 7.365
170: 0.766 / 5.217
175: 0.664 / 4.310
180: 1.187 / 4.304
185: 1.520 / 4.348
190: 0.671 / 4.092
195: 1.423 / 3.152
200: 0.978 / 3.140
205: 0.799 / 3.656
210: 0.720 / 3.377
215: 0.835 / 2.546
220: 0.775 / 2.473
225: 0.625 / 1.935
230: 0.749 / 2.120
235: 0.868 / 1.861
240: 0.705 / 1.543
245: 0.555 / 1.352
250: 0.432 / 1.208
255: 0.763 / 1.188
260: 0.868 / 1.093
265: 0.439 / 0.997
270: 0.740 / 1.000
275: 0.395 / 0.715
280: 0.381 / 0.906
285: 0.426 / 0.583
290: 0.469 / 0.763
295: 0.464 / 0.612
300: 0.329 / 0.524
305: 0.324 / 0.491
310: 0.339 / 0.468
315: 0.248 / 0.466
320: 0.291 / 0.374
325: 0.263 / 0.339
330: 0.499 / 0.295
335: 0.304 / 0.235
340: 0.248 / 0.258
345: 0.203 / 0.222
350: 0.375 / 0.215
355: 0.223 / 0.188
360: 0.261 / 0.173
365: 0.329 / 0.175
370: 0.365 / 0.165
375: 0.185 / 0.161
380: 0.202 / 0.184
385: 0.231 / 0.166
390: 0.167 / 0.168
395: 0.093 / 0.228
400: 0.347 / 0.246
Rewirer cayley with seed 23
Using MSE Loss...
Minimum val loss at epoch 395: 1.3897053569555282
Final val loss at epoch 400: 1.470556277036667
Train / Val loss by epoch
5: 26.527 / 18.021
10: 24.195 / 8.708
15: 16.205 / 6.082
20: 19.375 / 5.258
25: 6.586 / 5.092
30: 7.739 / 5.142
35: 8.171 / 4.413
40: 6.971 / 4.894
45: 8.650 / 6.353
50: 7.328 / 4.512
55: 4.563 / 6.150
60: 3.625 / 6.392
65: 5.277 / 6.462
70: 6.173 / 8.359
75: 4.475 / 6.414
80: 5.130 / 6.481
85: 1.979 / 4.234
90: 3.055 / 4.646
95: 1.507 / 4.283
100: 3.429 / 3.536
105: 2.840 / 3.742
110: 2.187 / 3.177
115: 1.364 / 3.526
120: 2.769 / 3.246
125: 3.076 / 2.306
130: 1.703 / 2.429
135: 3.060 / 2.233
140: 1.106 / 2.560
145: 1.103 / 2.287
150: 1.895 / 2.281
155: 2.382 / 1.645
160: 1.809 / 1.708
165: 1.995 / 2.188
170: 1.037 / 1.997
175: 1.049 / 1.993
180: 1.239 / 1.642
185: 0.833 / 1.654
190: 2.208 / 1.414
195: 0.609 / 1.619
200: 1.316 / 1.863
205: 0.421 / 2.345
210: 0.654 / 2.232
215: 1.388 / 1.576
220: 0.405 / 1.568
225: 1.472 / 1.855
230: 1.283 / 1.903
235: 0.935 / 1.790
240: 0.792 / 1.457
245: 0.837 / 1.624
250: 0.999 / 1.543
255: 0.449 / 1.803
260: 0.477 / 1.904
265: 0.821 / 1.639
270: 0.763 / 1.772
275: 0.528 / 1.824
280: 0.293 / 1.956
285: 0.186 / 1.883
290: 0.601 / 1.411
295: 0.634 / 1.708
300: 0.339 / 1.829
305: 0.609 / 1.611
310: 0.313 / 1.841
315: 0.287 / 1.445
320: 0.242 / 1.936
325: 0.502 / 1.427
330: 0.539 / 1.696
335: 0.236 / 1.741
340: 0.690 / 1.557
345: 0.407 / 1.718
350: 0.277 / 1.753
355: 0.232 / 1.747
360: 0.176 / 1.760
365: 0.538 / 1.461
370: 0.226 / 1.574
375: 0.169 / 1.549
380: 0.100 / 1.600
385: 0.132 / 1.456
390: 0.190 / 1.589
395: 0.095 / 1.390
400: 0.112 / 1.471
{'cayley': [{'best': 0.07928715422749519, 'end': 0.13088543713092804},
            {'best': 0.16115628331899642, 'end': 0.2464330591261387},
            {'best': 1.3897053569555282, 'end': 1.470556277036667}],
 'cayley_clusters': [{'best': 0.1683390349149704, 'end': 0.31281586512923243},
                     {'best': 0.21372386813163757, 'end': 0.2796403743326664},
                     {'best': 1.1256159782409667, 'end': 1.214662206172943}]}
Loading data...
Number of training graphs: 1500
Train graph sizes: Counter({101: 46, 114: 45, 79: 44, 87: 40, 94: 39, 122: 39, 88: 37, 92: 37, 107: 36, 119: 36, 80: 34, 105: 34, 75: 33, 99: 33, 112: 33, 116: 33, 121: 33, 78: 32, 84: 32, 98: 32, 100: 32, 120: 32, 96: 31, 111: 31, 118: 31, 81: 29, 83: 28, 95: 28, 103: 28, 108: 28, 82: 27, 89: 27, 106: 27, 93: 26, 90: 26, 97: 26, 109: 25, 115: 25, 117: 25, 85: 24, 124: 24, 76: 22, 86: 22, 91: 22, 102: 22, 104: 22, 110: 22, 123: 22, 77: 19, 113: 19})
{25: [], 26: [], 27: [], 28: [], 29: [], 30: [], 31: [], 32: [], 33: [], 34: []}
Number of validation graphs: 1500
Val graph sizes: Counter({159: 12, 168: 11, 172: 11, 130: 9, 164: 9, 174: 9, 126: 8, 142: 8, 141: 8, 149: 8, 125: 7, 129: 7, 133: 7, 134: 7, 139: 7, 154: 7, 153: 7, 160: 7, 128: 6, 135: 6, 137: 6, 136: 6, 143: 6, 146: 6, 147: 6, 152: 6, 150: 6, 158: 6, 157: 6, 165: 6, 138: 5, 144: 5, 148: 5, 145: 5, 156: 5, 163: 5, 162: 5, 166: 5, 132: 4, 151: 4, 161: 4, 169: 4, 167: 4, 171: 4, 173: 4, 131: 3, 140: 3, 127: 2, 170: 2, 155: 1})
{'wandb': {'experiment_name': '', 'project': 'ColourInteract Sweeps Friday', 'entity': 'commute_opt_gnn'}, 'data': {'name': 'ColourInteract', 'dataset': 'LRGB', 'c1': 1.0, 'c2s': [0.01, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 100.0], 'num_colours': 4, 'train_size': 1500, 'val_size': 300, 'min_train_nodes': 75, 'max_train_nodes': 125, 'min_val_nodes': 125, 'max_val_nodes': 175, 'rewirers': ['cayley_clusters', 'cayley'], 'normalise': True, 'seed': 93, 'c2': 0.2}, 'model': {'hidden_channels': 8, 'num_layers': 5, 'drop_prob': 0.1, 'global_pool_aggr': 'global_mean_pool', 'approaches': ['only_original', 'interleave'], 'seeds': [17, 47, 23], 'approach': 'only_original', 'seed': 23}, 'train': {'lr': 0.0001, 'num_epochs': 400, 'print_every': 5, 'train_batch_size': 32, 'val_batch_size': 32}, 'run': {'silent': False}}
Device: cpu
No rewiring.
train targets: 10.30 +/- 1.214
val targets: 13.97 +/- 1.246
Rewirer None with seed 17
Using MSE Loss...
Minimum val loss at epoch 400: 0.2583891820162535
Final val loss at epoch 400: 0.2583891820162535
Train / Val loss by epoch
5: 33.581 / 180.707
10: 22.271 / 79.404
15: 14.707 / 57.787
20: 9.523 / 45.298
25: 9.061 / 38.094
30: 8.087 / 30.456
35: 5.306 / 25.769
40: 8.473 / 23.335
45: 5.249 / 17.600
50: 5.331 / 17.972
55: 5.161 / 15.616
60: 9.596 / 15.175
65: 7.688 / 12.841
70: 3.575 / 13.704
75: 4.409 / 12.949
80: 3.846 / 12.477
85: 3.227 / 12.663
90: 2.904 / 11.338
95: 3.008 / 11.170
100: 5.341 / 11.411
105: 3.832 / 9.702
110: 2.741 / 11.513
115: 2.387 / 9.133
120: 2.825 / 8.784
125: 2.891 / 10.166
130: 4.082 / 9.362
135: 2.129 / 8.865
140: 3.854 / 8.991
145: 2.582 / 7.185
150: 1.549 / 8.747
155: 1.909 / 6.888
160: 2.273 / 6.673
165: 2.442 / 7.672
170: 4.257 / 8.242
175: 3.434 / 5.329
180: 1.402 / 5.290
185: 1.406 / 5.455
190: 1.162 / 5.063
195: 1.349 / 3.925
200: 1.552 / 3.947
205: 1.024 / 3.967
210: 1.034 / 3.370
215: 1.258 / 4.190
220: 1.060 / 2.740
225: 1.033 / 3.156
230: 1.689 / 2.424
235: 2.897 / 2.669
240: 0.775 / 1.859
245: 0.601 / 1.682
250: 1.322 / 1.858
255: 0.562 / 1.217
260: 1.036 / 1.642
265: 0.592 / 1.518
270: 0.710 / 1.296
275: 0.782 / 1.348
280: 0.583 / 1.096
285: 1.712 / 1.032
290: 0.842 / 0.974
295: 0.694 / 1.072
300: 0.399 / 0.961
305: 1.101 / 1.007
310: 0.488 / 0.858
315: 1.190 / 1.010
320: 0.392 / 0.722
325: 0.395 / 0.548
330: 0.689 / 0.600
335: 0.352 / 0.529
340: 0.819 / 0.647
345: 0.467 / 0.506
350: 0.153 / 0.487
355: 0.536 / 0.649
360: 0.406 / 0.511
365: 0.310 / 0.570
370: 0.209 / 0.466
375: 0.220 / 0.487
380: 0.213 / 0.450
385: 0.280 / 0.473
390: 0.163 / 0.352
395: 0.296 / 0.404
400: 0.294 / 0.258
Rewirer None with seed 47
Using MSE Loss...
Minimum val loss at epoch 395: 0.25543624609708787
Final val loss at epoch 400: 0.25549269318580625
Train / Val loss by epoch
5: 16.665 / 19.648
10: 17.504 / 23.448
15: 13.553 / 26.181
20: 8.332 / 26.438
25: 9.067 / 29.955
30: 6.104 / 23.929
35: 7.038 / 27.757
40: 10.734 / 26.587
45: 5.965 / 27.258
50: 8.045 / 26.696
55: 6.473 / 26.297
60: 6.269 / 27.185
65: 4.901 / 27.546
70: 5.587 / 22.928
75: 3.660 / 23.402
80: 3.358 / 21.751
85: 3.616 / 20.915
90: 2.862 / 21.035
95: 1.895 / 19.117
100: 2.030 / 17.223
105: 3.714 / 15.556
110: 2.663 / 15.786
115: 1.595 / 15.566
120: 1.843 / 13.545
125: 1.674 / 11.077
130: 1.731 / 10.524
135: 2.061 / 10.408
140: 1.572 / 9.009
145: 1.724 / 9.772
150: 1.583 / 9.467
155: 1.467 / 8.964
160: 1.602 / 8.906
165: 0.879 / 8.409
170: 0.795 / 6.569
175: 0.923 / 6.324
180: 0.665 / 5.720
185: 1.131 / 5.877
190: 1.211 / 5.166
195: 0.934 / 3.957
200: 1.195 / 3.850
205: 1.048 / 3.948
210: 1.810 / 3.995
215: 0.716 / 3.524
220: 1.081 / 3.134
225: 0.532 / 2.303
230: 0.723 / 2.794
235: 0.627 / 2.178
240: 0.693 / 2.004
245: 0.585 / 1.742
250: 0.269 / 1.590
255: 0.587 / 1.571
260: 1.044 / 1.442
265: 0.470 / 1.261
270: 0.485 / 1.054
275: 0.627 / 1.023
280: 0.458 / 1.057
285: 0.398 / 0.830
290: 0.763 / 0.942
295: 0.538 / 0.661
300: 0.462 / 0.550
305: 0.338 / 0.599
310: 0.370 / 0.442
315: 0.377 / 0.445
320: 0.244 / 0.378
325: 0.307 / 0.307
330: 0.345 / 0.347
335: 0.218 / 0.308
340: 0.161 / 0.309
345: 0.184 / 0.319
350: 0.196 / 0.304
355: 0.239 / 0.288
360: 0.336 / 0.281
365: 0.231 / 0.284
370: 0.305 / 0.271
375: 0.312 / 0.280
380: 0.382 / 0.268
385: 0.268 / 0.267
390: 0.220 / 0.269
395: 0.283 / 0.255
400: 0.386 / 0.255
Rewirer None with seed 23
Using MSE Loss...
Minimum val loss at epoch 315: 0.6551692813634873
Final val loss at epoch 400: 0.8290121048688889
Train / Val loss by epoch
5: 31.105 / 17.220
10: 18.419 / 9.433
15: 13.979 / 8.082
20: 23.694 / 6.802
25: 11.347 / 5.802
30: 6.545 / 6.129
35: 6.934 / 5.776
40: 6.189 / 5.844
45: 5.222 / 6.620
50: 6.657 / 6.338
55: 5.820 / 7.381
60: 3.796 / 8.526
65: 5.306 / 7.702
70: 4.446 / 9.452
75: 4.523 / 8.591
80: 4.295 / 7.664
85: 1.992 / 7.091
90: 3.185 / 6.710
95: 1.846 / 6.545
100: 3.720 / 5.848
105: 2.869 / 5.996
110: 2.723 / 5.043
115: 1.596 / 5.770
120: 3.302 / 5.031
125: 1.553 / 4.296
130: 1.932 / 3.986
135: 1.476 / 3.403
140: 1.709 / 3.464
145: 1.656 / 3.127
150: 1.826 / 2.823
155: 2.534 / 1.934
160: 2.122 / 1.901
165: 2.057 / 2.148
170: 0.534 / 2.202
175: 1.296 / 1.691
180: 1.887 / 1.403
185: 0.850 / 1.347
190: 2.982 / 1.291
195: 0.962 / 1.397
200: 1.043 / 1.504
205: 0.961 / 1.415
210: 0.600 / 1.562
215: 1.422 / 1.021
220: 0.827 / 0.888
225: 1.236 / 0.938
230: 1.154 / 1.085
235: 0.447 / 1.112
240: 0.608 / 0.982
245: 0.757 / 0.910
250: 0.816 / 0.870
255: 0.484 / 0.914
260: 0.633 / 0.953
265: 1.520 / 0.924
270: 0.896 / 0.717
275: 0.694 / 0.785
280: 0.349 / 1.084
285: 0.590 / 0.855
290: 0.973 / 0.730
295: 0.567 / 0.946
300: 0.695 / 0.803
305: 0.515 / 0.656
310: 0.370 / 0.987
315: 0.490 / 0.655
320: 0.250 / 0.925
325: 0.515 / 0.822
330: 0.633 / 0.827
335: 0.291 / 0.926
340: 0.703 / 0.839
345: 0.265 / 1.063
350: 0.532 / 0.794
355: 0.517 / 0.780
360: 0.315 / 0.847
365: 0.426 / 0.819
370: 0.309 / 0.915
375: 0.558 / 0.786
380: 0.403 / 0.826
385: 0.257 / 0.861
390: 0.195 / 0.907
395: 0.148 / 0.804
400: 0.267 / 0.829
{None: [{'best': 0.2583891820162535, 'end': 0.2583891820162535},
        {'best': 0.25543624609708787, 'end': 0.25549269318580625},
        {'best': 0.6551692813634873, 'end': 0.8290121048688889}]}
{'wandb': {'experiment_name': '', 'project': 'ColourInteract Sweeps Friday', 'entity': 'commute_opt_gnn'}, 'data': {'name': 'ColourInteract', 'dataset': 'LRGB', 'c1': 1.0, 'c2s': [0.01, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 100.0], 'num_colours': 4, 'train_size': 1500, 'val_size': 300, 'min_train_nodes': 75, 'max_train_nodes': 125, 'min_val_nodes': 125, 'max_val_nodes': 175, 'rewirers': ['cayley_clusters', 'cayley'], 'normalise': True, 'seed': 93, 'c2': 0.2}, 'model': {'hidden_channels': 8, 'num_layers': 5, 'drop_prob': 0.1, 'global_pool_aggr': 'global_mean_pool', 'approaches': ['only_original', 'interleave'], 'seeds': [17, 47, 23], 'approach': 'interleave', 'seed': 23}, 'train': {'lr': 0.0001, 'num_epochs': 400, 'print_every': 5, 'train_batch_size': 32, 'val_batch_size': 32}, 'run': {'silent': False}}
Device: cpu
train targets: 10.30 +/- 1.214
val targets: 13.97 +/- 1.246
Rewirer cayley_clusters with seed 17
Using MSE Loss...
Minimum val loss at epoch 395: 0.16763342767953873
Final val loss at epoch 400: 0.2196468085050583
Train / Val loss by epoch
5: 38.858 / 145.343
10: 30.810 / 63.452
15: 14.506 / 53.954
20: 15.959 / 43.449
25: 6.955 / 35.875
30: 8.812 / 30.916
35: 4.970 / 24.960
40: 9.372 / 25.600
45: 6.993 / 18.801
50: 8.778 / 17.121
55: 8.474 / 16.811
60: 7.399 / 16.214
65: 4.979 / 14.388
70: 4.098 / 14.809
75: 5.718 / 12.445
80: 4.839 / 11.802
85: 4.708 / 11.290
90: 3.307 / 10.647
95: 2.632 / 9.694
100: 3.295 / 8.722
105: 3.975 / 7.444
110: 1.764 / 6.856
115: 2.411 / 6.004
120: 1.576 / 5.908
125: 3.742 / 5.896
130: 3.734 / 5.075
135: 1.632 / 4.145
140: 2.595 / 4.003
145: 1.947 / 3.822
150: 1.232 / 4.228
155: 2.451 / 2.609
160: 2.953 / 2.974
165: 0.867 / 3.109
170: 3.142 / 3.691
175: 2.756 / 2.027
180: 1.268 / 2.866
185: 0.925 / 1.898
190: 0.880 / 1.827
195: 1.568 / 2.185
200: 1.807 / 1.662
205: 0.832 / 1.226
210: 1.125 / 1.324
215: 1.063 / 1.506
220: 1.433 / 1.314
225: 1.073 / 1.024
230: 1.851 / 1.007
235: 1.367 / 1.324
240: 0.814 / 0.658
245: 0.609 / 0.678
250: 0.857 / 0.730
255: 0.650 / 0.496
260: 1.053 / 0.561
265: 0.555 / 0.442
270: 1.652 / 0.680
275: 0.698 / 0.428
280: 0.915 / 0.389
285: 0.968 / 0.331
290: 0.854 / 0.320
295: 0.791 / 0.344
300: 0.410 / 0.275
305: 0.461 / 0.297
310: 0.380 / 0.276
315: 0.736 / 0.282
320: 0.296 / 0.225
325: 0.219 / 0.224
330: 0.496 / 0.217
335: 0.803 / 0.214
340: 0.967 / 0.217
345: 0.605 / 0.209
350: 0.155 / 0.200
355: 0.521 / 0.195
360: 0.543 / 0.184
365: 0.179 / 0.187
370: 0.152 / 0.184
375: 0.159 / 0.175
380: 0.247 / 0.179
385: 0.178 / 0.174
390: 0.167 / 0.178
395: 0.192 / 0.168
400: 0.253 / 0.220
Rewirer cayley_clusters with seed 47
Using MSE Loss...
Minimum val loss at epoch 395: 0.2678888037800789
Final val loss at epoch 400: 0.2730488196015358
Train / Val loss by epoch
5: 12.550 / 7.140
10: 22.875 / 6.832
15: 14.591 / 7.374
20: 13.305 / 7.332
25: 9.360 / 10.124
30: 6.992 / 6.863
35: 8.062 / 10.148
40: 7.552 / 8.566
45: 6.128 / 9.632
50: 10.370 / 10.336
55: 3.998 / 11.784
60: 6.823 / 13.299
65: 6.596 / 14.770
70: 5.796 / 12.598
75: 2.722 / 12.440
80: 3.563 / 13.043
85: 2.217 / 13.974
90: 3.552 / 14.021
95: 2.357 / 12.548
100: 2.665 / 11.007
105: 3.072 / 9.307
110: 2.162 / 10.146
115: 1.292 / 9.269
120: 1.533 / 9.197
125: 1.979 / 7.407
130: 1.777 / 6.374
135: 1.834 / 6.130
140: 0.909 / 4.945
145: 1.944 / 5.217
150: 1.641 / 5.069
155: 1.499 / 4.688
160: 1.519 / 4.145
165: 1.352 / 4.192
170: 1.034 / 3.245
175: 1.416 / 2.857
180: 1.207 / 2.785
185: 0.872 / 3.003
190: 0.677 / 2.887
195: 0.895 / 1.841
200: 0.994 / 2.311
205: 0.888 / 2.131
210: 1.311 / 2.578
215: 0.643 / 1.894
220: 1.085 / 1.907
225: 0.777 / 1.432
230: 0.868 / 1.780
235: 0.779 / 1.658
240: 0.631 / 1.322
245: 0.971 / 1.048
250: 0.429 / 0.981
255: 0.741 / 1.162
260: 0.820 / 1.054
265: 0.532 / 1.029
270: 0.528 / 0.890
275: 0.583 / 0.952
280: 0.556 / 0.885
285: 0.418 / 0.696
290: 1.003 / 0.805
295: 0.610 / 0.649
300: 0.358 / 0.583
305: 0.536 / 0.574
310: 0.471 / 0.454
315: 0.507 / 0.497
320: 0.316 / 0.383
325: 0.376 / 0.386
330: 0.704 / 0.394
335: 0.300 / 0.342
340: 0.360 / 0.377
345: 0.495 / 0.345
350: 0.339 / 0.320
355: 0.317 / 0.295
360: 0.582 / 0.291
365: 0.399 / 0.309
370: 0.431 / 0.280
375: 0.364 / 0.278
380: 0.219 / 0.272
385: 0.182 / 0.272
390: 0.110 / 0.271
395: 0.212 / 0.268
400: 0.200 / 0.273
Rewirer cayley_clusters with seed 23
Using MSE Loss...
Minimum val loss at epoch 240: 0.5769564002752304
Final val loss at epoch 400: 0.7243019118905067
Train / Val loss by epoch
5: 24.056 / 20.209
10: 28.522 / 12.198
15: 17.897 / 11.311
20: 17.287 / 10.048
25: 7.787 / 8.783
30: 9.545 / 8.988
35: 5.485 / 7.985
40: 6.436 / 7.907
45: 4.578 / 9.231
50: 9.856 / 7.978
55: 3.186 / 9.000
60: 4.256 / 9.662
65: 6.176 / 8.240
70: 4.510 / 10.008
75: 4.040 / 8.572
80: 4.503 / 8.070
85: 2.298 / 6.264
90: 3.286 / 5.549
95: 1.496 / 5.914
100: 3.395 / 5.282
105: 3.360 / 4.283
110: 2.580 / 3.391
115: 1.533 / 4.309
120: 3.214 / 3.644
125: 2.022 / 2.571
130: 1.810 / 2.378
135: 3.009 / 2.499
140: 1.284 / 3.206
145: 1.114 / 2.048
150: 1.873 / 1.721
155: 2.780 / 1.787
160: 3.170 / 1.507
165: 1.017 / 2.077
170: 0.768 / 1.454
175: 0.912 / 1.547
180: 1.843 / 1.190
185: 0.966 / 1.049
190: 3.676 / 0.946
195: 1.314 / 1.011
200: 1.168 / 1.206
205: 1.017 / 1.246
210: 0.955 / 0.951
215: 1.711 / 1.003
220: 0.754 / 0.731
225: 1.169 / 0.711
230: 1.861 / 0.964
235: 1.148 / 0.805
240: 1.195 / 0.577
245: 1.087 / 0.718
250: 1.296 / 0.597
255: 1.245 / 0.649
260: 0.633 / 0.622
265: 1.076 / 0.641
270: 1.029 / 0.610
275: 0.759 / 0.681
280: 0.700 / 1.008
285: 0.510 / 0.885
290: 1.391 / 0.618
295: 0.388 / 0.873
300: 0.357 / 0.923
305: 0.742 / 0.698
310: 0.530 / 0.749
315: 0.573 / 0.578
320: 0.238 / 0.824
325: 0.471 / 0.685
330: 0.873 / 0.754
335: 0.358 / 0.659
340: 0.678 / 0.737
345: 0.337 / 0.846
350: 0.547 / 0.650
355: 0.431 / 0.664
360: 0.440 / 0.729
365: 0.314 / 0.615
370: 0.334 / 0.802
375: 0.237 / 0.713
380: 0.223 / 0.749
385: 0.178 / 0.660
390: 0.207 / 0.761
395: 0.285 / 0.627
400: 0.210 / 0.724
Rewirer cayley with seed 17
Using MSE Loss...
Minimum val loss at epoch 400: 0.12956703528761865
Final val loss at epoch 400: 0.12956703528761865
Train / Val loss by epoch
5: 32.094 / 272.045
10: 23.362 / 126.513
15: 17.373 / 93.312
20: 14.129 / 72.295
25: 8.599 / 58.745
30: 11.550 / 48.770
35: 7.791 / 41.227
40: 7.414 / 38.425
45: 6.993 / 29.904
50: 7.845 / 29.751
55: 5.168 / 26.482
60: 9.226 / 23.293
65: 7.022 / 21.028
70: 4.830 / 19.924
75: 4.128 / 17.263
80: 3.780 / 16.996
85: 4.335 / 15.804
90: 4.833 / 13.299
95: 4.820 / 13.450
100: 3.264 / 12.558
105: 3.064 / 10.698
110: 3.245 / 11.090
115: 2.639 / 8.949
120: 2.839 / 7.884
125: 4.534 / 8.914
130: 5.529 / 7.369
135: 1.920 / 6.634
140: 3.048 / 5.532
145: 2.194 / 5.140
150: 1.373 / 6.342
155: 3.782 / 4.457
160: 2.684 / 4.434
165: 1.025 / 5.085
170: 3.466 / 5.173
175: 2.745 / 3.563
180: 0.932 / 3.932
185: 1.581 / 3.615
190: 0.960 / 3.115
195: 2.019 / 3.589
200: 1.708 / 2.684
205: 1.081 / 2.466
210: 1.288 / 2.309
215: 0.977 / 2.903
220: 1.157 / 2.128
225: 0.992 / 2.164
230: 2.382 / 1.887
235: 2.027 / 1.872
240: 0.799 / 1.113
245: 1.014 / 1.270
250: 0.908 / 1.206
255: 0.890 / 1.092
260: 1.035 / 1.098
265: 0.678 / 0.812
270: 1.722 / 1.178
275: 1.190 / 0.898
280: 0.723 / 0.938
285: 1.799 / 0.752
290: 0.788 / 0.516
295: 0.471 / 0.661
300: 0.580 / 0.550
305: 0.738 / 0.544
310: 0.400 / 0.526
315: 0.947 / 0.449
320: 0.292 / 0.337
325: 0.265 / 0.297
330: 0.467 / 0.303
335: 0.540 / 0.404
340: 0.482 / 0.317
345: 0.724 / 0.342
350: 0.294 / 0.215
355: 0.663 / 0.314
360: 0.691 / 0.237
365: 0.285 / 0.198
370: 0.168 / 0.166
375: 0.142 / 0.215
380: 0.268 / 0.198
385: 0.286 / 0.237
390: 0.238 / 0.163
395: 0.173 / 0.165
400: 0.127 / 0.130
Rewirer cayley with seed 47
Using MSE Loss...
Minimum val loss at epoch 395: 0.21541884392499924
Final val loss at epoch 400: 0.21730963587760926
Train / Val loss by epoch
5: 22.254 / 62.727
10: 18.461 / 64.108
15: 16.935 / 63.471
20: 11.651 / 57.411
25: 8.497 / 56.128
30: 7.422 / 46.540
35: 7.442 / 45.754
40: 10.567 / 37.658
45: 7.426 / 36.869
50: 9.865 / 34.446
55: 6.983 / 32.247
60: 7.033 / 30.290
65: 4.340 / 30.439
70: 6.504 / 25.702
75: 3.872 / 24.255
80: 4.132 / 22.217
85: 5.013 / 23.638
90: 4.294 / 22.427
95: 3.578 / 22.012
100: 4.064 / 20.178
105: 3.817 / 17.167
110: 2.725 / 18.841
115: 1.406 / 16.716
120: 1.580 / 14.876
125: 2.052 / 12.966
130: 2.504 / 11.277
135: 2.195 / 11.631
140: 2.199 / 9.092
145: 2.305 / 9.839
150: 1.214 / 8.956
155: 1.137 / 8.375
160: 1.697 / 7.342
165: 1.015 / 7.673
170: 0.838 / 5.776
175: 0.735 / 4.701
180: 1.098 / 4.853
185: 1.617 / 4.948
190: 0.660 / 4.611
195: 1.117 / 3.481
200: 0.962 / 3.588
205: 1.048 / 4.180
210: 0.863 / 4.004
215: 1.108 / 2.950
220: 0.923 / 2.959
225: 0.713 / 2.453
230: 0.960 / 2.626
235: 0.828 / 2.486
240: 0.686 / 1.997
245: 0.628 / 1.724
250: 0.414 / 1.622
255: 0.754 / 1.672
260: 1.000 / 1.580
265: 0.499 / 1.476
270: 0.720 / 1.403
275: 0.340 / 1.031
280: 0.428 / 1.213
285: 0.490 / 0.901
290: 0.745 / 1.145
295: 0.521 / 0.929
300: 0.469 / 0.845
305: 0.404 / 0.748
310: 0.365 / 0.650
315: 0.387 / 0.680
320: 0.246 / 0.625
325: 0.325 / 0.463
330: 0.764 / 0.489
335: 0.347 / 0.384
340: 0.259 / 0.441
345: 0.302 / 0.384
350: 0.484 / 0.364
355: 0.212 / 0.303
360: 0.353 / 0.270
365: 0.381 / 0.310
370: 0.473 / 0.250
375: 0.311 / 0.237
380: 0.313 / 0.226
385: 0.314 / 0.229
390: 0.192 / 0.224
395: 0.209 / 0.215
400: 0.386 / 0.217
Rewirer cayley with seed 23
Using MSE Loss...
Minimum val loss at epoch 315: 0.5747541800141335
Final val loss at epoch 400: 0.9033867061138153
Train / Val loss by epoch
5: 25.672 / 17.300
10: 24.089 / 8.001
15: 15.760 / 5.398
20: 18.988 / 4.519
25: 6.658 / 4.298
30: 7.705 / 4.709
35: 7.899 / 3.917
40: 6.752 / 4.365
45: 8.360 / 5.520
50: 7.248 / 3.930
55: 4.482 / 5.104
60: 3.585 / 5.198
65: 5.381 / 5.202
70: 6.300 / 6.511
75: 4.324 / 5.080
80: 4.817 / 4.719
85: 2.196 / 2.997
90: 3.290 / 3.272
95: 1.627 / 2.932
100: 3.513 / 2.454
105: 2.732 / 2.592
110: 2.388 / 2.149
115: 1.420 / 2.348
120: 2.728 / 2.249
125: 2.806 / 1.534
130: 1.862 / 1.499
135: 2.641 / 1.678
140: 1.234 / 1.862
145: 1.305 / 1.635
150: 2.079 / 1.628
155: 2.292 / 1.020
160: 1.877 / 1.082
165: 1.694 / 1.481
170: 1.199 / 1.193
175: 1.002 / 1.349
180: 1.440 / 0.964
185: 0.921 / 1.053
190: 2.674 / 0.848
195: 0.864 / 0.928
200: 1.398 / 1.001
205: 0.481 / 1.338
210: 0.734 / 1.163
215: 1.478 / 0.823
220: 0.666 / 0.999
225: 1.274 / 0.859
230: 1.423 / 0.921
235: 0.873 / 0.976
240: 0.914 / 0.740
245: 0.860 / 0.701
250: 1.246 / 0.624
255: 0.548 / 0.793
260: 0.770 / 0.703
265: 1.129 / 0.610
270: 0.787 / 0.624
275: 0.805 / 0.659
280: 0.511 / 0.893
285: 0.427 / 0.879
290: 0.755 / 0.627
295: 0.594 / 0.845
300: 0.604 / 0.841
305: 0.761 / 0.776
310: 0.376 / 0.896
315: 0.408 / 0.575
320: 0.325 / 0.901
325: 0.636 / 0.647
330: 0.879 / 0.775
335: 0.427 / 0.803
340: 1.247 / 0.728
345: 0.538 / 0.961
350: 0.405 / 0.843
355: 0.400 / 0.859
360: 0.512 / 0.849
365: 1.063 / 0.645
370: 0.504 / 0.833
375: 0.421 / 0.815
380: 0.186 / 0.853
385: 0.236 / 0.847
390: 0.171 / 0.899
395: 0.298 / 0.811
400: 0.181 / 0.903
{'cayley': [{'best': 0.12956703528761865, 'end': 0.12956703528761865},
            {'best': 0.21541884392499924, 'end': 0.21730963587760926},
            {'best': 0.5747541800141335, 'end': 0.9033867061138153}],
 'cayley_clusters': [{'best': 0.16763342767953873, 'end': 0.2196468085050583},
                     {'best': 0.2678888037800789, 'end': 0.2730488196015358},
                     {'best': 0.5769564002752304, 'end': 0.7243019118905067}]}
Loading data...
Number of training graphs: 1500
Train graph sizes: Counter({101: 46, 114: 45, 79: 44, 87: 40, 94: 39, 122: 39, 88: 37, 92: 37, 107: 36, 119: 36, 80: 34, 105: 34, 75: 33, 99: 33, 112: 33, 116: 33, 121: 33, 78: 32, 84: 32, 98: 32, 100: 32, 120: 32, 96: 31, 111: 31, 118: 31, 81: 29, 83: 28, 95: 28, 103: 28, 108: 28, 82: 27, 89: 27, 106: 27, 93: 26, 90: 26, 97: 26, 109: 25, 115: 25, 117: 25, 85: 24, 124: 24, 76: 22, 86: 22, 91: 22, 102: 22, 104: 22, 110: 22, 123: 22, 77: 19, 113: 19})
{25: [], 26: [], 27: [], 28: [], 29: [], 30: [], 31: [], 32: [], 33: [], 34: []}
Number of validation graphs: 1500
Val graph sizes: Counter({159: 12, 168: 11, 172: 11, 130: 9, 164: 9, 174: 9, 126: 8, 142: 8, 141: 8, 149: 8, 125: 7, 129: 7, 133: 7, 134: 7, 139: 7, 154: 7, 153: 7, 160: 7, 128: 6, 135: 6, 137: 6, 136: 6, 143: 6, 146: 6, 147: 6, 152: 6, 150: 6, 158: 6, 157: 6, 165: 6, 138: 5, 144: 5, 148: 5, 145: 5, 156: 5, 163: 5, 162: 5, 166: 5, 132: 4, 151: 4, 161: 4, 169: 4, 167: 4, 171: 4, 173: 4, 131: 3, 140: 3, 127: 2, 170: 2, 155: 1})
{'wandb': {'experiment_name': '', 'project': 'ColourInteract Sweeps Friday', 'entity': 'commute_opt_gnn'}, 'data': {'name': 'ColourInteract', 'dataset': 'LRGB', 'c1': 1.0, 'c2s': [0.01, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 100.0], 'num_colours': 4, 'train_size': 1500, 'val_size': 300, 'min_train_nodes': 75, 'max_train_nodes': 125, 'min_val_nodes': 125, 'max_val_nodes': 175, 'rewirers': ['cayley_clusters', 'cayley'], 'normalise': True, 'seed': 93, 'c2': 0.5}, 'model': {'hidden_channels': 8, 'num_layers': 5, 'drop_prob': 0.1, 'global_pool_aggr': 'global_mean_pool', 'approaches': ['only_original', 'interleave'], 'seeds': [17, 47, 23], 'approach': 'only_original', 'seed': 23}, 'train': {'lr': 0.0001, 'num_epochs': 400, 'print_every': 5, 'train_batch_size': 32, 'val_batch_size': 32}, 'run': {'silent': False}}
Device: cpu
No rewiring.
train targets: 21.22 +/- 2.926
val targets: 30.41 +/- 2.997
Rewirer None with seed 17
Using MSE Loss...
Minimum val loss at epoch 400: 2.2498239159584044
Final val loss at epoch 400: 2.2498239159584044
Train / Val loss by epoch
5: 74.878 / 377.223
10: 24.870 / 110.317
15: 15.885 / 82.740
20: 10.718 / 67.129
25: 10.352 / 56.805
30: 9.526 / 45.457
35: 6.223 / 39.860
40: 8.984 / 35.850
45: 6.373 / 28.454
50: 6.415 / 28.849
55: 5.203 / 24.566
60: 10.037 / 24.043
65: 7.939 / 21.581
70: 4.054 / 22.120
75: 5.073 / 21.144
80: 4.791 / 20.639
85: 3.381 / 21.335
90: 3.649 / 20.304
95: 3.184 / 20.388
100: 5.656 / 19.350
105: 5.575 / 18.033
110: 3.387 / 20.146
115: 3.095 / 17.214
120: 3.895 / 15.741
125: 4.533 / 18.211
130: 5.863 / 16.669
135: 2.555 / 15.443
140: 3.324 / 14.511
145: 3.198 / 12.829
150: 2.107 / 15.764
155: 2.272 / 12.241
160: 2.972 / 12.093
165: 1.987 / 12.934
170: 4.335 / 13.665
175: 4.032 / 9.540
180: 1.575 / 8.971
185: 2.078 / 8.660
190: 1.378 / 9.592
195: 2.374 / 8.054
200: 2.186 / 8.311
205: 1.346 / 7.797
210: 2.004 / 6.897
215: 1.681 / 7.841
220: 2.359 / 7.190
225: 2.247 / 7.495
230: 2.180 / 5.877
235: 4.140 / 7.030
240: 1.524 / 4.911
245: 1.418 / 5.467
250: 1.478 / 5.118
255: 0.763 / 4.695
260: 1.069 / 5.057
265: 0.671 / 4.482
270: 1.535 / 4.876
275: 1.566 / 5.009
280: 1.230 / 5.043
285: 3.111 / 4.371
290: 1.713 / 4.192
295: 1.259 / 4.553
300: 0.930 / 3.922
305: 1.544 / 4.353
310: 1.123 / 4.276
315: 2.695 / 4.566
320: 1.204 / 3.580
325: 0.972 / 3.115
330: 1.064 / 3.424
335: 1.038 / 3.256
340: 0.982 / 3.236
345: 1.860 / 3.492
350: 0.461 / 2.655
355: 1.702 / 3.464
360: 1.300 / 2.956
365: 0.507 / 3.607
370: 0.556 / 2.721
375: 0.825 / 2.950
380: 0.772 / 2.897
385: 0.873 / 2.893
390: 0.591 / 2.655
395: 0.882 / 2.863
400: 0.405 / 2.250
Rewirer None with seed 47
Using MSE Loss...
Minimum val loss at epoch 385: 1.7471012234687806
Final val loss at epoch 400: 1.9402641534805298
Train / Val loss by epoch
5: 16.947 / 35.139
10: 16.335 / 40.287
15: 13.099 / 41.046
20: 8.259 / 44.004
25: 8.157 / 44.491
30: 5.564 / 35.568
35: 5.070 / 40.381
40: 9.921 / 37.245
45: 5.050 / 35.506
50: 7.223 / 36.039
55: 4.814 / 33.973
60: 6.812 / 34.009
65: 4.677 / 33.210
70: 4.616 / 29.132
75: 3.283 / 26.756
80: 3.376 / 25.070
85: 3.449 / 22.438
90: 3.829 / 23.965
95: 2.292 / 23.163
100: 2.269 / 18.807
105: 4.429 / 16.949
110: 3.583 / 17.293
115: 1.796 / 16.777
120: 1.957 / 15.013
125: 1.814 / 13.169
130: 2.031 / 12.388
135: 2.210 / 13.504
140: 1.722 / 11.882
145: 1.677 / 11.560
150: 1.494 / 12.647
155: 1.695 / 11.239
160: 1.529 / 10.735
165: 0.939 / 9.766
170: 0.909 / 9.333
175: 1.166 / 9.121
180: 1.076 / 8.147
185: 1.677 / 7.779
190: 0.845 / 7.636
195: 1.000 / 6.177
200: 1.258 / 6.949
205: 1.017 / 6.460
210: 1.799 / 6.769
215: 0.856 / 5.807
220: 1.375 / 5.045
225: 1.336 / 4.380
230: 1.089 / 5.292
235: 1.090 / 4.719
240: 0.808 / 3.838
245: 1.183 / 3.907
250: 0.508 / 3.843
255: 0.689 / 3.568
260: 0.963 / 3.454
265: 0.545 / 3.411
270: 0.947 / 3.230
275: 1.221 / 3.445
280: 0.509 / 3.097
285: 0.764 / 2.652
290: 0.888 / 2.836
295: 0.646 / 2.775
300: 0.659 / 2.420
305: 0.792 / 2.274
310: 0.766 / 2.381
315: 0.599 / 2.781
320: 0.501 / 2.288
325: 0.604 / 2.050
330: 0.564 / 2.360
335: 0.882 / 2.143
340: 0.503 / 2.511
345: 0.449 / 1.953
350: 0.353 / 2.506
355: 0.512 / 2.072
360: 0.674 / 2.121
365: 0.329 / 2.173
370: 0.636 / 1.978
375: 0.915 / 2.067
380: 0.465 / 1.938
385: 0.459 / 1.747
390: 0.622 / 1.871
395: 0.872 / 1.794
400: 0.491 / 1.940
Rewirer None with seed 23
Using MSE Loss...
Minimum val loss at epoch 355: 0.48946532905101775
Final val loss at epoch 400: 0.5754266306757927
Train / Val loss by epoch
5: 27.283 / 12.167
10: 19.118 / 5.792
15: 11.741 / 4.408
20: 21.443 / 3.733
25: 9.419 / 3.129
30: 6.610 / 3.499
35: 7.541 / 3.455
40: 4.867 / 2.776
45: 5.395 / 2.800
50: 7.641 / 2.802
55: 5.398 / 3.000
60: 3.512 / 3.334
65: 4.838 / 2.784
70: 5.239 / 3.471
75: 4.884 / 3.029
80: 3.676 / 2.627
85: 2.894 / 2.428
90: 3.408 / 2.540
95: 2.067 / 2.543
100: 3.599 / 2.172
105: 2.764 / 2.423
110: 2.074 / 2.029
115: 1.887 / 2.411
120: 3.693 / 2.248
125: 1.795 / 1.917
130: 2.129 / 1.711
135: 2.094 / 1.827
140: 2.314 / 1.843
145: 1.515 / 1.693
150: 2.258 / 1.580
155: 3.074 / 1.280
160: 2.814 / 1.269
165: 1.816 / 1.250
170: 1.381 / 1.311
175: 1.726 / 1.152
180: 1.906 / 1.055
185: 1.409 / 0.861
190: 3.491 / 0.926
195: 1.211 / 0.978
200: 1.510 / 0.947
205: 1.924 / 1.014
210: 1.317 / 0.971
215: 1.679 / 0.970
220: 1.484 / 0.819
225: 1.474 / 0.861
230: 1.594 / 0.935
235: 0.932 / 0.833
240: 1.384 / 0.717
245: 1.244 / 0.823
250: 1.755 / 0.687
255: 0.554 / 0.733
260: 1.018 / 0.750
265: 2.243 / 0.764
270: 0.814 / 0.683
275: 1.831 / 0.640
280: 0.539 / 0.834
285: 1.279 / 0.847
290: 1.216 / 0.672
295: 0.725 / 0.742
300: 0.984 / 0.751
305: 0.863 / 0.597
310: 0.810 / 0.805
315: 1.044 / 0.615
320: 0.883 / 0.646
325: 1.588 / 0.594
330: 1.649 / 0.606
335: 0.925 / 0.629
340: 1.198 / 0.603
345: 0.723 / 0.841
350: 1.419 / 0.606
355: 1.819 / 0.489
360: 0.963 / 0.591
365: 1.129 / 0.501
370: 0.942 / 0.612
375: 1.159 / 0.586
380: 1.459 / 0.534
385: 0.683 / 0.570
390: 0.485 / 0.544
395: 0.635 / 0.574
400: 0.627 / 0.575
{None: [{'best': 2.2498239159584044, 'end': 2.2498239159584044},
        {'best': 1.7471012234687806, 'end': 1.9402641534805298},
        {'best': 0.48946532905101775, 'end': 0.5754266306757927}]}
{'wandb': {'experiment_name': '', 'project': 'ColourInteract Sweeps Friday', 'entity': 'commute_opt_gnn'}, 'data': {'name': 'ColourInteract', 'dataset': 'LRGB', 'c1': 1.0, 'c2s': [0.01, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 100.0], 'num_colours': 4, 'train_size': 1500, 'val_size': 300, 'min_train_nodes': 75, 'max_train_nodes': 125, 'min_val_nodes': 125, 'max_val_nodes': 175, 'rewirers': ['cayley_clusters', 'cayley'], 'normalise': True, 'seed': 93, 'c2': 0.5}, 'model': {'hidden_channels': 8, 'num_layers': 5, 'drop_prob': 0.1, 'global_pool_aggr': 'global_mean_pool', 'approaches': ['only_original', 'interleave'], 'seeds': [17, 47, 23], 'approach': 'interleave', 'seed': 23}, 'train': {'lr': 0.0001, 'num_epochs': 400, 'print_every': 5, 'train_batch_size': 32, 'val_batch_size': 32}, 'run': {'silent': False}}
Device: cpu
train targets: 21.22 +/- 2.926
val targets: 30.41 +/- 2.997
Rewirer cayley_clusters with seed 17
Using MSE Loss...
Minimum val loss at epoch 400: 1.159334772825241
Final val loss at epoch 400: 1.159334772825241
Train / Val loss by epoch
5: 74.755 / 314.195
10: 35.512 / 85.082
15: 15.646 / 72.190
20: 17.108 / 59.103
25: 8.077 / 49.444
30: 10.345 / 42.423
35: 6.154 / 35.786
40: 10.536 / 35.336
45: 8.294 / 27.080
50: 10.280 / 25.241
55: 9.284 / 23.503
60: 8.292 / 22.704
65: 5.891 / 20.647
70: 4.811 / 20.702
75: 6.605 / 16.927
80: 6.205 / 16.399
85: 5.184 / 15.938
90: 3.941 / 16.606
95: 3.535 / 15.682
100: 3.413 / 13.037
105: 4.627 / 12.209
110: 2.253 / 11.533
115: 2.880 / 9.951
120: 2.500 / 8.733
125: 4.569 / 9.498
130: 4.526 / 8.367
135: 2.025 / 6.802
140: 3.057 / 6.586
145: 2.406 / 5.836
150: 1.875 / 7.124
155: 2.567 / 4.622
160: 3.493 / 5.395
165: 1.234 / 5.094
170: 3.019 / 5.714
175: 3.770 / 3.513
180: 1.786 / 4.567
185: 1.515 / 3.707
190: 1.248 / 3.451
195: 1.683 / 3.796
200: 2.271 / 3.070
205: 1.106 / 2.571
210: 1.768 / 2.868
215: 2.239 / 3.256
220: 2.004 / 3.161
225: 1.637 / 2.508
230: 2.417 / 2.519
235: 2.189 / 3.172
240: 1.207 / 1.994
245: 1.078 / 2.356
250: 1.310 / 2.215
255: 0.878 / 2.045
260: 1.457 / 1.957
265: 0.900 / 1.631
270: 2.919 / 2.491
275: 1.516 / 2.133
280: 1.124 / 2.112
285: 1.809 / 1.772
290: 1.524 / 1.692
295: 1.475 / 2.074
300: 0.885 / 1.823
305: 1.120 / 1.721
310: 0.796 / 1.789
315: 2.473 / 2.114
320: 0.887 / 1.618
325: 0.489 / 1.495
330: 1.091 / 1.609
335: 1.274 / 1.603
340: 1.601 / 1.627
345: 1.311 / 1.661
350: 0.713 / 1.606
355: 1.570 / 1.883
360: 1.450 / 1.607
365: 0.534 / 1.723
370: 0.341 / 1.733
375: 0.471 / 1.774
380: 0.823 / 1.681
385: 0.609 / 1.842
390: 0.723 / 1.537
395: 0.768 / 1.672
400: 0.517 / 1.159
Rewirer cayley_clusters with seed 47
Using MSE Loss...
Minimum val loss at epoch 370: 0.6896916598081588
Final val loss at epoch 400: 0.7737202554941177
Train / Val loss by epoch
5: 14.413 / 12.499
10: 21.271 / 13.183
15: 13.999 / 13.556
20: 12.051 / 14.113
25: 7.846 / 16.868
30: 6.081 / 12.869
35: 6.366 / 16.361
40: 7.727 / 14.138
45: 6.021 / 14.846
50: 9.117 / 17.505
55: 3.626 / 17.034
60: 7.096 / 19.070
65: 5.477 / 20.412
70: 6.039 / 18.308
75: 2.185 / 17.003
80: 2.822 / 18.648
85: 3.054 / 18.909
90: 4.106 / 17.974
95: 2.690 / 17.405
100: 2.530 / 16.005
105: 2.950 / 13.632
110: 2.262 / 14.038
115: 1.401 / 12.787
120: 2.066 / 12.570
125: 2.246 / 10.856
130: 2.031 / 9.532
135: 1.775 / 9.837
140: 1.517 / 8.576
145: 1.823 / 8.749
150: 1.793 / 8.915
155: 1.818 / 8.112
160: 1.917 / 6.733
165: 1.339 / 6.674
170: 1.135 / 6.282
175: 1.557 / 5.804
180: 1.563 / 5.443
185: 1.440 / 5.113
190: 0.664 / 5.215
195: 1.067 / 3.549
200: 1.206 / 4.315
205: 1.660 / 3.904
210: 1.391 / 4.145
215: 1.176 / 3.500
220: 1.994 / 3.369
225: 1.014 / 2.867
230: 1.108 / 3.050
235: 1.203 / 3.087
240: 1.152 / 2.381
245: 1.302 / 2.149
250: 0.963 / 1.935
255: 1.269 / 2.120
260: 1.194 / 1.936
265: 0.813 / 1.831
270: 1.083 / 1.608
275: 0.645 / 1.590
280: 0.709 / 1.557
285: 0.676 / 1.402
290: 0.931 / 1.233
295: 0.725 / 1.197
300: 0.458 / 1.037
305: 0.780 / 1.136
310: 0.749 / 1.007
315: 0.466 / 1.275
320: 0.938 / 0.988
325: 0.752 / 0.949
330: 0.697 / 0.847
335: 0.616 / 0.858
340: 0.202 / 1.003
345: 0.430 / 0.938
350: 0.495 / 0.900
355: 0.402 / 0.843
360: 0.633 / 0.888
365: 0.468 / 0.861
370: 0.605 / 0.690
375: 0.571 / 0.710
380: 0.534 / 0.835
385: 0.408 / 0.872
390: 0.310 / 0.736
395: 0.453 / 0.705
400: 0.419 / 0.774
Rewirer cayley_clusters with seed 23
Using MSE Loss...
Minimum val loss at epoch 395: 0.44490463733673097
Final val loss at epoch 400: 0.445828965306282
Train / Val loss by epoch
5: 21.933 / 13.144
10: 28.050 / 6.845
15: 17.448 / 5.863
20: 16.639 / 5.052
25: 7.065 / 4.587
30: 9.343 / 5.195
35: 5.382 / 4.240
40: 5.930 / 3.912
45: 4.401 / 3.736
50: 10.173 / 3.337
55: 3.133 / 3.881
60: 3.299 / 4.094
65: 6.368 / 2.909
70: 5.408 / 3.840
75: 4.256 / 3.652
80: 4.173 / 3.039
85: 1.964 / 2.616
90: 3.842 / 2.248
95: 2.012 / 2.456
100: 3.216 / 2.163
105: 2.232 / 1.948
110: 2.671 / 1.475
115: 1.426 / 1.918
120: 3.278 / 1.741
125: 2.530 / 1.277
130: 1.999 / 1.153
135: 3.411 / 1.237
140: 1.960 / 1.518
145: 1.306 / 1.212
150: 2.328 / 1.000
155: 3.198 / 1.026
160: 2.750 / 0.939
165: 2.021 / 0.920
170: 1.196 / 0.817
175: 1.391 / 0.874
180: 2.315 / 0.754
185: 1.376 / 0.728
190: 4.611 / 0.721
195: 1.948 / 0.710
200: 1.356 / 0.721
205: 1.578 / 0.659
210: 1.341 / 0.665
215: 2.100 / 0.661
220: 1.409 / 0.607
225: 1.587 / 0.609
230: 2.252 / 0.648
235: 1.239 / 0.598
240: 2.240 / 0.555
245: 1.276 / 0.569
250: 1.752 / 0.533
255: 1.115 / 0.544
260: 1.353 / 0.539
265: 1.600 / 0.530
270: 1.391 / 0.511
275: 1.868 / 0.514
280: 0.798 / 0.554
285: 1.128 / 0.528
290: 1.771 / 0.498
295: 0.637 / 0.506
300: 0.787 / 0.579
305: 0.862 / 0.496
310: 0.714 / 0.495
315: 1.095 / 0.487
320: 0.635 / 0.502
325: 1.129 / 0.471
330: 1.622 / 0.473
335: 1.010 / 0.465
340: 1.184 / 0.468
345: 0.686 / 0.511
350: 1.148 / 0.485
355: 1.093 / 0.461
360: 0.881 / 0.478
365: 0.864 / 0.452
370: 1.332 / 0.469
375: 0.793 / 0.472
380: 0.723 / 0.477
385: 0.836 / 0.472
390: 0.563 / 0.459
395: 0.837 / 0.445
400: 0.391 / 0.446
Rewirer cayley with seed 17
Using MSE Loss...
Minimum val loss at epoch 400: 1.8200342178344726
Final val loss at epoch 400: 1.8200342178344726
Train / Val loss by epoch
5: 67.101 / 515.285
10: 26.505 / 171.049
15: 18.456 / 129.784
20: 15.411 / 102.204
25: 9.451 / 83.942
30: 12.644 / 69.742
35: 9.043 / 59.735
40: 8.219 / 56.156
45: 8.261 / 44.872
50: 9.672 / 45.090
55: 5.707 / 40.241
60: 10.274 / 35.701
65: 7.215 / 33.205
70: 5.316 / 31.691
75: 5.498 / 27.458
80: 5.518 / 27.932
85: 4.383 / 26.479
90: 6.168 / 24.079
95: 5.361 / 24.070
100: 3.451 / 21.213
105: 3.835 / 19.458
110: 3.683 / 19.106
115: 2.855 / 16.482
120: 3.704 / 14.135
125: 5.524 / 15.708
130: 6.365 / 13.249
135: 2.779 / 11.550
140: 3.089 / 10.014
145: 3.485 / 9.718
150: 1.422 / 11.645
155: 4.705 / 7.687
160: 3.149 / 8.121
165: 1.561 / 8.829
170: 3.778 / 9.233
175: 3.576 / 6.871
180: 1.354 / 7.487
185: 2.114 / 7.049
190: 1.354 / 6.273
195: 2.032 / 7.409
200: 1.908 / 5.806
205: 1.237 / 5.804
210: 1.792 / 4.866
215: 1.614 / 6.241
220: 1.659 / 5.273
225: 1.578 / 5.165
230: 2.412 / 4.741
235: 2.688 / 5.003
240: 1.002 / 3.576
245: 1.266 / 3.930
250: 1.289 / 3.667
255: 1.465 / 3.805
260: 1.552 / 3.394
265: 0.974 / 2.750
270: 2.997 / 4.034
275: 2.285 / 3.394
280: 0.972 / 3.706
285: 3.518 / 3.437
290: 1.218 / 2.572
295: 0.628 / 3.190
300: 1.063 / 2.890
305: 0.958 / 2.868
310: 1.055 / 2.952
315: 2.082 / 2.848
320: 0.574 / 2.360
325: 0.535 / 2.168
330: 0.797 / 2.283
335: 1.256 / 2.491
340: 0.862 / 2.414
345: 1.662 / 2.636
350: 0.483 / 2.132
355: 1.563 / 2.543
360: 1.926 / 2.387
365: 0.643 / 2.169
370: 0.401 / 2.076
375: 0.649 / 2.431
380: 0.781 / 2.147
385: 0.711 / 2.297
390: 0.952 / 2.322
395: 0.875 / 2.140
400: 0.418 / 1.820
Rewirer cayley with seed 47
Using MSE Loss...
Minimum val loss at epoch 395: 1.6492727637290954
Final val loss at epoch 400: 1.7136608123779298
Train / Val loss by epoch
5: 24.884 / 76.662
10: 17.989 / 74.771
15: 16.889 / 71.442
20: 11.186 / 64.760
25: 7.339 / 61.773
30: 7.417 / 51.748
35: 6.490 / 49.921
40: 11.284 / 40.946
45: 6.749 / 40.571
50: 9.602 / 38.022
55: 5.867 / 35.764
60: 6.930 / 34.361
65: 4.000 / 36.280
70: 6.538 / 30.237
75: 3.302 / 28.078
80: 3.612 / 27.602
85: 4.849 / 28.016
90: 5.010 / 26.907
95: 2.780 / 26.552
100: 4.033 / 23.847
105: 3.660 / 22.239
110: 3.583 / 22.428
115: 1.873 / 20.590
120: 2.043 / 19.812
125: 2.212 / 17.204
130: 2.203 / 14.919
135: 2.219 / 15.875
140: 2.380 / 12.844
145: 2.297 / 13.616
150: 1.622 / 13.001
155: 1.838 / 13.053
160: 1.772 / 11.423
165: 1.077 / 11.233
170: 1.115 / 10.277
175: 1.326 / 9.989
180: 1.045 / 9.151
185: 2.492 / 8.453
190: 0.676 / 8.132
195: 1.390 / 6.664
200: 0.765 / 6.983
205: 1.483 / 7.062
210: 1.226 / 6.845
215: 1.166 / 6.505
220: 1.652 / 6.012
225: 1.225 / 5.314
230: 1.343 / 5.548
235: 1.373 / 5.558
240: 0.856 / 4.457
245: 1.187 / 4.391
250: 0.488 / 4.498
255: 0.910 / 3.902
260: 1.191 / 3.669
265: 0.676 / 3.375
270: 1.028 / 3.525
275: 0.625 / 3.311
280: 0.632 / 3.241
285: 0.856 / 2.896
290: 0.938 / 3.155
295: 0.627 / 2.940
300: 0.440 / 2.722
305: 1.008 / 2.638
310: 0.632 / 2.412
315: 0.482 / 2.726
320: 1.050 / 2.353
325: 0.988 / 2.150
330: 0.798 / 2.124
335: 0.875 / 2.071
340: 0.286 / 2.367
345: 0.526 / 2.492
350: 0.641 / 2.451
355: 0.346 / 2.428
360: 0.789 / 2.093
365: 0.378 / 2.337
370: 0.594 / 2.018
375: 0.581 / 2.120
380: 0.418 / 2.120
385: 0.477 / 1.966
390: 0.419 / 1.758
395: 0.524 / 1.649
400: 0.669 / 1.714
Rewirer cayley with seed 23
Using MSE Loss...
Minimum val loss at epoch 325: 0.5116586863994599
Final val loss at epoch 400: 0.5566842332482338
Train / Val loss by epoch
5: 23.404 / 16.588
10: 24.443 / 7.801
15: 14.825 / 4.515
20: 17.077 / 3.829
25: 6.779 / 3.466
30: 8.032 / 4.159
35: 7.582 / 3.274
40: 6.574 / 3.350
45: 8.286 / 3.032
50: 7.672 / 2.405
55: 4.152 / 2.867
60: 3.615 / 3.000
65: 5.740 / 2.599
70: 6.558 / 3.159
75: 4.047 / 2.727
80: 4.837 / 2.057
85: 3.347 / 1.429
90: 3.715 / 1.582
95: 2.369 / 1.561
100: 3.654 / 1.261
105: 2.291 / 1.199
110: 2.147 / 1.008
115: 1.695 / 1.119
120: 3.600 / 1.102
125: 2.630 / 0.885
130: 1.867 / 0.840
135: 2.222 / 0.915
140: 1.970 / 1.065
145: 1.842 / 1.004
150: 2.273 / 0.852
155: 2.315 / 0.742
160: 2.182 / 0.770
165: 1.370 / 0.834
170: 1.222 / 0.734
175: 1.408 / 0.839
180: 1.828 / 0.691
185: 1.533 / 0.667
190: 3.590 / 0.674
195: 1.148 / 0.670
200: 1.455 / 0.627
205: 1.201 / 0.687
210: 1.006 / 0.634
215: 1.719 / 0.615
220: 1.188 / 0.621
225: 1.130 / 0.616
230: 1.951 / 0.588
235: 1.177 / 0.600
240: 1.539 / 0.567
245: 1.112 / 0.560
250: 1.630 / 0.552
255: 0.793 / 0.589
260: 1.678 / 0.564
265: 2.233 / 0.563
270: 1.088 / 0.548
275: 1.960 / 0.540
280: 1.055 / 0.629
285: 0.929 / 0.595
290: 1.084 / 0.547
295: 1.306 / 0.565
300: 1.407 / 0.635
305: 1.017 / 0.548
310: 0.646 / 0.591
315: 0.896 / 0.526
320: 0.684 / 0.591
325: 0.944 / 0.512
330: 1.420 / 0.535
335: 0.969 / 0.541
340: 1.602 / 0.544
345: 0.839 / 0.615
350: 0.634 / 0.570
355: 1.053 / 0.525
360: 1.016 / 0.554
365: 1.354 / 0.526
370: 1.322 / 0.615
375: 1.037 / 0.605
380: 0.661 / 0.562
385: 0.776 / 0.609
390: 0.651 / 0.570
395: 0.895 / 0.526
400: 0.486 / 0.557
{'cayley': [{'best': 1.8200342178344726, 'end': 1.8200342178344726},
            {'best': 1.6492727637290954, 'end': 1.7136608123779298},
            {'best': 0.5116586863994599, 'end': 0.5566842332482338}],
 'cayley_clusters': [{'best': 1.159334772825241, 'end': 1.159334772825241},
                     {'best': 0.6896916598081588, 'end': 0.7737202554941177},
                     {'best': 0.44490463733673097, 'end': 0.445828965306282}]}
Loading data...
Number of training graphs: 1500
Train graph sizes: Counter({101: 46, 114: 45, 79: 44, 87: 40, 94: 39, 122: 39, 88: 37, 92: 37, 107: 36, 119: 36, 80: 34, 105: 34, 75: 33, 99: 33, 112: 33, 116: 33, 121: 33, 78: 32, 84: 32, 98: 32, 100: 32, 120: 32, 96: 31, 111: 31, 118: 31, 81: 29, 83: 28, 95: 28, 103: 28, 108: 28, 82: 27, 89: 27, 106: 27, 93: 26, 90: 26, 97: 26, 109: 25, 115: 25, 117: 25, 85: 24, 124: 24, 76: 22, 86: 22, 91: 22, 102: 22, 104: 22, 110: 22, 123: 22, 77: 19, 113: 19})
{25: [], 26: [], 27: [], 28: [], 29: [], 30: [], 31: [], 32: [], 33: [], 34: []}
Number of validation graphs: 1500
Val graph sizes: Counter({159: 12, 168: 11, 172: 11, 130: 9, 164: 9, 174: 9, 126: 8, 142: 8, 141: 8, 149: 8, 125: 7, 129: 7, 133: 7, 134: 7, 139: 7, 154: 7, 153: 7, 160: 7, 128: 6, 135: 6, 137: 6, 136: 6, 143: 6, 146: 6, 147: 6, 152: 6, 150: 6, 158: 6, 157: 6, 165: 6, 138: 5, 144: 5, 148: 5, 145: 5, 156: 5, 163: 5, 162: 5, 166: 5, 132: 4, 151: 4, 161: 4, 169: 4, 167: 4, 171: 4, 173: 4, 131: 3, 140: 3, 127: 2, 170: 2, 155: 1})
{'wandb': {'experiment_name': '', 'project': 'ColourInteract Sweeps Friday', 'entity': 'commute_opt_gnn'}, 'data': {'name': 'ColourInteract', 'dataset': 'LRGB', 'c1': 1.0, 'c2s': [0.01, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 100.0], 'num_colours': 4, 'train_size': 1500, 'val_size': 300, 'min_train_nodes': 75, 'max_train_nodes': 125, 'min_val_nodes': 125, 'max_val_nodes': 175, 'rewirers': ['cayley_clusters', 'cayley'], 'normalise': True, 'seed': 93, 'c2': 1.0}, 'model': {'hidden_channels': 8, 'num_layers': 5, 'drop_prob': 0.1, 'global_pool_aggr': 'global_mean_pool', 'approaches': ['only_original', 'interleave'], 'seeds': [17, 47, 23], 'approach': 'only_original', 'seed': 23}, 'train': {'lr': 0.0001, 'num_epochs': 400, 'print_every': 5, 'train_batch_size': 32, 'val_batch_size': 32}, 'run': {'silent': False}}
Device: cpu
No rewiring.
train targets: 39.43 +/- 5.791
val targets: 57.83 +/- 5.922
Rewirer None with seed 17
Using MSE Loss...
Minimum val loss at epoch 400: 11.918347549438476
Final val loss at epoch 400: 11.918347549438476
Train / Val loss by epoch
5: 303.094 / 1088.045
10: 29.476 / 183.942
15: 19.095 / 137.381
20: 13.907 / 115.882
25: 14.134 / 100.202
30: 12.588 / 82.588
35: 8.822 / 75.065
40: 10.975 / 68.453
45: 9.302 / 57.286
50: 10.110 / 57.229
55: 6.326 / 47.976
60: 12.036 / 46.212
65: 8.385 / 42.144
70: 5.206 / 40.667
75: 6.565 / 37.517
80: 7.557 / 36.366
85: 4.463 / 36.941
90: 6.079 / 36.755
95: 4.389 / 35.420
100: 6.929 / 31.782
105: 8.975 / 30.986
110: 4.663 / 34.837
115: 4.444 / 30.235
120: 5.552 / 26.176
125: 5.908 / 31.245
130: 8.542 / 28.433
135: 3.691 / 26.695
140: 3.219 / 24.842
145: 4.452 / 22.684
150: 4.610 / 28.178
155: 4.553 / 21.896
160: 5.759 / 22.037
165: 2.645 / 23.222
170: 5.567 / 25.895
175: 6.410 / 20.062
180: 2.856 / 21.137
185: 3.992 / 19.245
190: 2.717 / 19.732
195: 3.292 / 19.870
200: 2.712 / 19.216
205: 2.324 / 19.422
210: 2.739 / 18.057
215: 3.748 / 20.014
220: 2.980 / 18.959
225: 3.092 / 19.927
230: 3.151 / 17.841
235: 4.851 / 18.052
240: 3.128 / 16.027
245: 1.751 / 17.032
250: 1.899 / 15.787
255: 2.431 / 15.142
260: 1.481 / 16.581
265: 1.629 / 14.315
270: 3.230 / 17.063
275: 2.682 / 16.774
280: 1.551 / 17.169
285: 4.642 / 17.098
290: 2.761 / 15.241
295: 1.786 / 16.687
300: 2.028 / 17.662
305: 1.769 / 15.863
310: 2.382 / 17.009
315: 4.528 / 16.742
320: 1.958 / 15.118
325: 1.578 / 13.439
330: 1.860 / 15.113
335: 2.027 / 15.284
340: 1.496 / 13.843
345: 2.504 / 13.893
350: 1.339 / 12.568
355: 2.785 / 14.457
360: 3.306 / 13.807
365: 1.285 / 15.989
370: 1.113 / 13.114
375: 2.047 / 13.692
380: 1.854 / 14.759
385: 1.845 / 13.870
390: 1.620 / 13.729
395: 1.748 / 14.477
400: 1.979 / 11.918
Rewirer None with seed 47
Using MSE Loss...
Minimum val loss at epoch 195: 14.536898136138916
Final val loss at epoch 400: 22.60292339324951
Train / Val loss by epoch
5: 18.044 / 62.734
10: 14.855 / 70.487
15: 13.366 / 76.141
20: 9.950 / 66.964
25: 7.185 / 68.206
30: 5.928 / 54.441
35: 4.308 / 59.137
40: 8.802 / 53.498
45: 6.209 / 47.828
50: 8.073 / 45.486
55: 6.103 / 42.720
60: 8.893 / 40.823
65: 4.539 / 39.979
70: 5.164 / 36.205
75: 4.051 / 32.392
80: 4.894 / 29.785
85: 3.544 / 27.884
90: 5.080 / 27.782
95: 3.927 / 28.496
100: 4.019 / 23.634
105: 5.035 / 24.457
110: 5.361 / 23.192
115: 2.440 / 22.271
120: 3.461 / 20.692
125: 3.267 / 18.877
130: 2.914 / 18.879
135: 3.264 / 19.145
140: 2.868 / 18.438
145: 2.063 / 16.493
150: 2.160 / 19.310
155: 3.321 / 18.239
160: 1.651 / 18.482
165: 2.346 / 16.976
170: 1.691 / 16.748
175: 2.028 / 16.401
180: 2.470 / 15.608
185: 4.139 / 15.544
190: 1.762 / 17.407
195: 2.493 / 14.537
200: 1.892 / 16.836
205: 2.933 / 15.214
210: 2.347 / 18.063
215: 2.023 / 16.925
220: 3.279 / 15.853
225: 1.466 / 16.219
230: 2.071 / 17.113
235: 1.983 / 16.700
240: 1.718 / 16.823
245: 2.875 / 17.260
250: 1.287 / 16.478
255: 2.327 / 18.561
260: 2.463 / 18.526
265: 1.761 / 18.431
270: 2.139 / 16.738
275: 1.573 / 18.610
280: 2.040 / 17.446
285: 1.965 / 18.914
290: 2.684 / 18.460
295: 1.050 / 17.574
300: 1.699 / 17.687
305: 2.554 / 17.858
310: 1.719 / 18.132
315: 1.538 / 21.250
320: 1.389 / 18.794
325: 1.526 / 18.343
330: 1.849 / 20.405
335: 2.252 / 19.818
340: 0.887 / 20.930
345: 1.141 / 19.844
350: 1.806 / 21.217
355: 1.005 / 21.267
360: 2.240 / 20.403
365: 1.046 / 20.566
370: 1.415 / 19.441
375: 2.078 / 20.169
380: 1.302 / 22.323
385: 1.431 / 21.126
390: 1.199 / 21.841
395: 1.411 / 20.721
400: 1.368 / 22.603
Rewirer None with seed 23
Using MSE Loss...
Minimum val loss at epoch 280: 1.3577342838048936
Final val loss at epoch 400: 1.8898409008979797
Train / Val loss by epoch
5: 22.378 / 11.952
10: 18.357 / 8.502
15: 10.254 / 7.428
20: 18.071 / 5.670
25: 8.961 / 6.359
30: 7.752 / 5.297
35: 8.809 / 5.010
40: 4.713 / 5.008
45: 6.277 / 4.316
50: 9.019 / 3.813
55: 5.503 / 3.273
60: 4.134 / 3.091
65: 5.888 / 3.138
70: 6.003 / 2.773
75: 4.924 / 2.638
80: 3.942 / 2.512
85: 4.111 / 2.392
90: 4.433 / 2.382
95: 3.548 / 2.318
100: 4.057 / 2.264
105: 3.432 / 2.199
110: 3.940 / 2.119
115: 4.087 / 2.108
120: 4.006 / 2.036
125: 3.354 / 2.032
130: 3.344 / 1.990
135: 3.012 / 1.922
140: 3.228 / 1.890
145: 2.471 / 1.877
150: 2.868 / 1.825
155: 3.906 / 1.778
160: 4.566 / 1.726
165: 3.468 / 1.696
170: 3.150 / 1.665
175: 2.179 / 1.661
180: 3.553 / 1.626
185: 2.372 / 1.727
190: 4.744 / 1.655
195: 2.431 / 1.588
200: 2.272 / 1.581
205: 3.835 / 1.509
210: 2.080 / 1.504
215: 3.613 / 1.481
220: 3.391 / 1.451
225: 2.939 / 1.461
230: 2.169 / 1.436
235: 2.077 / 1.463
240: 3.547 / 1.474
245: 2.717 / 1.438
250: 3.126 / 1.500
255: 1.709 / 1.454
260: 2.623 / 1.379
265: 3.561 / 1.437
270: 2.007 / 1.533
275: 3.248 / 1.550
280: 1.877 / 1.358
285: 2.744 / 1.403
290: 2.738 / 1.625
295: 2.093 / 1.470
300: 1.968 / 1.392
305: 1.840 / 1.705
310: 1.732 / 1.460
315: 2.450 / 1.641
320: 2.074 / 1.595
325: 2.417 / 1.783
330: 1.978 / 1.602
335: 1.439 / 1.708
340: 2.240 / 1.642
345: 2.439 / 1.482
350: 1.993 / 1.751
355: 1.985 / 2.132
360: 1.685 / 1.728
365: 2.395 / 2.109
370: 2.367 / 1.735
375: 2.055 / 1.934
380: 2.638 / 2.336
385: 1.798 / 2.055
390: 1.387 / 2.029
395: 2.054 / 1.812
400: 1.143 / 1.890
{None: [{'best': 11.918347549438476, 'end': 11.918347549438476},
        {'best': 14.536898136138916, 'end': 22.60292339324951},
        {'best': 1.3577342838048936, 'end': 1.8898409008979797}]}
{'wandb': {'experiment_name': '', 'project': 'ColourInteract Sweeps Friday', 'entity': 'commute_opt_gnn'}, 'data': {'name': 'ColourInteract', 'dataset': 'LRGB', 'c1': 1.0, 'c2s': [0.01, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 100.0], 'num_colours': 4, 'train_size': 1500, 'val_size': 300, 'min_train_nodes': 75, 'max_train_nodes': 125, 'min_val_nodes': 125, 'max_val_nodes': 175, 'rewirers': ['cayley_clusters', 'cayley'], 'normalise': True, 'seed': 93, 'c2': 1.0}, 'model': {'hidden_channels': 8, 'num_layers': 5, 'drop_prob': 0.1, 'global_pool_aggr': 'global_mean_pool', 'approaches': ['only_original', 'interleave'], 'seeds': [17, 47, 23], 'approach': 'interleave', 'seed': 23}, 'train': {'lr': 0.0001, 'num_epochs': 400, 'print_every': 5, 'train_batch_size': 32, 'val_batch_size': 32}, 'run': {'silent': False}}
Device: cpu
train targets: 39.43 +/- 5.791
val targets: 57.83 +/- 5.922
Rewirer cayley_clusters with seed 17
Using MSE Loss...
Minimum val loss at epoch 265: 7.457056140899658
Final val loss at epoch 400: 8.25535249710083
Train / Val loss by epoch
5: 287.570 / 956.985
10: 42.451 / 138.675
15: 17.999 / 112.990
20: 20.117 / 95.451
25: 11.885 / 80.199
30: 14.060 / 69.864
35: 9.493 / 61.135
40: 13.705 / 58.913
45: 12.508 / 47.028
50: 13.984 / 45.158
55: 11.806 / 39.676
60: 10.995 / 38.678
65: 8.804 / 34.958
70: 7.129 / 34.361
75: 8.561 / 26.580
80: 10.054 / 27.613
85: 6.178 / 25.941
90: 6.745 / 28.746
95: 5.478 / 28.627
100: 3.896 / 23.321
105: 6.918 / 22.980
110: 3.732 / 23.268
115: 4.623 / 19.765
120: 3.738 / 17.936
125: 6.881 / 20.784
130: 6.366 / 18.686
135: 3.541 / 16.191
140: 3.775 / 15.385
145: 4.380 / 13.264
150: 3.945 / 16.196
155: 3.758 / 13.273
160: 5.546 / 14.292
165: 2.588 / 12.526
170: 3.323 / 14.744
175: 5.484 / 10.824
180: 3.488 / 12.128
185: 3.206 / 11.901
190: 2.754 / 10.668
195: 2.501 / 11.253
200: 3.027 / 9.912
205: 2.230 / 9.458
210: 3.519 / 9.031
215: 3.939 / 10.401
220: 3.092 / 9.978
225: 2.897 / 9.009
230: 3.557 / 8.929
235: 4.298 / 10.081
240: 2.400 / 7.930
245: 1.957 / 8.736
250: 1.954 / 8.518
255: 1.639 / 8.587
260: 1.873 / 7.767
265: 1.521 / 7.457
270: 3.730 / 8.840
275: 2.040 / 8.305
280: 1.782 / 8.685
285: 2.766 / 8.152
290: 2.499 / 7.967
295: 2.434 / 10.066
300: 1.787 / 9.412
305: 1.942 / 8.731
310: 1.952 / 9.328
315: 4.341 / 10.391
320: 2.104 / 8.745
325: 1.490 / 8.642
330: 2.160 / 8.598
335: 1.797 / 8.759
340: 2.913 / 8.448
345: 2.353 / 8.145
350: 1.911 / 8.658
355: 2.364 / 9.979
360: 2.764 / 9.203
365: 1.303 / 10.422
370: 1.139 / 9.591
375: 1.440 / 9.694
380: 2.505 / 9.198
385: 1.618 / 9.191
390: 1.567 / 9.423
395: 1.555 / 10.000
400: 1.569 / 8.255
Rewirer cayley_clusters with seed 47
Using MSE Loss...
Minimum val loss at epoch 195: 8.33939757347107
Final val loss at epoch 400: 13.386022472381592
Train / Val loss by epoch
5: 18.543 / 22.884
10: 17.229 / 24.675
15: 14.161 / 28.725
20: 12.286 / 25.413
25: 7.775 / 28.250
30: 5.430 / 22.501
35: 6.922 / 28.564
40: 8.128 / 22.860
45: 6.992 / 23.429
50: 8.289 / 24.897
55: 3.456 / 24.349
60: 8.704 / 24.967
65: 4.832 / 27.028
70: 5.667 / 23.300
75: 3.279 / 22.478
80: 4.375 / 22.960
85: 4.065 / 21.485
90: 6.259 / 19.492
95: 3.631 / 20.515
100: 3.268 / 17.918
105: 5.062 / 16.729
110: 3.290 / 16.574
115: 1.966 / 15.123
120: 2.778 / 14.824
125: 2.693 / 13.015
130: 3.014 / 12.356
135: 3.433 / 12.268
140: 2.193 / 11.307
145: 2.150 / 10.410
150: 2.912 / 11.692
155: 2.356 / 11.489
160: 1.968 / 9.759
165: 2.379 / 9.782
170: 2.181 / 9.547
175: 2.704 / 10.245
180: 2.101 / 10.407
185: 3.622 / 10.332
190: 1.524 / 10.546
195: 2.009 / 8.339
200: 1.458 / 9.827
205: 2.944 / 9.154
210: 1.422 / 10.701
215: 1.881 / 10.843
220: 2.944 / 9.717
225: 1.669 / 9.134
230: 2.419 / 10.304
235: 1.405 / 10.573
240: 2.232 / 10.590
245: 2.201 / 9.571
250: 1.936 / 9.702
255: 2.326 / 10.261
260: 2.293 / 11.116
265: 1.875 / 10.501
270: 2.177 / 10.783
275: 1.228 / 10.715
280: 1.677 / 10.837
285: 1.557 / 10.851
290: 2.077 / 11.302
295: 1.552 / 11.180
300: 1.384 / 10.886
305: 1.931 / 11.300
310: 1.745 / 11.384
315: 1.312 / 12.513
320: 2.405 / 11.094
325: 2.130 / 11.601
330: 1.609 / 12.646
335: 2.069 / 11.721
340: 0.969 / 12.760
345: 1.245 / 13.807
350: 1.252 / 12.894
355: 1.322 / 13.467
360: 2.083 / 13.400
365: 0.909 / 13.038
370: 1.450 / 11.882
375: 1.689 / 13.078
380: 1.184 / 13.907
385: 1.035 / 14.490
390: 1.053 / 13.515
395: 1.482 / 13.019
400: 1.458 / 13.386
Rewirer cayley_clusters with seed 23
Using MSE Loss...
Minimum val loss at epoch 215: 1.6996383130550385
Final val loss at epoch 400: 3.1567949771881105
Train / Val loss by epoch
5: 18.529 / 14.832
10: 25.622 / 10.024
15: 15.890 / 7.976
20: 18.779 / 6.696
25: 8.262 / 6.178
30: 9.689 / 5.368
35: 7.090 / 5.344
40: 5.828 / 4.901
45: 4.537 / 4.492
50: 11.674 / 4.268
55: 3.442 / 3.654
60: 3.922 / 3.471
65: 7.906 / 3.625
70: 7.795 / 3.156
75: 4.328 / 2.993
80: 4.488 / 2.880
85: 2.599 / 2.786
90: 5.457 / 2.902
95: 4.964 / 2.569
100: 3.119 / 2.623
105: 2.951 / 2.659
110: 3.726 / 2.625
115: 2.281 / 2.389
120: 5.369 / 2.316
125: 3.982 / 2.562
130: 3.106 / 2.477
135: 4.483 / 2.372
140: 2.868 / 2.150
145: 2.160 / 2.075
150: 3.504 / 2.386
155: 3.554 / 1.965
160: 3.591 / 2.042
165: 2.818 / 1.970
170: 2.640 / 2.073
175: 2.659 / 1.893
180: 3.032 / 2.110
185: 2.271 / 2.440
190: 5.877 / 2.290
195: 1.847 / 2.015
200: 2.212 / 2.014
205: 3.503 / 2.181
210: 1.931 / 2.016
215: 3.267 / 1.700
220: 3.451 / 1.830
225: 2.511 / 1.971
230: 3.279 / 1.834
235: 1.419 / 1.940
240: 3.517 / 2.326
245: 2.379 / 1.933
250: 2.785 / 1.903
255: 2.047 / 1.846
260: 3.092 / 1.854
265: 2.978 / 2.108
270: 2.307 / 2.452
275: 4.662 / 2.530
280: 2.394 / 1.826
285: 2.164 / 1.976
290: 2.291 / 2.562
295: 1.727 / 1.942
300: 2.100 / 1.857
305: 2.153 / 2.563
310: 1.450 / 2.340
315: 2.378 / 2.636
320: 1.298 / 2.455
325: 2.057 / 2.900
330: 2.166 / 2.467
335: 2.232 / 2.583
340: 2.667 / 2.609
345: 1.564 / 2.538
350: 2.087 / 2.710
355: 1.337 / 3.048
360: 2.087 / 2.791
365: 2.055 / 3.048
370: 2.418 / 3.102
375: 1.624 / 2.888
380: 2.123 / 3.322
385: 1.657 / 3.087
390: 1.323 / 3.062
395: 2.606 / 2.941
400: 1.213 / 3.157
Rewirer cayley with seed 17
Using MSE Loss...
Minimum val loss at epoch 350: 11.025555992126465
Final val loss at epoch 400: 11.290830326080322
Train / Val loss by epoch
5: 278.534 / 1333.742
10: 32.164 / 267.250
15: 21.188 / 203.500
20: 17.899 / 164.020
25: 12.068 / 136.339
30: 14.517 / 115.080
35: 11.607 / 100.503
40: 10.590 / 94.327
45: 12.112 / 77.784
50: 13.594 / 79.429
55: 6.674 / 70.296
60: 13.135 / 63.426
65: 8.375 / 59.770
70: 7.087 / 57.156
75: 8.023 / 49.679
80: 8.929 / 51.312
85: 5.730 / 48.608
90: 10.423 / 47.268
95: 6.987 / 47.381
100: 4.617 / 41.777
105: 5.694 / 42.510
110: 4.779 / 39.790
115: 4.038 / 35.829
120: 5.509 / 31.074
125: 7.123 / 34.432
130: 7.911 / 29.884
135: 4.615 / 26.309
140: 3.663 / 24.499
145: 6.853 / 23.769
150: 2.917 / 26.959
155: 7.316 / 20.007
160: 5.310 / 22.389
165: 2.849 / 21.258
170: 4.861 / 21.289
175: 5.728 / 18.793
180: 3.581 / 19.065
185: 4.210 / 19.189
190: 2.605 / 16.781
195: 2.488 / 17.600
200: 2.686 / 16.038
205: 1.818 / 16.315
210: 3.951 / 13.382
215: 2.876 / 16.888
220: 2.517 / 14.932
225: 2.540 / 15.524
230: 2.582 / 14.295
235: 4.051 / 15.323
240: 2.627 / 12.688
245: 1.978 / 13.849
250: 2.040 / 12.329
255: 3.145 / 12.640
260: 1.771 / 12.222
265: 2.084 / 11.628
270: 4.601 / 13.809
275: 2.839 / 13.362
280: 1.637 / 14.541
285: 4.697 / 13.516
290: 2.137 / 11.922
295: 1.500 / 12.682
300: 2.382 / 12.970
305: 1.712 / 11.838
310: 2.041 / 12.826
315: 4.299 / 12.477
320: 1.570 / 11.478
325: 1.033 / 11.440
330: 2.115 / 12.119
335: 2.204 / 11.799
340: 1.712 / 11.593
345: 3.265 / 12.145
350: 1.364 / 11.026
355: 2.927 / 12.515
360: 3.901 / 12.556
365: 1.605 / 12.080
370: 1.194 / 11.800
375: 1.551 / 12.977
380: 1.719 / 12.992
385: 1.664 / 12.039
390: 1.987 / 12.644
395: 2.239 / 13.071
400: 1.894 / 11.291
Rewirer cayley with seed 47
Using MSE Loss...
Minimum val loss at epoch 195: 12.901735305786133
Final val loss at epoch 400: 18.08298454284668
Train / Val loss by epoch
5: 28.941 / 86.759
10: 16.591 / 78.498
15: 18.018 / 84.575
20: 10.693 / 67.612
25: 7.418 / 68.791
30: 8.453 / 55.734
35: 6.475 / 60.998
40: 11.889 / 48.323
45: 7.783 / 47.165
50: 11.069 / 44.751
55: 5.473 / 42.616
60: 8.740 / 42.044
65: 5.174 / 43.526
70: 6.976 / 37.110
75: 4.551 / 33.761
80: 3.920 / 32.195
85: 4.524 / 31.559
90: 7.203 / 28.459
95: 2.931 / 29.667
100: 4.338 / 26.406
105: 5.022 / 24.943
110: 3.676 / 24.306
115: 2.769 / 22.390
120: 3.062 / 22.458
125: 2.369 / 20.213
130: 3.927 / 17.962
135: 3.163 / 19.238
140: 3.378 / 16.251
145: 3.536 / 16.387
150: 3.412 / 17.197
155: 3.279 / 17.273
160: 2.604 / 15.737
165: 1.812 / 15.391
170: 1.881 / 14.376
175: 2.492 / 15.661
180: 1.830 / 14.440
185: 3.651 / 14.582
190: 1.736 / 15.102
195: 3.122 / 12.902
200: 1.720 / 14.708
205: 3.170 / 14.126
210: 1.638 / 15.057
215: 2.216 / 15.245
220: 3.579 / 14.001
225: 2.812 / 13.540
230: 1.920 / 15.008
235: 2.224 / 15.681
240: 1.852 / 15.707
245: 2.803 / 15.879
250: 1.021 / 16.250
255: 2.710 / 15.610
260: 2.951 / 15.051
265: 1.937 / 15.597
270: 2.411 / 15.086
275: 1.812 / 16.135
280: 2.009 / 15.321
285: 2.012 / 15.278
290: 2.007 / 16.307
295: 1.852 / 16.004
300: 1.322 / 15.736
305: 2.547 / 16.635
310: 1.910 / 16.185
315: 1.605 / 18.697
320: 2.881 / 15.636
325: 1.630 / 16.650
330: 2.087 / 17.567
335: 2.333 / 16.418
340: 1.262 / 17.925
345: 1.324 / 18.257
350: 1.732 / 18.340
355: 1.020 / 19.620
360: 1.987 / 17.917
365: 0.882 / 18.715
370: 1.390 / 16.858
375: 1.741 / 18.086
380: 1.163 / 18.903
385: 1.227 / 18.288
390: 1.185 / 17.678
395: 1.642 / 17.737
400: 2.155 / 18.083
Rewirer cayley with seed 23
Using MSE Loss...
Minimum val loss at epoch 280: 1.5830759942531585
Final val loss at epoch 400: 2.241704523563385
Train / Val loss by epoch
5: 20.075 / 17.769
10: 23.996 / 9.215
15: 14.762 / 7.067
20: 15.891 / 5.584
25: 8.507 / 5.166
30: 9.027 / 4.710
35: 8.282 / 4.498
40: 6.452 / 4.063
45: 8.408 / 3.767
50: 9.227 / 3.669
55: 4.265 / 3.282
60: 3.824 / 3.333
65: 7.050 / 3.040
70: 6.874 / 3.204
75: 5.535 / 3.001
80: 4.959 / 2.716
85: 4.199 / 2.524
90: 4.331 / 2.484
95: 4.220 / 2.537
100: 5.226 / 2.444
105: 2.917 / 2.381
110: 3.031 / 2.294
115: 2.447 / 2.290
120: 6.064 / 2.273
125: 3.137 / 2.191
130: 3.177 / 2.237
135: 3.594 / 2.165
140: 3.663 / 2.100
145: 2.709 / 2.113
150: 2.672 / 2.069
155: 3.109 / 2.000
160: 3.620 / 1.989
165: 3.935 / 2.001
170: 2.019 / 1.956
175: 2.604 / 1.946
180: 3.007 / 1.887
185: 3.442 / 2.088
190: 5.008 / 2.088
195: 1.542 / 1.949
200: 2.573 / 1.895
205: 2.740 / 1.823
210: 1.792 / 1.887
215: 2.860 / 1.737
220: 2.538 / 1.753
225: 1.815 / 1.789
230: 2.440 / 1.812
235: 2.023 / 1.862
240: 3.081 / 1.845
245: 2.420 / 1.749
250: 2.352 / 1.795
255: 1.671 / 1.631
260: 2.914 / 1.728
265: 3.614 / 1.807
270: 2.068 / 1.892
275: 3.952 / 2.005
280: 2.569 / 1.583
285: 2.396 / 1.633
290: 2.175 / 1.940
295: 2.469 / 1.840
300: 2.194 / 1.734
305: 1.927 / 2.002
310: 1.640 / 1.809
315: 2.856 / 2.054
320: 1.532 / 1.973
325: 1.728 / 2.379
330: 1.821 / 1.947
335: 1.874 / 2.265
340: 2.542 / 2.054
345: 2.742 / 1.804
350: 1.394 / 2.155
355: 1.644 / 2.387
360: 1.687 / 1.999
365: 1.865 / 2.323
370: 2.516 / 2.228
375: 1.813 / 2.048
380: 2.060 / 2.352
385: 1.401 / 2.249
390: 1.529 / 2.333
395: 2.140 / 2.346
400: 1.095 / 2.242
{'cayley': [{'best': 11.025555992126465, 'end': 11.290830326080322},
            {'best': 12.901735305786133, 'end': 18.08298454284668},
            {'best': 1.5830759942531585, 'end': 2.241704523563385}],
 'cayley_clusters': [{'best': 7.457056140899658, 'end': 8.25535249710083},
                     {'best': 8.33939757347107, 'end': 13.386022472381592},
                     {'best': 1.6996383130550385, 'end': 3.1567949771881105}]}
Loading data...
Number of training graphs: 1500
Train graph sizes: Counter({101: 46, 114: 45, 79: 44, 87: 40, 94: 39, 122: 39, 88: 37, 92: 37, 107: 36, 119: 36, 80: 34, 105: 34, 75: 33, 99: 33, 112: 33, 116: 33, 121: 33, 78: 32, 84: 32, 98: 32, 100: 32, 120: 32, 96: 31, 111: 31, 118: 31, 81: 29, 83: 28, 95: 28, 103: 28, 108: 28, 82: 27, 89: 27, 106: 27, 93: 26, 90: 26, 97: 26, 109: 25, 115: 25, 117: 25, 85: 24, 124: 24, 76: 22, 86: 22, 91: 22, 102: 22, 104: 22, 110: 22, 123: 22, 77: 19, 113: 19})
{25: [], 26: [], 27: [], 28: [], 29: [], 30: [], 31: [], 32: [], 33: [], 34: []}
Number of validation graphs: 1500
Val graph sizes: Counter({159: 12, 168: 11, 172: 11, 130: 9, 164: 9, 174: 9, 126: 8, 142: 8, 141: 8, 149: 8, 125: 7, 129: 7, 133: 7, 134: 7, 139: 7, 154: 7, 153: 7, 160: 7, 128: 6, 135: 6, 137: 6, 136: 6, 143: 6, 146: 6, 147: 6, 152: 6, 150: 6, 158: 6, 157: 6, 165: 6, 138: 5, 144: 5, 148: 5, 145: 5, 156: 5, 163: 5, 162: 5, 166: 5, 132: 4, 151: 4, 161: 4, 169: 4, 167: 4, 171: 4, 173: 4, 131: 3, 140: 3, 127: 2, 170: 2, 155: 1})
{'wandb': {'experiment_name': '', 'project': 'ColourInteract Sweeps Friday', 'entity': 'commute_opt_gnn'}, 'data': {'name': 'ColourInteract', 'dataset': 'LRGB', 'c1': 1.0, 'c2s': [0.01, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 100.0], 'num_colours': 4, 'train_size': 1500, 'val_size': 300, 'min_train_nodes': 75, 'max_train_nodes': 125, 'min_val_nodes': 125, 'max_val_nodes': 175, 'rewirers': ['cayley_clusters', 'cayley'], 'normalise': True, 'seed': 93, 'c2': 2.0}, 'model': {'hidden_channels': 8, 'num_layers': 5, 'drop_prob': 0.1, 'global_pool_aggr': 'global_mean_pool', 'approaches': ['only_original', 'interleave'], 'seeds': [17, 47, 23], 'approach': 'only_original', 'seed': 23}, 'train': {'lr': 0.0001, 'num_epochs': 400, 'print_every': 5, 'train_batch_size': 32, 'val_batch_size': 32}, 'run': {'silent': False}}
Device: cpu
No rewiring.
train targets: 75.85 +/- 11.526
val targets: 112.65 +/- 11.774
Rewirer None with seed 17
Using MSE Loss...
Minimum val loss at epoch 325: 47.517055130004884
Final val loss at epoch 400: 55.3301815032959
Train / Val loss by epoch
5: 1982.743 / 4871.832
10: 47.485 / 521.197
15: 30.767 / 312.606
20: 23.391 / 271.101
25: 29.816 / 236.348
30: 22.313 / 206.171
35: 18.845 / 191.921
40: 18.026 / 188.257
45: 20.090 / 163.331
50: 23.938 / 167.139
55: 13.033 / 143.006
60: 19.033 / 141.512
65: 13.538 / 130.421
70: 10.567 / 119.387
75: 12.086 / 109.673
80: 18.532 / 103.300
85: 9.767 / 100.633
90: 13.078 / 101.061
95: 7.794 / 92.821
100: 10.857 / 83.378
105: 17.263 / 85.263
110: 9.578 / 87.429
115: 9.610 / 76.804
120: 10.231 / 69.291
125: 13.046 / 80.129
130: 12.124 / 72.377
135: 9.134 / 68.256
140: 8.680 / 66.080
145: 13.797 / 63.476
150: 10.257 / 70.035
155: 9.113 / 60.014
160: 14.259 / 60.104
165: 7.758 / 59.726
170: 8.473 / 68.904
175: 16.036 / 63.317
180: 6.963 / 60.210
185: 10.283 / 61.100
190: 6.616 / 56.080
195: 9.166 / 56.595
200: 7.500 / 55.207
205: 6.253 / 55.430
210: 8.625 / 53.099
215: 10.110 / 58.732
220: 5.984 / 55.656
225: 7.460 / 57.610
230: 8.288 / 50.112
235: 10.474 / 54.570
240: 8.894 / 53.035
245: 3.735 / 53.192
250: 5.839 / 48.299
255: 8.474 / 48.029
260: 5.721 / 49.972
265: 5.886 / 49.814
270: 8.232 / 50.816
275: 5.988 / 51.615
280: 5.122 / 54.492
285: 6.108 / 54.244
290: 6.019 / 48.937
295: 5.557 / 50.892
300: 7.188 / 58.237
305: 3.278 / 49.810
310: 7.542 / 52.387
315: 9.545 / 53.514
320: 6.415 / 51.583
325: 6.719 / 47.517
330: 7.298 / 47.791
335: 7.129 / 50.874
340: 5.447 / 52.793
345: 7.579 / 48.797
350: 6.780 / 48.168
355: 6.212 / 54.253
360: 9.720 / 52.047
365: 5.979 / 56.892
370: 3.215 / 52.384
375: 7.400 / 51.902
380: 6.520 / 57.270
385: 6.832 / 54.162
390: 6.164 / 53.973
395: 5.873 / 59.104
400: 8.011 / 55.330
Rewirer None with seed 47
Using MSE Loss...
Minimum val loss at epoch 145: 94.9193229675293
Final val loss at epoch 400: 134.06820678710938
Train / Val loss by epoch
5: 34.811 / 155.648
10: 29.263 / 176.600
15: 28.804 / 190.523
20: 21.604 / 178.903
25: 15.234 / 190.474
30: 9.672 / 164.066
35: 9.144 / 177.519
40: 20.670 / 167.012
45: 15.103 / 163.445
50: 18.490 / 155.418
55: 18.051 / 147.229
60: 22.388 / 145.089
65: 12.834 / 157.211
70: 14.552 / 138.308
75: 11.275 / 135.906
80: 11.818 / 126.397
85: 11.971 / 123.812
90: 12.593 / 123.476
95: 12.682 / 117.158
100: 13.142 / 106.018
105: 16.568 / 120.209
110: 14.850 / 108.063
115: 8.444 / 113.046
120: 8.228 / 111.392
125: 9.880 / 102.395
130: 11.065 / 97.688
135: 11.914 / 107.052
140: 11.184 / 101.188
145: 6.796 / 94.919
150: 7.356 / 106.418
155: 10.383 / 109.320
160: 4.629 / 108.277
165: 6.898 / 101.748
170: 6.391 / 102.395
175: 6.686 / 105.039
180: 7.507 / 105.328
185: 11.550 / 105.497
190: 5.755 / 112.113
195: 7.694 / 104.031
200: 4.802 / 111.433
205: 9.279 / 106.350
210: 5.931 / 115.848
215: 6.514 / 111.691
220: 9.619 / 110.158
225: 5.271 / 107.412
230: 7.459 / 113.546
235: 7.335 / 120.084
240: 6.202 / 118.241
245: 10.062 / 109.366
250: 4.129 / 117.229
255: 8.106 / 120.144
260: 8.717 / 122.804
265: 6.372 / 117.212
270: 8.568 / 112.722
275: 5.117 / 123.683
280: 6.322 / 117.520
285: 6.688 / 121.293
290: 8.705 / 119.964
295: 4.930 / 116.193
300: 5.616 / 115.893
305: 6.445 / 120.064
310: 5.563 / 118.297
315: 6.084 / 127.841
320: 5.619 / 124.266
325: 5.809 / 127.250
330: 6.292 / 127.534
335: 7.963 / 129.123
340: 3.961 / 135.182
345: 3.911 / 125.481
350: 4.319 / 131.186
355: 3.877 / 128.354
360: 8.481 / 128.177
365: 3.812 / 130.473
370: 4.679 / 127.140
375: 5.643 / 132.600
380: 5.546 / 134.144
385: 6.603 / 130.957
390: 4.963 / 140.039
395: 5.163 / 127.414
400: 5.336 / 134.068
Rewirer None with seed 23
Using MSE Loss...
Minimum val loss at epoch 180: 9.916394329071045
Final val loss at epoch 400: 26.349783897399902
Train / Val loss by epoch
5: 39.265 / 73.257
10: 33.667 / 57.926
15: 19.588 / 43.166
20: 32.162 / 35.735
25: 19.811 / 34.105
30: 18.186 / 27.261
35: 20.221 / 24.202
40: 10.480 / 25.334
45: 12.874 / 23.530
50: 19.223 / 21.374
55: 12.597 / 16.373
60: 9.185 / 16.238
65: 11.944 / 16.321
70: 11.742 / 14.465
75: 9.498 / 12.420
80: 12.264 / 11.634
85: 10.417 / 12.344
90: 9.724 / 12.799
95: 11.880 / 11.773
100: 11.413 / 12.942
105: 8.826 / 12.109
110: 11.083 / 12.977
115: 11.631 / 11.453
120: 8.530 / 11.556
125: 9.304 / 12.600
130: 9.860 / 11.437
135: 7.549 / 11.904
140: 7.825 / 11.504
145: 6.578 / 10.572
150: 6.097 / 11.653
155: 9.118 / 10.713
160: 10.340 / 10.188
165: 11.093 / 10.492
170: 8.774 / 10.482
175: 5.964 / 11.859
180: 9.114 / 9.916
185: 7.041 / 13.398
190: 11.225 / 13.214
195: 6.659 / 11.429
200: 4.603 / 12.716
205: 11.017 / 11.899
210: 5.269 / 11.270
215: 9.161 / 11.439
220: 10.113 / 12.783
225: 6.867 / 11.933
230: 7.481 / 13.201
235: 7.562 / 14.100
240: 8.571 / 14.272
245: 8.016 / 14.269
250: 7.221 / 14.615
255: 5.288 / 14.213
260: 9.313 / 13.638
265: 8.063 / 15.619
270: 7.440 / 16.634
275: 8.681 / 16.984
280: 6.856 / 14.515
285: 9.383 / 15.668
290: 7.256 / 17.653
295: 5.410 / 15.224
300: 6.230 / 15.741
305: 6.380 / 18.278
310: 5.413 / 17.604
315: 7.141 / 18.458
320: 6.512 / 19.230
325: 5.215 / 20.397
330: 3.678 / 20.481
335: 4.715 / 22.136
340: 6.243 / 20.319
345: 8.161 / 20.416
350: 5.379 / 23.065
355: 3.720 / 24.804
360: 6.009 / 21.468
365: 4.945 / 24.000
370: 7.474 / 22.395
375: 3.641 / 23.181
380: 6.167 / 27.711
385: 5.615 / 27.820
390: 4.341 / 27.804
395: 7.110 / 24.762
400: 3.946 / 26.350
{None: [{'best': 47.517055130004884, 'end': 55.3301815032959},
        {'best': 94.9193229675293, 'end': 134.06820678710938},
        {'best': 9.916394329071045, 'end': 26.349783897399902}]}
{'wandb': {'experiment_name': '', 'project': 'ColourInteract Sweeps Friday', 'entity': 'commute_opt_gnn'}, 'data': {'name': 'ColourInteract', 'dataset': 'LRGB', 'c1': 1.0, 'c2s': [0.01, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 100.0], 'num_colours': 4, 'train_size': 1500, 'val_size': 300, 'min_train_nodes': 75, 'max_train_nodes': 125, 'min_val_nodes': 125, 'max_val_nodes': 175, 'rewirers': ['cayley_clusters', 'cayley'], 'normalise': True, 'seed': 93, 'c2': 2.0}, 'model': {'hidden_channels': 8, 'num_layers': 5, 'drop_prob': 0.1, 'global_pool_aggr': 'global_mean_pool', 'approaches': ['only_original', 'interleave'], 'seeds': [17, 47, 23], 'approach': 'interleave', 'seed': 23}, 'train': {'lr': 0.0001, 'num_epochs': 400, 'print_every': 5, 'train_batch_size': 32, 'val_batch_size': 32}, 'run': {'silent': False}}
Device: cpu
train targets: 75.85 +/- 11.526
val targets: 112.65 +/- 11.774
Rewirer cayley_clusters with seed 17
Using MSE Loss...
Minimum val loss at epoch 260: 29.870646476745605
Final val loss at epoch 400: 38.57057762145996
Train / Val loss by epoch
5: 1914.249 / 4528.135
10: 59.812 / 406.580
15: 27.941 / 246.140
20: 26.714 / 214.994
25: 25.943 / 183.912
30: 24.245 / 165.990
35: 21.890 / 151.018
40: 24.083 / 145.980
45: 29.461 / 124.841
50: 22.786 / 124.854
55: 19.435 / 106.566
60: 19.601 / 103.236
65: 20.355 / 97.436
70: 15.847 / 92.811
75: 14.963 / 74.541
80: 23.961 / 79.111
85: 10.529 / 72.006
90: 17.318 / 75.705
95: 9.059 / 74.330
100: 6.884 / 62.239
105: 15.290 / 64.064
110: 8.125 / 64.877
115: 9.580 / 59.060
120: 7.912 / 52.393
125: 14.722 / 61.064
130: 12.942 / 55.888
135: 7.990 / 47.551
140: 6.272 / 49.698
145: 10.202 / 46.475
150: 8.397 / 49.326
155: 9.146 / 46.082
160: 12.954 / 46.819
165: 7.717 / 43.442
170: 7.241 / 49.109
175: 9.918 / 38.898
180: 7.778 / 38.824
185: 9.938 / 42.231
190: 9.401 / 37.316
195: 6.354 / 37.981
200: 6.536 / 37.858
205: 5.268 / 38.695
210: 8.006 / 33.196
215: 11.927 / 36.567
220: 6.433 / 35.798
225: 6.736 / 33.483
230: 8.171 / 32.241
235: 9.256 / 34.965
240: 8.134 / 32.777
245: 3.906 / 34.698
250: 5.094 / 32.667
255: 8.010 / 31.478
260: 3.734 / 29.871
265: 3.869 / 31.880
270: 7.853 / 33.812
275: 4.534 / 32.291
280: 4.151 / 33.599
285: 6.293 / 32.113
290: 6.170 / 30.740
295: 5.342 / 34.911
300: 5.127 / 38.414
305: 4.218 / 35.845
310: 6.956 / 34.832
315: 9.674 / 37.307
320: 5.983 / 35.891
325: 7.347 / 34.085
330: 6.645 / 34.455
335: 5.618 / 34.682
340: 5.297 / 34.891
345: 6.288 / 33.986
350: 6.740 / 33.794
355: 4.247 / 40.401
360: 8.056 / 37.910
365: 4.203 / 42.539
370: 4.191 / 39.076
375: 5.022 / 41.118
380: 8.607 / 40.489
385: 5.171 / 40.089
390: 5.571 / 41.242
395: 4.695 / 44.499
400: 6.458 / 38.571
Rewirer cayley_clusters with seed 47
Using MSE Loss...
Minimum val loss at epoch 130: 68.92535438537598
Final val loss at epoch 400: 94.74102630615235
Train / Val loss by epoch
5: 40.296 / 93.106
10: 34.650 / 93.686
15: 32.866 / 95.412
20: 26.225 / 92.325
25: 14.785 / 103.122
30: 10.601 / 85.194
35: 16.809 / 100.322
40: 19.997 / 83.410
45: 15.702 / 94.091
50: 18.870 / 95.810
55: 10.020 / 89.812
60: 18.467 / 93.780
65: 12.809 / 102.549
70: 12.864 / 87.471
75: 11.792 / 88.879
80: 11.492 / 81.631
85: 13.084 / 78.720
90: 13.796 / 77.532
95: 11.962 / 82.025
100: 12.749 / 74.090
105: 17.474 / 77.232
110: 13.058 / 79.210
115: 7.611 / 79.101
120: 8.715 / 76.737
125: 6.550 / 74.184
130: 10.633 / 68.925
135: 9.297 / 71.925
140: 10.722 / 69.329
145: 6.246 / 69.291
150: 8.449 / 73.451
155: 8.380 / 76.950
160: 6.265 / 72.939
165: 8.183 / 71.306
170: 5.569 / 72.458
175: 8.102 / 73.121
180: 8.066 / 73.659
185: 10.772 / 75.981
190: 6.035 / 77.986
195: 7.286 / 71.501
200: 4.203 / 77.326
205: 8.833 / 72.014
210: 4.581 / 75.539
215: 5.153 / 82.164
220: 8.699 / 78.340
225: 6.733 / 76.161
230: 8.745 / 77.705
235: 6.335 / 78.294
240: 7.135 / 85.406
245: 10.140 / 79.387
250: 6.098 / 79.843
255: 8.657 / 84.066
260: 8.039 / 80.064
265: 6.591 / 76.769
270: 5.816 / 82.491
275: 4.894 / 84.901
280: 5.088 / 83.297
285: 5.720 / 83.221
290: 6.631 / 85.958
295: 5.190 / 82.108
300: 4.720 / 83.525
305: 6.455 / 84.725
310: 7.488 / 85.203
315: 6.041 / 93.201
320: 8.902 / 87.527
325: 6.531 / 89.443
330: 5.948 / 90.742
335: 7.573 / 87.233
340: 3.783 / 95.801
345: 5.391 / 93.722
350: 4.797 / 97.663
355: 5.229 / 92.001
360: 8.320 / 94.252
365: 3.763 / 93.688
370: 5.571 / 88.659
375: 5.878 / 90.696
380: 5.510 / 96.082
385: 6.010 / 97.958
390: 4.301 / 95.172
395: 5.282 / 92.126
400: 6.372 / 94.741
Rewirer cayley_clusters with seed 23
Using MSE Loss...
Minimum val loss at epoch 155: 16.813734149932863
Final val loss at epoch 400: 34.98615074157715
Train / Val loss by epoch
5: 36.398 / 80.890
10: 43.727 / 68.315
15: 26.742 / 49.619
20: 34.509 / 42.984
25: 20.861 / 36.568
30: 21.110 / 32.018
35: 15.703 / 29.237
40: 12.629 / 28.311
45: 10.780 / 28.339
50: 23.157 / 27.244
55: 13.098 / 23.265
60: 9.743 / 22.135
65: 14.378 / 22.781
70: 15.380 / 20.334
75: 9.315 / 20.162
80: 9.374 / 19.036
85: 8.158 / 19.479
90: 9.623 / 20.591
95: 13.192 / 17.635
100: 9.431 / 19.974
105: 8.261 / 19.741
110: 10.649 / 20.300
115: 8.139 / 18.806
120: 11.535 / 19.215
125: 9.292 / 21.128
130: 9.788 / 19.364
135: 9.638 / 21.066
140: 8.606 / 19.436
145: 6.895 / 18.926
150: 7.477 / 21.052
155: 7.088 / 16.814
160: 7.589 / 17.511
165: 8.450 / 18.683
170: 5.857 / 19.035
175: 5.532 / 18.431
180: 6.835 / 20.804
185: 6.719 / 22.020
190: 11.370 / 22.108
195: 5.308 / 20.052
200: 6.405 / 22.165
205: 8.941 / 22.272
210: 5.001 / 21.332
215: 9.150 / 19.999
220: 11.757 / 21.153
225: 5.332 / 22.749
230: 7.893 / 22.100
235: 4.215 / 23.654
240: 7.389 / 25.302
245: 7.684 / 23.862
250: 6.074 / 24.129
255: 6.235 / 22.969
260: 9.978 / 23.557
265: 8.353 / 24.828
270: 6.414 / 27.081
275: 9.093 / 29.011
280: 6.981 / 24.024
285: 7.582 / 25.343
290: 6.437 / 28.281
295: 7.213 / 25.130
300: 5.250 / 25.391
305: 7.008 / 30.576
310: 4.906 / 30.258
315: 6.199 / 30.678
320: 7.210 / 29.248
325: 6.507 / 31.591
330: 3.972 / 31.074
335: 6.191 / 31.344
340: 5.557 / 29.385
345: 4.646 / 29.817
350: 4.918 / 32.304
355: 3.159 / 35.465
360: 7.332 / 31.386
365: 4.272 / 34.798
370: 6.089 / 32.838
375: 3.059 / 32.662
380: 6.381 / 36.023
385: 4.573 / 37.037
390: 4.362 / 37.537
395: 7.590 / 34.010
400: 4.178 / 34.986
Rewirer cayley with seed 17
Using MSE Loss...
Minimum val loss at epoch 350: 56.20774383544922
Final val loss at epoch 400: 60.821434020996094
Train / Val loss by epoch
5: 1908.257 / 5405.153
10: 50.145 / 656.191
15: 32.700 / 412.005
20: 25.209 / 344.489
25: 23.937 / 299.814
30: 20.214 / 263.969
35: 22.417 / 241.200
40: 20.540 / 235.598
45: 25.867 / 203.904
50: 23.637 / 211.696
55: 12.861 / 189.516
60: 22.772 / 178.007
65: 14.901 / 172.920
70: 12.481 / 164.568
75: 14.783 / 149.667
80: 22.263 / 151.463
85: 9.585 / 145.169
90: 19.227 / 143.227
95: 12.406 / 141.696
100: 8.528 / 130.461
105: 13.064 / 138.887
110: 9.815 / 134.309
115: 7.621 / 124.280
120: 10.084 / 114.657
125: 16.796 / 123.658
130: 11.738 / 110.962
135: 10.872 / 102.535
140: 7.510 / 97.998
145: 13.620 / 99.294
150: 8.717 / 102.149
155: 13.686 / 83.406
160: 14.211 / 86.270
165: 5.385 / 84.816
170: 8.211 / 82.845
175: 10.327 / 81.640
180: 8.689 / 78.479
185: 9.649 / 77.437
190: 6.828 / 71.325
195: 5.106 / 76.089
200: 8.352 / 70.240
205: 5.255 / 71.102
210: 10.035 / 64.936
215: 7.031 / 73.940
220: 6.865 / 67.476
225: 6.515 / 70.635
230: 7.682 / 69.405
235: 10.964 / 70.939
240: 7.646 / 66.501
245: 4.997 / 68.901
250: 5.885 / 65.694
255: 8.358 / 65.590
260: 6.507 / 60.818
265: 6.694 / 66.627
270: 9.441 / 64.106
275: 4.796 / 66.279
280: 5.633 / 70.417
285: 5.679 / 68.364
290: 4.853 / 67.590
295: 5.304 / 69.230
300: 6.575 / 71.350
305: 4.183 / 66.138
310: 6.647 / 69.546
315: 10.088 / 71.357
320: 5.637 / 66.145
325: 4.900 / 65.205
330: 8.413 / 61.939
335: 7.159 / 60.469
340: 4.761 / 62.569
345: 8.353 / 57.833
350: 6.991 / 56.208
355: 7.571 / 66.190
360: 9.379 / 61.144
365: 6.310 / 64.406
370: 4.140 / 63.375
375: 5.724 / 63.970
380: 7.030 / 63.778
385: 5.885 / 61.058
390: 6.867 / 59.733
395: 7.015 / 64.614
400: 8.356 / 60.821
Rewirer cayley with seed 47
Using MSE Loss...
Minimum val loss at epoch 140: 73.28850784301758
Final val loss at epoch 400: 113.98860473632813
Train / Val loss by epoch
5: 52.983 / 182.799
10: 33.683 / 177.235
15: 38.868 / 179.153
20: 24.066 / 170.492
25: 14.474 / 184.428
30: 14.540 / 163.575
35: 16.660 / 174.620
40: 25.771 / 146.737
45: 20.796 / 140.952
50: 22.739 / 134.338
55: 13.251 / 135.680
60: 20.946 / 139.145
65: 14.326 / 134.845
70: 18.824 / 112.261
75: 14.486 / 109.035
80: 11.507 / 99.066
85: 10.543 / 96.382
90: 17.329 / 91.564
95: 9.388 / 93.371
100: 14.219 / 83.178
105: 17.054 / 86.964
110: 12.125 / 88.058
115: 6.804 / 83.300
120: 8.936 / 83.977
125: 7.732 / 81.231
130: 10.893 / 76.374
135: 10.775 / 79.801
140: 12.571 / 73.289
145: 10.069 / 75.466
150: 7.690 / 81.235
155: 9.334 / 87.058
160: 7.761 / 80.356
165: 6.763 / 81.280
170: 6.315 / 82.615
175: 8.733 / 83.160
180: 7.345 / 82.235
185: 12.784 / 83.280
190: 6.315 / 86.161
195: 9.880 / 79.926
200: 4.716 / 87.046
205: 9.143 / 82.557
210: 6.229 / 86.087
215: 6.415 / 88.324
220: 10.326 / 82.589
225: 7.853 / 82.320
230: 8.890 / 88.983
235: 6.564 / 91.695
240: 5.957 / 95.695
245: 10.880 / 88.591
250: 2.756 / 95.016
255: 8.233 / 92.518
260: 9.073 / 90.314
265: 6.948 / 91.713
270: 7.825 / 91.731
275: 5.687 / 97.998
280: 5.993 / 90.211
285: 5.366 / 94.345
290: 7.149 / 96.011
295: 6.508 / 98.796
300: 4.427 / 96.067
305: 7.740 / 98.624
310: 6.602 / 98.923
315: 7.257 / 105.758
320: 9.128 / 98.698
325: 5.466 / 102.281
330: 7.235 / 105.296
335: 8.985 / 102.999
340: 4.724 / 108.407
345: 5.041 / 110.712
350: 4.627 / 107.353
355: 4.593 / 114.098
360: 9.084 / 110.988
365: 3.041 / 110.426
370: 6.289 / 105.854
375: 6.382 / 110.549
380: 5.370 / 115.485
385: 6.283 / 114.893
390: 5.493 / 114.363
395: 6.907 / 108.756
400: 6.932 / 113.989
Rewirer cayley with seed 23
Using MSE Loss...
Minimum val loss at epoch 145: 9.37830319404602
Final val loss at epoch 400: 34.82493915557861
Train / Val loss by epoch
5: 42.513 / 68.078
10: 39.860 / 42.079
15: 27.593 / 28.897
20: 33.325 / 24.465
25: 19.576 / 21.076
30: 22.809 / 18.556
35: 16.165 / 16.780
40: 16.444 / 15.447
45: 15.619 / 14.988
50: 21.169 / 15.521
55: 13.731 / 12.575
60: 10.505 / 12.453
65: 11.940 / 11.506
70: 13.593 / 11.565
75: 12.806 / 10.645
80: 11.610 / 10.401
85: 8.074 / 11.012
90: 8.610 / 10.556
95: 11.742 / 9.717
100: 12.277 / 10.554
105: 9.000 / 10.549
110: 7.920 / 10.672
115: 7.905 / 9.998
120: 12.970 / 9.900
125: 8.373 / 10.943
130: 7.778 / 10.714
135: 8.608 / 10.911
140: 9.829 / 11.050
145: 7.622 / 9.378
150: 6.816 / 11.361
155: 5.929 / 10.855
160: 7.780 / 10.370
165: 11.426 / 10.861
170: 6.405 / 11.066
175: 4.495 / 11.885
180: 7.737 / 11.758
185: 6.570 / 13.245
190: 10.252 / 13.405
195: 5.091 / 13.047
200: 5.504 / 15.179
205: 9.277 / 13.240
210: 5.755 / 13.482
215: 7.425 / 12.143
220: 9.229 / 14.032
225: 4.588 / 14.490
230: 6.500 / 14.785
235: 5.537 / 16.286
240: 8.399 / 17.527
245: 6.693 / 17.105
250: 7.204 / 17.358
255: 6.014 / 15.818
260: 8.783 / 15.310
265: 9.415 / 16.908
270: 7.467 / 21.083
275: 7.168 / 21.732
280: 10.515 / 17.775
285: 8.285 / 18.490
290: 6.344 / 22.187
295: 6.541 / 19.413
300: 5.844 / 19.824
305: 6.872 / 22.544
310: 4.961 / 24.569
315: 9.119 / 25.288
320: 5.792 / 24.104
325: 5.318 / 27.218
330: 4.580 / 25.933
335: 5.893 / 28.598
340: 5.565 / 25.945
345: 7.293 / 25.372
350: 4.667 / 27.692
355: 4.576 / 32.390
360: 7.208 / 28.188
365: 3.595 / 31.096
370: 5.095 / 28.928
375: 3.583 / 27.571
380: 6.019 / 34.538
385: 4.548 / 34.049
390: 4.022 / 35.684
395: 7.160 / 32.341
400: 4.223 / 34.825
{'cayley': [{'best': 56.20774383544922, 'end': 60.821434020996094},
            {'best': 73.28850784301758, 'end': 113.98860473632813},
            {'best': 9.37830319404602, 'end': 34.82493915557861}],
 'cayley_clusters': [{'best': 29.870646476745605, 'end': 38.57057762145996},
                     {'best': 68.92535438537598, 'end': 94.74102630615235},
                     {'best': 16.813734149932863, 'end': 34.98615074157715}]}
Loading data...
Number of training graphs: 1500
Train graph sizes: Counter({101: 46, 114: 45, 79: 44, 87: 40, 94: 39, 122: 39, 88: 37, 92: 37, 107: 36, 119: 36, 80: 34, 105: 34, 75: 33, 99: 33, 112: 33, 116: 33, 121: 33, 78: 32, 84: 32, 98: 32, 100: 32, 120: 32, 96: 31, 111: 31, 118: 31, 81: 29, 83: 28, 95: 28, 103: 28, 108: 28, 82: 27, 89: 27, 106: 27, 93: 26, 90: 26, 97: 26, 109: 25, 115: 25, 117: 25, 85: 24, 124: 24, 76: 22, 86: 22, 91: 22, 102: 22, 104: 22, 110: 22, 123: 22, 77: 19, 113: 19})
{25: [], 26: [], 27: [], 28: [], 29: [], 30: [], 31: [], 32: [], 33: [], 34: []}
Number of validation graphs: 1500
Val graph sizes: Counter({159: 12, 168: 11, 172: 11, 130: 9, 164: 9, 174: 9, 126: 8, 142: 8, 141: 8, 149: 8, 125: 7, 129: 7, 133: 7, 134: 7, 139: 7, 154: 7, 153: 7, 160: 7, 128: 6, 135: 6, 137: 6, 136: 6, 143: 6, 146: 6, 147: 6, 152: 6, 150: 6, 158: 6, 157: 6, 165: 6, 138: 5, 144: 5, 148: 5, 145: 5, 156: 5, 163: 5, 162: 5, 166: 5, 132: 4, 151: 4, 161: 4, 169: 4, 167: 4, 171: 4, 173: 4, 131: 3, 140: 3, 127: 2, 170: 2, 155: 1})
{'wandb': {'experiment_name': '', 'project': 'ColourInteract Sweeps Friday', 'entity': 'commute_opt_gnn'}, 'data': {'name': 'ColourInteract', 'dataset': 'LRGB', 'c1': 1.0, 'c2s': [0.01, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 100.0], 'num_colours': 4, 'train_size': 1500, 'val_size': 300, 'min_train_nodes': 75, 'max_train_nodes': 125, 'min_val_nodes': 125, 'max_val_nodes': 175, 'rewirers': ['cayley_clusters', 'cayley'], 'normalise': True, 'seed': 93, 'c2': 5.0}, 'model': {'hidden_channels': 8, 'num_layers': 5, 'drop_prob': 0.1, 'global_pool_aggr': 'global_mean_pool', 'approaches': ['only_original', 'interleave'], 'seeds': [17, 47, 23], 'approach': 'only_original', 'seed': 23}, 'train': {'lr': 0.0001, 'num_epochs': 400, 'print_every': 5, 'train_batch_size': 32, 'val_batch_size': 32}, 'run': {'silent': False}}
Device: cpu
No rewiring.
train targets: 185.09 +/- 28.734
val targets: 277.12 +/- 29.332
Rewirer None with seed 17
Using MSE Loss...
Minimum val loss at epoch 190: 539.1165435791015
Final val loss at epoch 400: 783.3211242675782
Train / Val loss by epoch
5: 24448.596 / 45253.616
10: 1963.227 / 8144.585
15: 125.650 / 2072.245
20: 91.489 / 1454.399
25: 129.812 / 1288.956
30: 100.714 / 1152.982
35: 102.474 / 1108.357
40: 73.561 / 1080.684
45: 95.737 / 986.351
50: 111.713 / 987.500
55: 74.680 / 895.888
60: 59.076 / 877.760
65: 70.598 / 880.557
70: 66.570 / 811.528
75: 51.068 / 760.040
80: 105.535 / 770.088
85: 44.737 / 730.006
90: 64.934 / 756.052
95: 40.504 / 732.314
100: 43.325 / 671.525
105: 73.390 / 687.196
110: 43.430 / 690.083
115: 36.498 / 646.677
120: 45.715 / 614.873
125: 62.161 / 662.865
130: 42.115 / 646.752
135: 57.271 / 608.472
140: 40.503 / 602.553
145: 64.779 / 616.572
150: 38.798 / 589.840
155: 38.233 / 560.325
160: 47.066 / 583.598
165: 43.300 / 546.832
170: 36.375 / 605.225
175: 50.434 / 597.788
180: 32.135 / 595.401
185: 49.993 / 572.335
190: 38.745 / 539.117
195: 45.015 / 584.317
200: 34.276 / 559.531
205: 28.458 / 578.460
210: 39.459 / 577.410
215: 48.692 / 593.952
220: 40.511 / 561.920
225: 28.479 / 605.687
230: 38.216 / 584.190
235: 46.861 / 610.654
240: 34.771 / 613.585
245: 29.358 / 642.503
250: 36.247 / 583.490
255: 46.145 / 593.280
260: 30.525 / 610.703
265: 29.806 / 633.949
270: 40.566 / 631.934
275: 41.613 / 642.939
280: 26.621 / 672.436
285: 27.276 / 644.179
290: 31.500 / 645.120
295: 37.225 / 663.985
300: 54.131 / 719.607
305: 19.693 / 684.444
310: 49.665 / 696.340
315: 50.780 / 688.669
320: 32.138 / 689.357
325: 30.957 / 681.781
330: 46.007 / 712.289
335: 45.073 / 707.164
340: 26.240 / 719.193
345: 36.113 / 694.423
350: 33.111 / 690.923
355: 48.869 / 769.908
360: 51.604 / 744.948
365: 27.790 / 778.288
370: 27.687 / 758.658
375: 33.584 / 765.454
380: 30.198 / 786.998
385: 26.821 / 767.195
390: 31.288 / 763.364
395: 36.530 / 829.559
400: 44.308 / 783.321
Rewirer None with seed 47
Using MSE Loss...
Minimum val loss at epoch 15: 860.2727661132812
Final val loss at epoch 400: 1514.2861938476562
Train / Val loss by epoch
5: 1304.095 / 4329.207
10: 126.904 / 934.822
15: 157.355 / 860.273
20: 99.856 / 867.491
25: 64.099 / 919.919
30: 48.070 / 943.199
35: 70.673 / 993.332
40: 105.078 / 1102.057
45: 58.768 / 1116.557
50: 64.649 / 1145.019
55: 95.198 / 1127.942
60: 85.808 / 1121.404
65: 45.262 / 1143.980
70: 89.072 / 1058.193
75: 68.155 / 1097.538
80: 48.183 / 1125.120
85: 63.412 / 1078.944
90: 66.218 / 1073.850
95: 59.589 / 1059.203
100: 57.570 / 1027.987
105: 96.436 / 1038.242
110: 68.394 / 1061.425
115: 55.468 / 1033.727
120: 37.996 / 1091.944
125: 51.947 / 1038.235
130: 79.855 / 985.560
135: 51.814 / 1017.150
140: 63.327 / 1003.898
145: 44.629 / 1000.537
150: 49.541 / 995.378
155: 58.286 / 1059.696
160: 29.810 / 1048.606
165: 47.172 / 964.219
170: 32.872 / 1021.097
175: 37.034 / 1039.107
180: 37.749 / 1048.482
185: 48.415 / 1065.835
190: 48.151 / 1055.103
195: 54.200 / 1056.131
200: 28.354 / 1081.875
205: 42.552 / 1056.629
210: 34.045 / 1112.425
215: 32.301 / 1103.907
220: 46.246 / 1077.402
225: 31.469 / 1132.606
230: 32.654 / 1199.946
235: 37.888 / 1120.224
240: 35.252 / 1190.046
245: 65.263 / 1123.369
250: 27.032 / 1219.810
255: 40.259 / 1207.472
260: 44.972 / 1231.930
265: 41.415 / 1196.423
270: 40.856 / 1176.024
275: 27.820 / 1244.184
280: 25.947 / 1229.848
285: 26.033 / 1269.889
290: 47.542 / 1257.472
295: 29.615 / 1266.016
300: 29.530 / 1306.503
305: 26.499 / 1316.660
310: 27.012 / 1295.458
315: 28.661 / 1340.774
320: 31.620 / 1346.978
325: 35.735 / 1352.359
330: 39.107 / 1325.997
335: 41.348 / 1310.606
340: 23.077 / 1356.898
345: 27.076 / 1388.802
350: 24.342 / 1427.853
355: 31.370 / 1377.931
360: 60.107 / 1387.139
365: 29.153 / 1408.383
370: 30.820 / 1432.708
375: 33.102 / 1447.111
380: 36.077 / 1473.015
385: 35.661 / 1453.753
390: 18.998 / 1503.550
395: 33.837 / 1435.659
400: 43.933 / 1514.286
Rewirer None with seed 23
Using MSE Loss...
Minimum val loss at epoch 90: 355.1628189086914
Final val loss at epoch 400: 831.0940795898438
Train / Val loss by epoch
5: 281.371 / 1751.904
10: 149.412 / 840.841
15: 118.199 / 747.795
20: 118.676 / 667.935
25: 133.830 / 593.428
30: 133.934 / 565.916
35: 89.093 / 510.753
40: 68.593 / 493.067
45: 62.887 / 457.728
50: 88.148 / 423.191
55: 77.325 / 393.723
60: 46.760 / 368.822
65: 80.113 / 382.376
70: 60.179 / 356.313
75: 48.390 / 366.738
80: 55.317 / 368.710
85: 52.216 / 363.875
90: 47.341 / 355.163
95: 67.279 / 366.447
100: 59.721 / 395.015
105: 56.144 / 376.933
110: 61.622 / 385.747
115: 41.310 / 359.001
120: 44.491 / 361.366
125: 43.981 / 396.024
130: 48.500 / 373.455
135: 32.589 / 395.423
140: 53.998 / 384.610
145: 46.634 / 383.419
150: 30.109 / 407.622
155: 41.465 / 382.914
160: 35.952 / 386.589
165: 41.051 / 413.920
170: 36.437 / 415.470
175: 26.284 / 433.431
180: 38.823 / 429.351
185: 37.494 / 464.638
190: 39.386 / 484.759
195: 29.309 / 467.378
200: 26.022 / 498.487
205: 46.893 / 508.979
210: 32.226 / 494.788
215: 35.143 / 484.622
220: 53.183 / 513.348
225: 32.890 / 514.327
230: 42.507 / 521.076
235: 40.915 / 526.166
240: 44.680 / 546.974
245: 35.924 / 556.060
250: 35.002 / 531.906
255: 31.047 / 557.658
260: 47.432 / 546.481
265: 38.322 / 573.548
270: 36.990 / 589.132
275: 46.653 / 592.314
280: 32.821 / 605.598
285: 54.193 / 618.613
290: 29.837 / 639.649
295: 31.353 / 614.066
300: 35.883 / 650.613
305: 40.284 / 674.811
310: 29.429 / 707.989
315: 39.132 / 710.984
320: 34.642 / 695.503
325: 33.457 / 695.319
330: 22.998 / 717.132
335: 32.633 / 733.470
340: 29.746 / 719.561
345: 40.253 / 714.574
350: 35.534 / 759.676
355: 26.664 / 771.134
360: 39.034 / 733.564
365: 21.248 / 763.172
370: 33.801 / 769.269
375: 19.126 / 767.023
380: 38.117 / 842.685
385: 33.093 / 811.457
390: 27.776 / 816.097
395: 42.228 / 792.644
400: 23.800 / 831.094
{None: [{'best': 539.1165435791015, 'end': 783.3211242675782},
        {'best': 860.2727661132812, 'end': 1514.2861938476562},
        {'best': 355.1628189086914, 'end': 831.0940795898438}]}
{'wandb': {'experiment_name': '', 'project': 'ColourInteract Sweeps Friday', 'entity': 'commute_opt_gnn'}, 'data': {'name': 'ColourInteract', 'dataset': 'LRGB', 'c1': 1.0, 'c2s': [0.01, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 100.0], 'num_colours': 4, 'train_size': 1500, 'val_size': 300, 'min_train_nodes': 75, 'max_train_nodes': 125, 'min_val_nodes': 125, 'max_val_nodes': 175, 'rewirers': ['cayley_clusters', 'cayley'], 'normalise': True, 'seed': 93, 'c2': 5.0}, 'model': {'hidden_channels': 8, 'num_layers': 5, 'drop_prob': 0.1, 'global_pool_aggr': 'global_mean_pool', 'approaches': ['only_original', 'interleave'], 'seeds': [17, 47, 23], 'approach': 'interleave', 'seed': 23}, 'train': {'lr': 0.0001, 'num_epochs': 400, 'print_every': 5, 'train_batch_size': 32, 'val_batch_size': 32}, 'run': {'silent': False}}
Device: cpu
train targets: 185.09 +/- 28.734
val targets: 277.12 +/- 29.332
Rewirer cayley_clusters with seed 17
Using MSE Loss...
Minimum val loss at epoch 135: 489.3513885498047
Final val loss at epoch 400: 782.8069885253906
Train / Val loss by epoch
5: 24131.672 / 43963.947
10: 1896.121 / 7223.756
15: 131.258 / 1681.658
20: 71.618 / 1169.474
25: 117.767 / 1021.517
30: 76.359 / 934.600
35: 108.022 / 870.898
40: 69.152 / 853.046
45: 114.921 / 784.789
50: 97.561 / 781.787
55: 71.442 / 711.795
60: 62.832 / 704.525
65: 91.879 / 697.215
70: 60.753 / 650.039
75: 53.746 / 617.318
80: 101.279 / 614.532
85: 49.850 / 611.462
90: 75.168 / 586.370
95: 38.906 / 585.903
100: 36.924 / 543.660
105: 85.619 / 551.245
110: 50.364 / 547.131
115: 35.440 / 527.437
120: 36.900 / 539.321
125: 73.130 / 560.519
130: 40.550 / 536.052
135: 54.124 / 489.351
140: 44.055 / 500.538
145: 45.107 / 522.346
150: 35.729 / 514.770
155: 42.009 / 500.058
160: 44.829 / 519.674
165: 36.408 / 508.350
170: 36.944 / 542.185
175: 38.227 / 523.837
180: 37.415 / 536.538
185: 47.925 / 532.268
190: 39.514 / 496.052
195: 42.833 / 559.239
200: 27.752 / 530.722
205: 25.017 / 546.308
210: 34.159 / 543.344
215: 46.250 / 579.806
220: 34.031 / 560.311
225: 29.107 / 588.167
230: 38.639 / 564.405
235: 37.676 / 561.343
240: 32.646 / 595.077
245: 24.318 / 631.833
250: 23.688 / 577.332
255: 47.558 / 564.385
260: 26.043 / 568.911
265: 25.138 / 609.933
270: 38.832 / 586.143
275: 21.834 / 650.038
280: 27.226 / 628.067
285: 18.785 / 627.922
290: 29.034 / 637.600
295: 35.075 / 632.709
300: 48.378 / 698.229
305: 23.848 / 694.381
310: 41.010 / 680.985
315: 45.404 / 653.859
320: 33.924 / 674.595
325: 31.531 / 692.159
330: 44.689 / 690.968
335: 40.919 / 692.733
340: 20.505 / 700.482
345: 35.406 / 687.986
350: 37.215 / 705.103
355: 33.216 / 742.787
360: 44.557 / 729.945
365: 19.026 / 756.567
370: 26.029 / 772.165
375: 27.136 / 760.168
380: 33.745 / 755.026
385: 22.692 / 787.707
390: 31.006 / 755.240
395: 34.552 / 815.497
400: 37.739 / 782.807
Rewirer cayley_clusters with seed 47
Using MSE Loss...
Minimum val loss at epoch 15: 775.58837890625
Final val loss at epoch 400: 1471.1168151855468
Train / Val loss by epoch
5: 1256.123 / 4051.394
10: 131.401 / 838.708
15: 159.520 / 775.588
20: 123.776 / 799.115
25: 65.767 / 855.686
30: 58.974 / 854.791
35: 85.132 / 902.950
40: 96.181 / 952.846
45: 62.360 / 958.434
50: 55.942 / 967.705
55: 68.826 / 947.271
60: 80.388 / 944.557
65: 39.775 / 985.554
70: 64.971 / 903.448
75: 49.071 / 888.136
80: 35.816 / 935.102
85: 73.629 / 904.502
90: 55.975 / 900.097
95: 42.396 / 909.307
100: 51.696 / 906.375
105: 75.783 / 971.901
110: 67.626 / 982.990
115: 43.705 / 983.056
120: 34.798 / 996.704
125: 35.183 / 945.456
130: 70.991 / 960.526
135: 48.969 / 947.798
140: 57.128 / 957.471
145: 38.389 / 963.672
150: 48.869 / 982.032
155: 54.267 / 1057.714
160: 32.891 / 1053.100
165: 50.109 / 977.526
170: 28.497 / 1013.040
175: 39.325 / 1021.429
180: 33.857 / 1059.580
185: 37.454 / 1061.280
190: 43.140 / 1074.488
195: 46.993 / 1050.651
200: 21.428 / 1087.131
205: 39.851 / 1064.235
210: 30.149 / 1128.783
215: 26.680 / 1140.413
220: 38.164 / 1093.226
225: 35.140 / 1129.595
230: 39.460 / 1193.413
235: 29.002 / 1135.927
240: 32.821 / 1185.257
245: 52.466 / 1141.421
250: 29.322 / 1213.898
255: 45.151 / 1220.528
260: 37.393 / 1270.752
265: 44.220 / 1190.194
270: 35.913 / 1215.090
275: 26.613 / 1252.477
280: 27.913 / 1240.022
285: 26.147 / 1268.642
290: 35.078 / 1285.927
295: 31.171 / 1294.207
300: 30.023 / 1309.535
305: 27.079 / 1306.343
310: 26.113 / 1308.103
315: 34.290 / 1333.372
320: 32.962 / 1322.545
325: 37.304 / 1339.883
330: 34.243 / 1334.343
335: 36.802 / 1368.268
340: 25.188 / 1355.004
345: 30.702 / 1434.942
350: 29.138 / 1385.003
355: 29.646 / 1407.124
360: 55.207 / 1468.769
365: 25.385 / 1407.798
370: 30.376 / 1422.891
375: 33.667 / 1398.532
380: 23.799 / 1402.708
385: 35.192 / 1437.380
390: 20.656 / 1486.776
395: 31.317 / 1384.325
400: 41.491 / 1471.117
Rewirer cayley_clusters with seed 23
Using MSE Loss...
Minimum val loss at epoch 140: 461.8747955322266
Final val loss at epoch 400: 957.0859069824219
Train / Val loss by epoch
5: 271.171 / 1653.911
10: 168.485 / 828.552
15: 115.719 / 735.657
20: 136.321 / 703.468
25: 125.001 / 647.290
30: 120.358 / 663.037
35: 80.821 / 635.081
40: 72.585 / 636.329
45: 53.716 / 640.289
50: 100.212 / 607.633
55: 92.479 / 583.490
60: 59.000 / 551.516
65: 98.890 / 560.611
70: 86.887 / 515.438
75: 48.250 / 531.799
80: 57.469 / 534.566
85: 48.931 / 501.759
90: 40.978 / 491.860
95: 74.492 / 502.487
100: 54.007 / 496.004
105: 56.326 / 497.706
110: 55.695 / 479.596
115: 37.994 / 463.368
120: 51.328 / 480.881
125: 49.218 / 474.714
130: 51.109 / 470.540
135: 38.715 / 473.371
140: 56.860 / 461.875
145: 39.061 / 488.206
150: 32.186 / 504.293
155: 40.550 / 487.091
160: 34.601 / 470.051
165: 42.796 / 537.601
170: 32.756 / 507.860
175: 18.938 / 515.279
180: 36.211 / 516.453
185: 33.396 / 539.447
190: 41.334 / 577.840
195: 35.268 / 536.749
200: 33.966 / 569.246
205: 32.401 / 567.742
210: 37.070 / 565.357
215: 49.176 / 581.709
220: 54.934 / 579.827
225: 28.658 / 599.726
230: 41.372 / 589.415
235: 29.970 / 603.305
240: 34.876 / 648.953
245: 34.274 / 627.101
250: 38.870 / 623.033
255: 30.949 / 629.234
260: 43.788 / 610.539
265: 34.366 / 654.509
270: 39.624 / 670.038
275: 37.494 / 698.258
280: 33.043 / 695.485
285: 42.984 / 706.745
290: 32.759 / 736.324
295: 32.286 / 716.763
300: 31.983 / 744.009
305: 37.023 / 755.343
310: 22.125 / 795.571
315: 34.856 / 797.224
320: 38.632 / 792.341
325: 34.177 / 799.490
330: 22.924 / 826.546
335: 35.158 / 826.588
340: 24.172 / 817.734
345: 30.273 / 840.100
350: 25.390 / 861.196
355: 22.755 / 913.592
360: 31.219 / 905.948
365: 21.562 / 905.581
370: 30.543 / 892.053
375: 20.711 / 884.800
380: 39.898 / 962.373
385: 31.540 / 938.846
390: 27.934 / 988.464
395: 40.339 / 940.538
400: 27.215 / 957.086
Rewirer cayley with seed 17
Using MSE Loss...
Minimum val loss at epoch 190: 668.6577575683593
Final val loss at epoch 400: 904.8483520507813
Train / Val loss by epoch
5: 24160.865 / 46945.468
10: 1891.439 / 8398.072
15: 145.931 / 2192.809
20: 79.412 / 1599.083
25: 97.709 / 1531.940
30: 60.359 / 1464.566
35: 127.184 / 1469.839
40: 81.952 / 1526.419
45: 97.127 / 1421.684
50: 115.771 / 1461.105
55: 78.196 / 1359.705
60: 86.508 / 1310.443
65: 90.284 / 1274.387
70: 67.969 / 1192.625
75: 65.801 / 1121.672
80: 113.917 / 1121.427
85: 57.587 / 1065.193
90: 75.674 / 1054.122
95: 55.679 / 1008.002
100: 60.499 / 961.318
105: 77.197 / 958.202
110: 47.934 / 941.355
115: 31.467 / 871.705
120: 49.322 / 869.640
125: 71.908 / 907.138
130: 52.456 / 866.808
135: 57.119 / 821.067
140: 38.489 / 784.504
145: 54.159 / 816.044
150: 35.074 / 787.430
155: 56.486 / 718.520
160: 56.309 / 732.304
165: 46.250 / 717.525
170: 43.544 / 737.786
175: 38.432 / 709.716
180: 33.950 / 694.158
185: 39.412 / 698.852
190: 42.132 / 668.658
195: 42.120 / 717.539
200: 39.471 / 671.981
205: 25.845 / 689.845
210: 34.329 / 675.974
215: 38.281 / 746.732
220: 35.333 / 685.422
225: 27.370 / 719.057
230: 38.494 / 698.089
235: 39.745 / 692.678
240: 34.743 / 706.678
245: 30.433 / 741.300
250: 31.724 / 698.927
255: 43.218 / 705.263
260: 24.234 / 707.195
265: 33.875 / 778.017
270: 37.191 / 754.624
275: 21.103 / 788.648
280: 23.431 / 776.187
285: 19.009 / 757.879
290: 28.205 / 777.981
295: 34.111 / 772.735
300: 42.547 / 842.548
305: 25.085 / 794.904
310: 42.455 / 809.197
315: 42.005 / 760.909
320: 27.875 / 809.706
325: 26.663 / 812.568
330: 35.747 / 812.174
335: 37.563 / 839.727
340: 22.456 / 812.981
345: 37.139 / 806.847
350: 31.682 / 810.226
355: 31.610 / 854.697
360: 46.759 / 836.717
365: 26.730 / 879.170
370: 24.160 / 851.673
375: 31.488 / 890.319
380: 36.964 / 877.421
385: 25.155 / 856.004
390: 37.548 / 885.391
395: 30.388 / 936.079
400: 39.857 / 904.848
Rewirer cayley with seed 47
Using MSE Loss...
Minimum val loss at epoch 15: 901.276644897461
Final val loss at epoch 400: 1538.6225219726562
Train / Val loss by epoch
5: 1294.835 / 4449.793
10: 120.571 / 968.286
15: 163.251 / 901.277
20: 127.008 / 928.024
25: 69.346 / 1008.450
30: 65.756 / 1020.372
35: 80.592 / 1063.727
40: 95.272 / 1176.949
45: 94.515 / 1180.029
50: 69.898 / 1205.962
55: 91.217 / 1166.709
60: 77.258 / 1176.448
65: 46.814 / 1193.640
70: 84.100 / 1106.643
75: 65.955 / 1115.058
80: 38.536 / 1142.694
85: 70.540 / 1085.993
90: 68.909 / 1077.548
95: 40.853 / 1063.834
100: 77.187 / 1054.481
105: 91.872 / 1098.239
110: 63.663 / 1103.658
115: 39.563 / 1084.848
120: 37.131 / 1083.878
125: 48.038 / 1080.239
130: 75.903 / 1029.168
135: 51.076 / 1034.834
140: 62.875 / 1032.246
145: 47.275 / 1063.126
150: 44.511 / 1020.821
155: 50.828 / 1138.935
160: 31.601 / 1115.727
165: 49.790 / 1024.833
170: 38.113 / 1087.726
175: 38.176 / 1097.963
180: 40.354 / 1083.675
185: 42.604 / 1099.825
190: 46.129 / 1137.593
195: 55.418 / 1101.666
200: 22.600 / 1115.153
205: 49.287 / 1141.153
210: 30.822 / 1203.912
215: 33.340 / 1200.430
220: 44.529 / 1138.976
225: 36.332 / 1164.842
230: 36.768 / 1264.015
235: 27.511 / 1172.875
240: 29.606 / 1244.233
245: 49.370 / 1182.961
250: 24.934 / 1257.336
255: 49.655 / 1217.542
260: 45.421 / 1280.745
265: 41.959 / 1230.352
270: 38.854 / 1225.007
275: 34.584 / 1268.871
280: 30.945 / 1260.942
285: 20.567 / 1246.484
290: 37.146 / 1280.189
295: 36.997 / 1281.283
300: 31.814 / 1312.612
305: 33.735 / 1323.373
310: 22.442 / 1322.462
315: 32.610 / 1338.503
320: 35.835 / 1331.819
325: 30.124 / 1360.216
330: 37.018 / 1349.396
335: 44.578 / 1391.373
340: 26.691 / 1373.429
345: 29.780 / 1437.833
350: 25.108 / 1417.911
355: 32.652 / 1456.553
360: 63.810 / 1481.060
365: 22.384 / 1440.372
370: 36.373 / 1451.078
375: 34.789 / 1448.032
380: 29.563 / 1517.092
385: 37.029 / 1478.569
390: 22.027 / 1527.753
395: 34.562 / 1465.143
400: 47.066 / 1538.623
Rewirer cayley with seed 23
Using MSE Loss...
Minimum val loss at epoch 70: 291.0696990966797
Final val loss at epoch 400: 957.2343078613281
Train / Val loss by epoch
5: 307.675 / 1451.416
10: 135.260 / 567.703
15: 134.288 / 448.385
20: 134.142 / 397.873
25: 120.017 / 349.611
30: 129.366 / 364.739
35: 85.601 / 350.018
40: 91.100 / 359.806
45: 68.481 / 346.293
50: 97.397 / 343.643
55: 82.063 / 314.295
60: 53.558 / 303.658
65: 78.309 / 303.780
70: 72.249 / 291.070
75: 49.961 / 313.756
80: 60.447 / 320.864
85: 45.573 / 305.199
90: 36.809 / 311.046
95: 68.736 / 319.742
100: 63.111 / 326.087
105: 60.362 / 328.215
110: 52.360 / 329.756
115: 37.011 / 329.257
120: 51.831 / 354.170
125: 46.140 / 353.023
130: 49.557 / 370.131
135: 31.535 / 369.208
140: 66.843 / 392.006
145: 38.586 / 399.975
150: 35.794 / 435.283
155: 40.009 / 404.343
160: 34.735 / 394.365
165: 44.118 / 440.866
170: 31.699 / 432.453
175: 17.333 / 441.367
180: 38.148 / 443.140
185: 39.715 / 462.081
190: 35.365 / 491.109
195: 30.190 / 464.553
200: 22.777 / 497.852
205: 35.430 / 504.704
210: 32.953 / 520.284
215: 45.083 / 510.968
220: 55.795 / 522.703
225: 29.000 / 563.014
230: 39.352 / 555.389
235: 34.533 / 554.777
240: 40.154 / 594.496
245: 37.358 / 567.871
250: 44.879 / 587.009
255: 36.007 / 572.839
260: 41.057 / 562.059
265: 34.138 / 615.980
270: 42.259 / 636.938
275: 37.047 / 666.299
280: 37.293 / 652.680
285: 51.779 / 638.014
290: 31.505 / 695.421
295: 30.911 / 664.109
300: 31.285 / 696.177
305: 35.567 / 715.978
310: 26.450 / 758.548
315: 43.009 / 769.663
320: 38.999 / 769.634
325: 27.105 / 763.894
330: 25.729 / 781.649
335: 33.779 / 792.674
340: 27.073 / 794.333
345: 30.542 / 803.542
350: 28.189 / 812.640
355: 26.873 / 850.126
360: 35.685 / 858.298
365: 20.467 / 881.602
370: 26.579 / 864.583
375: 27.147 / 863.795
380: 39.169 / 951.997
385: 24.597 / 920.707
390: 21.253 / 961.881
395: 38.533 / 924.009
400: 23.751 / 957.234
{'cayley': [{'best': 668.6577575683593, 'end': 904.8483520507813},
            {'best': 901.276644897461, 'end': 1538.6225219726562},
            {'best': 291.0696990966797, 'end': 957.2343078613281}],
 'cayley_clusters': [{'best': 489.3513885498047, 'end': 782.8069885253906},
                     {'best': 775.58837890625, 'end': 1471.1168151855468},
                     {'best': 461.8747955322266, 'end': 957.0859069824219}]}
Loading data...
Number of training graphs: 1500
Train graph sizes: Counter({101: 46, 114: 45, 79: 44, 87: 40, 94: 39, 122: 39, 88: 37, 92: 37, 107: 36, 119: 36, 80: 34, 105: 34, 75: 33, 99: 33, 112: 33, 116: 33, 121: 33, 78: 32, 84: 32, 98: 32, 100: 32, 120: 32, 96: 31, 111: 31, 118: 31, 81: 29, 83: 28, 95: 28, 103: 28, 108: 28, 82: 27, 89: 27, 106: 27, 93: 26, 90: 26, 97: 26, 109: 25, 115: 25, 117: 25, 85: 24, 124: 24, 76: 22, 86: 22, 91: 22, 102: 22, 104: 22, 110: 22, 123: 22, 77: 19, 113: 19})
{25: [], 26: [], 27: [], 28: [], 29: [], 30: [], 31: [], 32: [], 33: [], 34: []}
Number of validation graphs: 1500
Val graph sizes: Counter({159: 12, 168: 11, 172: 11, 130: 9, 164: 9, 174: 9, 126: 8, 142: 8, 141: 8, 149: 8, 125: 7, 129: 7, 133: 7, 134: 7, 139: 7, 154: 7, 153: 7, 160: 7, 128: 6, 135: 6, 137: 6, 136: 6, 143: 6, 146: 6, 147: 6, 152: 6, 150: 6, 158: 6, 157: 6, 165: 6, 138: 5, 144: 5, 148: 5, 145: 5, 156: 5, 163: 5, 162: 5, 166: 5, 132: 4, 151: 4, 161: 4, 169: 4, 167: 4, 171: 4, 173: 4, 131: 3, 140: 3, 127: 2, 170: 2, 155: 1})
{'wandb': {'experiment_name': '', 'project': 'ColourInteract Sweeps Friday', 'entity': 'commute_opt_gnn'}, 'data': {'name': 'ColourInteract', 'dataset': 'LRGB', 'c1': 1.0, 'c2s': [0.01, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 100.0], 'num_colours': 4, 'train_size': 1500, 'val_size': 300, 'min_train_nodes': 75, 'max_train_nodes': 125, 'min_val_nodes': 125, 'max_val_nodes': 175, 'rewirers': ['cayley_clusters', 'cayley'], 'normalise': True, 'seed': 93, 'c2': 10.0}, 'model': {'hidden_channels': 8, 'num_layers': 5, 'drop_prob': 0.1, 'global_pool_aggr': 'global_mean_pool', 'approaches': ['only_original', 'interleave'], 'seeds': [17, 47, 23], 'approach': 'only_original', 'seed': 23}, 'train': {'lr': 0.0001, 'num_epochs': 400, 'print_every': 5, 'train_batch_size': 32, 'val_batch_size': 32}, 'run': {'silent': False}}
Device: cpu
No rewiring.
train targets: 367.16 +/- 57.416
val targets: 551.23 +/- 58.596
Rewirer None with seed 17
Using MSE Loss...
Minimum val loss at epoch 155: 2981.1933715820314
Final val loss at epoch 400: 5540.626196289062
Train / Val loss by epoch
5: 135568.703 / 229123.944
10: 37575.844 / 85300.035
15: 3778.882 / 21713.571
20: 423.764 / 8229.706
25: 437.498 / 6133.259
30: 343.059 / 5633.149
35: 404.463 / 5474.903
40: 249.426 / 5272.099
45: 368.661 / 5080.776
50: 398.633 / 4888.217
55: 263.050 / 4601.309
60: 268.360 / 4417.901
65: 273.636 / 4380.427
70: 272.984 / 4104.379
75: 177.995 / 3856.888
80: 423.021 / 3878.542
85: 164.840 / 3572.120
90: 258.749 / 3548.630
95: 184.002 / 3490.422
100: 169.754 / 3240.288
105: 238.163 / 3324.109
110: 171.984 / 3266.463
115: 137.319 / 3037.128
120: 178.357 / 3012.313
125: 192.324 / 3239.219
130: 167.353 / 3201.577
135: 176.493 / 3004.399
140: 166.380 / 3035.503
145: 220.207 / 3086.150
150: 125.710 / 3075.421
155: 174.363 / 2981.193
160: 178.936 / 3228.617
165: 178.661 / 3103.901
170: 151.628 / 3273.886
175: 148.927 / 3209.013
180: 132.544 / 3295.953
185: 171.056 / 3278.598
190: 148.990 / 3239.660
195: 151.109 / 3626.027
200: 132.482 / 3446.425
205: 94.560 / 3550.309
210: 105.149 / 3650.857
215: 164.746 / 3851.993
220: 153.517 / 3688.870
225: 110.616 / 3985.190
230: 133.053 / 3888.829
235: 150.376 / 3952.627
240: 146.001 / 4027.410
245: 106.912 / 4146.452
250: 112.049 / 4130.260
255: 182.752 / 4108.893
260: 86.267 / 4224.390
265: 99.643 / 4482.780
270: 179.189 / 4386.850
275: 115.455 / 4534.396
280: 101.594 / 4432.896
285: 91.169 / 4632.989
290: 87.429 / 4688.141
295: 139.251 / 4896.004
300: 180.428 / 4930.978
305: 87.391 / 5049.942
310: 165.799 / 4917.656
315: 174.380 / 4753.162
320: 115.721 / 4968.353
325: 124.651 / 5209.112
330: 118.883 / 5203.779
335: 142.349 / 5205.528
340: 85.431 / 5313.112
345: 117.601 / 5361.560
350: 128.622 / 5331.149
355: 158.425 / 5307.418
360: 185.024 / 5265.541
365: 111.746 / 5412.528
370: 82.720 / 5490.699
375: 137.653 / 5347.586
380: 126.392 / 5524.322
385: 100.716 / 5422.401
390: 142.012 / 5544.270
395: 140.472 / 5797.785
400: 156.318 / 5540.626
Rewirer None with seed 47
Using MSE Loss...
Minimum val loss at epoch 20: 5662.342260742187
Final val loss at epoch 400: 10151.7158203125
Train / Val loss by epoch
5: 50339.176 / 94109.784
10: 6279.491 / 22365.959
15: 849.730 / 7287.630
20: 523.461 / 5662.342
25: 312.890 / 5741.452
30: 220.558 / 5840.429
35: 288.988 / 6057.868
40: 312.496 / 6482.150
45: 216.483 / 6439.347
50: 137.455 / 6621.772
55: 258.583 / 6586.689
60: 268.240 / 6785.065
65: 181.424 / 6955.794
70: 243.044 / 6835.657
75: 202.585 / 7019.369
80: 168.269 / 7238.525
85: 246.015 / 7055.110
90: 233.409 / 7031.754
95: 183.061 / 7007.537
100: 179.365 / 7096.921
105: 255.659 / 7295.561
110: 222.693 / 7363.336
115: 207.512 / 7269.889
120: 160.715 / 7452.916
125: 177.382 / 7201.225
130: 254.054 / 7247.098
135: 181.631 / 7244.550
140: 179.252 / 7266.988
145: 156.890 / 7158.161
150: 163.744 / 6863.213
155: 186.806 / 7429.916
160: 149.650 / 7244.099
165: 159.821 / 7016.623
170: 116.186 / 7082.021
175: 106.121 / 7192.582
180: 120.587 / 7314.384
185: 120.132 / 7470.053
190: 182.994 / 7464.027
195: 184.306 / 7283.615
200: 97.709 / 7287.909
205: 151.964 / 7532.436
210: 135.468 / 7720.461
215: 115.153 / 7480.418
220: 171.407 / 7582.071
225: 142.104 / 7808.631
230: 114.358 / 8142.227
235: 112.802 / 7830.467
240: 155.160 / 7987.429
245: 215.720 / 7770.427
250: 156.843 / 7898.868
255: 184.420 / 8165.422
260: 160.816 / 8444.131
265: 129.663 / 7949.303
270: 156.354 / 8187.211
275: 140.345 / 8500.659
280: 115.083 / 8217.751
285: 96.567 / 8359.757
290: 164.052 / 8522.046
295: 157.518 / 8551.712
300: 125.538 / 8777.600
305: 102.839 / 8717.961
310: 99.292 / 8699.621
315: 101.156 / 8857.080
320: 131.331 / 8941.565
325: 141.005 / 8975.172
330: 148.709 / 8820.208
335: 140.784 / 9097.316
340: 100.920 / 8990.947
345: 117.078 / 9563.697
350: 134.375 / 9553.466
355: 107.953 / 9324.971
360: 218.700 / 9483.206
365: 118.755 / 9252.734
370: 138.685 / 9591.211
375: 136.534 / 9495.055
380: 114.747 / 9606.549
385: 128.832 / 9879.519
390: 72.506 / 10186.804
395: 128.269 / 9841.118
400: 168.505 / 10151.716
Rewirer None with seed 23
Using MSE Loss...
Minimum val loss at epoch 80: 2853.98134765625
Final val loss at epoch 400: 6980.87021484375
Train / Val loss by epoch
5: 27900.799 / 60116.528
10: 1617.691 / 9669.878
15: 538.170 / 4084.746
20: 480.325 / 3505.390
25: 550.896 / 3413.817
30: 513.433 / 3368.512
35: 350.789 / 3217.617
40: 381.595 / 3276.128
45: 289.760 / 3207.974
50: 285.357 / 3222.810
55: 366.389 / 3299.428
60: 183.235 / 3174.175
65: 218.474 / 3222.423
70: 208.021 / 2971.228
75: 151.702 / 2983.818
80: 237.657 / 2853.981
85: 182.365 / 3003.045
90: 190.414 / 2958.461
95: 230.655 / 2902.413
100: 209.039 / 3083.936
105: 261.745 / 2891.520
110: 161.611 / 2988.880
115: 146.726 / 2916.479
120: 193.473 / 2964.465
125: 145.842 / 3131.788
130: 124.179 / 3167.556
135: 111.788 / 3281.188
140: 138.935 / 3378.780
145: 152.196 / 3364.276
150: 98.867 / 3505.824
155: 163.619 / 3443.670
160: 120.430 / 3480.776
165: 172.476 / 3638.709
170: 116.402 / 3684.766
175: 133.515 / 3750.766
180: 76.305 / 3799.618
185: 124.373 / 3897.461
190: 133.229 / 3992.371
195: 127.880 / 4053.981
200: 129.529 / 4238.515
205: 137.327 / 4307.499
210: 144.832 / 4282.302
215: 158.262 / 4336.565
220: 231.821 / 4458.946
225: 137.031 / 4547.897
230: 159.538 / 4541.549
235: 161.540 / 4620.645
240: 168.192 / 4717.147
245: 107.363 / 4737.500
250: 108.330 / 4876.726
255: 113.259 / 4829.741
260: 144.579 / 4880.246
265: 118.367 / 4959.240
270: 158.349 / 5191.226
275: 173.378 / 5161.492
280: 113.645 / 5318.452
285: 168.174 / 5421.587
290: 115.748 / 5575.391
295: 119.694 / 5434.112
300: 129.298 / 5523.072
305: 130.226 / 5658.406
310: 134.229 / 5963.854
315: 126.319 / 6039.819
320: 114.470 / 6117.628
325: 134.584 / 5903.608
330: 129.850 / 6098.155
335: 160.616 / 6117.826
340: 79.490 / 6090.057
345: 140.893 / 6015.972
350: 148.089 / 6420.577
355: 84.439 / 6486.209
360: 117.371 / 6493.074
365: 53.207 / 6506.748
370: 155.885 / 6624.061
375: 75.999 / 6511.532
380: 124.897 / 7123.877
385: 116.412 / 6760.352
390: 107.374 / 7048.792
395: 149.503 / 6785.806
400: 107.033 / 6980.870
{None: [{'best': 2981.1933715820314, 'end': 5540.626196289062},
        {'best': 5662.342260742187, 'end': 10151.7158203125},
        {'best': 2853.98134765625, 'end': 6980.87021484375}]}
{'wandb': {'experiment_name': '', 'project': 'ColourInteract Sweeps Friday', 'entity': 'commute_opt_gnn'}, 'data': {'name': 'ColourInteract', 'dataset': 'LRGB', 'c1': 1.0, 'c2s': [0.01, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 100.0], 'num_colours': 4, 'train_size': 1500, 'val_size': 300, 'min_train_nodes': 75, 'max_train_nodes': 125, 'min_val_nodes': 125, 'max_val_nodes': 175, 'rewirers': ['cayley_clusters', 'cayley'], 'normalise': True, 'seed': 93, 'c2': 10.0}, 'model': {'hidden_channels': 8, 'num_layers': 5, 'drop_prob': 0.1, 'global_pool_aggr': 'global_mean_pool', 'approaches': ['only_original', 'interleave'], 'seeds': [17, 47, 23], 'approach': 'interleave', 'seed': 23}, 'train': {'lr': 0.0001, 'num_epochs': 400, 'print_every': 5, 'train_batch_size': 32, 'val_batch_size': 32}, 'run': {'silent': False}}
Device: cpu
train targets: 367.16 +/- 57.416
val targets: 551.23 +/- 58.596
Rewirer cayley_clusters with seed 17
Using MSE Loss...
Minimum val loss at epoch 140: 3197.7169921875
Final val loss at epoch 400: 5334.197119140625
Train / Val loss by epoch
5: 134757.672 / 225972.231
10: 37061.613 / 81168.392
15: 3893.763 / 20038.854
20: 393.440 / 7560.309
25: 460.255 / 5576.677
30: 250.220 / 5155.035
35: 424.399 / 4817.063
40: 233.293 / 4736.549
45: 331.966 / 4508.935
50: 350.517 / 4356.044
55: 244.360 / 4141.154
60: 245.967 / 4033.659
65: 333.529 / 3914.493
70: 268.091 / 3631.730
75: 183.313 / 3579.516
80: 364.402 / 3623.066
85: 173.208 / 3526.269
90: 276.146 / 3475.519
95: 175.595 / 3470.269
100: 175.616 / 3339.276
105: 309.188 / 3373.570
110: 192.281 / 3328.208
115: 99.504 / 3297.809
120: 126.909 / 3370.527
125: 195.828 / 3458.841
130: 144.190 / 3464.857
135: 194.507 / 3282.201
140: 174.805 / 3197.717
145: 169.151 / 3347.755
150: 110.551 / 3484.619
155: 169.447 / 3330.035
160: 155.304 / 3629.418
165: 161.665 / 3555.177
170: 147.173 / 3678.503
175: 116.203 / 3555.936
180: 120.494 / 3715.895
185: 157.650 / 3690.985
190: 148.983 / 3526.275
195: 151.051 / 3986.764
200: 113.755 / 3871.585
205: 101.237 / 3878.852
210: 84.469 / 3908.642
215: 150.560 / 4167.368
220: 115.189 / 4164.183
225: 116.390 / 4231.427
230: 125.427 / 4179.307
235: 147.548 / 4097.503
240: 137.740 / 4168.639
245: 86.824 / 4243.597
250: 83.453 / 4181.139
255: 189.256 / 4111.619
260: 107.607 / 4162.056
265: 109.217 / 4454.791
270: 173.453 / 4399.879
275: 111.999 / 4661.411
280: 89.211 / 4489.787
285: 90.517 / 4502.591
290: 76.109 / 4591.770
295: 124.710 / 4644.318
300: 161.365 / 4834.055
305: 78.582 / 4944.274
310: 172.660 / 4794.666
315: 157.371 / 4570.484
320: 116.174 / 4811.829
325: 142.682 / 5037.993
330: 128.847 / 5001.974
335: 151.672 / 5152.779
340: 65.284 / 5039.119
345: 118.547 / 5041.976
350: 138.310 / 5180.544
355: 107.519 / 5019.739
360: 182.044 / 5174.971
365: 79.835 / 5118.994
370: 104.707 / 5258.295
375: 114.086 / 5225.455
380: 138.737 / 5202.155
385: 87.496 / 5377.835
390: 107.197 / 5320.621
395: 140.132 / 5606.433
400: 163.479 / 5334.197
Rewirer cayley_clusters with seed 47
Using MSE Loss...
Minimum val loss at epoch 20: 5582.916357421875
Final val loss at epoch 400: 9221.503662109375
Train / Val loss by epoch
5: 49953.852 / 92946.025
10: 6241.527 / 22264.242
15: 812.835 / 7140.255
20: 561.474 / 5582.916
25: 295.209 / 5670.464
30: 225.605 / 5795.750
35: 287.887 / 6066.548
40: 301.348 / 6512.739
45: 215.789 / 6387.205
50: 131.435 / 6617.497
55: 215.260 / 6516.255
60: 253.059 / 6596.620
65: 161.091 / 6721.850
70: 191.931 / 6552.579
75: 162.380 / 6697.057
80: 150.673 / 6706.343
85: 263.152 / 6489.394
90: 222.098 / 6534.731
95: 167.761 / 6381.032
100: 186.498 / 6360.566
105: 239.675 / 6580.124
110: 227.076 / 6765.575
115: 156.526 / 6502.549
120: 169.762 / 6527.709
125: 129.614 / 6383.397
130: 203.785 / 6395.810
135: 182.550 / 6389.289
140: 163.787 / 6360.249
145: 144.828 / 6248.209
150: 148.809 / 6014.769
155: 208.686 / 6467.849
160: 128.075 / 6404.272
165: 141.764 / 6146.206
170: 105.438 / 6166.026
175: 123.546 / 6427.122
180: 118.953 / 6392.714
185: 100.485 / 6452.285
190: 173.241 / 6553.887
195: 170.400 / 6372.490
200: 65.635 / 6431.125
205: 125.793 / 6646.392
210: 117.615 / 6909.990
215: 107.804 / 6892.580
220: 134.393 / 6586.227
225: 115.521 / 6914.871
230: 115.560 / 7112.792
235: 93.891 / 6885.219
240: 151.416 / 7095.897
245: 199.188 / 6939.981
250: 128.471 / 6930.985
255: 153.242 / 7284.395
260: 135.773 / 7562.297
265: 143.810 / 7103.112
270: 134.092 / 7342.245
275: 115.114 / 7491.965
280: 113.912 / 7399.278
285: 94.824 / 7623.060
290: 145.999 / 7827.904
295: 139.157 / 7728.442
300: 128.010 / 7818.654
305: 116.427 / 7847.234
310: 113.806 / 7962.051
315: 105.889 / 8071.840
320: 112.428 / 7970.658
325: 135.341 / 8092.921
330: 139.426 / 8080.131
335: 135.928 / 8193.001
340: 115.017 / 8071.202
345: 118.715 / 8501.517
350: 125.004 / 8421.010
355: 110.434 / 8407.520
360: 219.130 / 8698.456
365: 81.111 / 8474.732
370: 129.284 / 8843.120
375: 142.545 / 8700.324
380: 112.350 / 8747.860
385: 129.806 / 8974.045
390: 66.066 / 9316.764
395: 125.635 / 8876.393
400: 139.458 / 9221.504
Rewirer cayley_clusters with seed 23
Using MSE Loss...
Minimum val loss at epoch 115: 3253.177209472656
Final val loss at epoch 400: 8694.0875
Train / Val loss by epoch
5: 27692.107 / 59678.604
10: 1630.800 / 10306.187
15: 509.153 / 4566.886
20: 473.846 / 4025.371
25: 474.656 / 3918.328
30: 487.922 / 3865.454
35: 323.691 / 3728.917
40: 424.191 / 3816.423
45: 294.137 / 3848.324
50: 353.589 / 3832.650
55: 387.247 / 4016.512
60: 183.206 / 3943.207
65: 278.061 / 3951.153
70: 257.679 / 3695.479
75: 203.684 / 3517.105
80: 190.096 / 3479.054
85: 183.370 / 3486.746
90: 161.672 / 3414.188
95: 219.881 / 3382.379
100: 157.394 / 3473.837
105: 162.724 / 3358.721
110: 167.494 / 3399.985
115: 120.135 / 3253.177
120: 146.050 / 3414.744
125: 111.162 / 3561.876
130: 149.284 / 3518.787
135: 113.654 / 3723.825
140: 139.552 / 3739.682
145: 148.235 / 3740.113
150: 94.693 / 3936.557
155: 137.030 / 3844.353
160: 106.125 / 3913.224
165: 129.065 / 4169.765
170: 125.726 / 4147.952
175: 92.450 / 4158.306
180: 105.695 / 4343.364
185: 137.171 / 4465.107
190: 147.635 / 4728.648
195: 94.482 / 4701.411
200: 128.220 / 4924.172
205: 131.831 / 4998.413
210: 132.454 / 5087.691
215: 172.759 / 5303.506
220: 208.398 / 5408.500
225: 131.094 / 5428.512
230: 118.450 / 5459.852
235: 128.506 / 5614.684
240: 159.307 / 5698.431
245: 104.516 / 5750.557
250: 125.309 / 5804.277
255: 121.744 / 5891.871
260: 130.135 / 5729.335
265: 121.741 / 6048.346
270: 144.859 / 6209.346
275: 174.376 / 6404.931
280: 110.524 / 6559.290
285: 151.501 / 6600.346
290: 120.567 / 6828.961
295: 115.551 / 6719.691
300: 100.566 / 6901.161
305: 125.696 / 6922.855
310: 71.570 / 7294.088
315: 112.088 / 7299.043
320: 122.039 / 7533.899
325: 112.357 / 7295.200
330: 122.358 / 7466.993
335: 149.893 / 7546.033
340: 95.429 / 7671.266
345: 142.480 / 7601.595
350: 123.052 / 7867.682
355: 73.823 / 8077.842
360: 109.311 / 8050.319
365: 82.099 / 8017.255
370: 137.144 / 8091.990
375: 68.031 / 7992.001
380: 132.406 / 8627.547
385: 110.570 / 8368.643
390: 94.880 / 8757.897
395: 146.713 / 8442.410
400: 97.312 / 8694.087
Rewirer cayley with seed 17
Using MSE Loss...
Minimum val loss at epoch 175: 3776.039794921875
Final val loss at epoch 400: 5508.117846679687
Train / Val loss by epoch
5: 134848.125 / 233061.966
10: 36905.312 / 85291.325
15: 3831.408 / 20989.797
20: 358.299 / 7915.316
25: 363.629 / 6399.655
30: 264.841 / 6282.293
35: 484.553 / 6547.333
40: 235.727 / 6641.255
45: 317.637 / 6432.291
50: 371.704 / 6352.492
55: 312.502 / 5992.820
60: 321.225 / 5800.359
65: 337.823 / 5564.434
70: 249.538 / 5295.863
75: 213.679 / 5095.625
80: 390.096 / 5161.766
85: 210.311 / 4906.410
90: 257.693 / 4942.334
95: 193.929 / 4791.634
100: 251.608 / 4711.788
105: 268.796 / 4583.652
110: 186.340 / 4578.814
115: 113.530 / 4269.294
120: 167.647 / 4307.839
125: 255.523 / 4382.103
130: 181.441 / 4326.598
135: 188.291 / 4216.326
140: 157.655 / 3998.451
145: 198.898 / 4161.850
150: 134.306 / 4104.207
155: 172.196 / 3887.519
160: 195.296 / 4038.102
165: 182.816 / 3897.411
170: 151.229 / 3968.427
175: 142.560 / 3776.040
180: 120.169 / 3859.516
185: 145.168 / 3981.184
190: 180.750 / 3791.589
195: 155.817 / 4195.986
200: 145.998 / 4086.474
205: 86.630 / 4050.151
210: 91.277 / 4008.600
215: 153.473 / 4345.735
220: 125.698 / 4059.584
225: 100.974 / 4332.620
230: 125.515 / 4159.833
235: 119.254 / 4242.071
240: 145.271 / 4223.301
245: 99.784 / 4438.073
250: 106.392 / 4357.798
255: 158.565 / 4299.607
260: 75.449 / 4333.970
265: 119.167 / 4599.037
270: 148.360 / 4588.396
275: 85.117 / 4800.870
280: 89.289 / 4784.940
285: 84.495 / 4737.758
290: 95.039 / 4809.488
295: 142.630 / 4920.065
300: 149.701 / 5119.934
305: 99.556 / 4906.947
310: 157.251 / 4921.299
315: 128.085 / 4654.439
320: 99.932 / 4970.444
325: 120.541 / 5152.701
330: 117.181 / 5127.463
335: 149.626 / 5112.573
340: 82.987 / 5171.752
345: 123.840 / 5284.377
350: 129.800 / 5282.182
355: 117.684 / 5174.182
360: 174.665 / 5249.130
365: 86.677 / 5311.999
370: 78.067 / 5469.361
375: 119.701 / 5429.944
380: 165.151 / 5433.114
385: 101.068 / 5221.222
390: 127.240 / 5527.318
395: 125.461 / 5631.202
400: 160.631 / 5508.118
Rewirer cayley with seed 47
Using MSE Loss...
Minimum val loss at epoch 20: 5501.281420898437
Final val loss at epoch 400: 8751.583984375
Train / Val loss by epoch
5: 50046.004 / 94716.223
10: 6190.597 / 22441.947
15: 869.438 / 7033.679
20: 553.584 / 5501.281
25: 318.939 / 5734.062
30: 234.909 / 5925.000
35: 291.165 / 6452.386
40: 318.259 / 7033.516
45: 283.324 / 7106.029
50: 153.183 / 7387.875
55: 237.051 / 7400.529
60: 257.443 / 7541.732
65: 173.509 / 7520.333
70: 207.229 / 7297.594
75: 221.826 / 7296.385
80: 166.507 / 7403.129
85: 266.909 / 7077.246
90: 232.766 / 7085.141
95: 157.754 / 7018.893
100: 222.531 / 6919.181
105: 263.771 / 7130.420
110: 210.025 / 7122.495
115: 146.813 / 7066.976
120: 142.742 / 7050.336
125: 157.782 / 6934.340
130: 247.153 / 6836.605
135: 211.139 / 6904.691
140: 173.357 / 6805.165
145: 167.711 / 6641.673
150: 138.750 / 6488.492
155: 195.705 / 6993.198
160: 116.739 / 6723.532
165: 164.951 / 6502.228
170: 132.537 / 6472.341
175: 106.529 / 6636.415
180: 132.204 / 6710.538
185: 112.361 / 6666.200
190: 180.611 / 6741.128
195: 182.087 / 6519.691
200: 84.042 / 6607.936
205: 155.304 / 6805.112
210: 115.712 / 6952.602
215: 122.486 / 6745.014
220: 172.405 / 6561.401
225: 121.008 / 6944.025
230: 124.835 / 7122.799
235: 93.455 / 6769.296
240: 138.014 / 6932.780
245: 186.448 / 6932.573
250: 124.808 / 6834.064
255: 183.923 / 6787.899
260: 147.168 / 7195.957
265: 126.672 / 6919.065
270: 152.445 / 6944.978
275: 126.664 / 7186.161
280: 105.135 / 7055.618
285: 75.987 / 7056.034
290: 135.437 / 7289.013
295: 138.119 / 7099.956
300: 142.792 / 7424.449
305: 121.581 / 7355.121
310: 94.789 / 7632.439
315: 108.026 / 7640.269
320: 121.924 / 7755.783
325: 102.867 / 7815.630
330: 147.900 / 7694.973
335: 146.635 / 7930.762
340: 102.581 / 7713.337
345: 115.604 / 8180.069
350: 124.983 / 8029.772
355: 108.363 / 8065.997
360: 245.762 / 8332.215
365: 83.251 / 8158.704
370: 134.840 / 8435.317
375: 120.569 / 8086.351
380: 107.634 / 8203.795
385: 137.043 / 8426.428
390: 60.514 / 8667.590
395: 139.616 / 8325.881
400: 147.518 / 8751.584
Rewirer cayley with seed 23
Using MSE Loss...
Minimum val loss at epoch 115: 2215.328649902344
Final val loss at epoch 400: 7579.422705078125
Train / Val loss by epoch
5: 27960.740 / 57756.002
10: 1531.777 / 8101.643
15: 542.964 / 2975.649
20: 530.522 / 2537.968
25: 507.065 / 2446.778
30: 527.801 / 2451.994
35: 393.205 / 2393.168
40: 402.034 / 2481.569
45: 289.559 / 2536.227
50: 393.485 / 2615.387
55: 383.320 / 2758.403
60: 163.009 / 2723.543
65: 209.523 / 2805.489
70: 249.954 / 2602.077
75: 174.256 / 2488.466
80: 228.025 / 2428.572
85: 190.825 / 2386.561
90: 150.713 / 2359.570
95: 267.540 / 2262.469
100: 215.874 / 2408.592
105: 216.495 / 2225.146
110: 193.172 / 2290.354
115: 163.468 / 2215.329
120: 154.685 / 2406.228
125: 129.831 / 2504.657
130: 133.646 / 2628.233
135: 115.374 / 2819.330
140: 137.747 / 2860.273
145: 149.816 / 2869.279
150: 105.536 / 2995.519
155: 115.772 / 2975.387
160: 123.387 / 3015.131
165: 150.781 / 3311.519
170: 110.421 / 3430.137
175: 100.590 / 3327.135
180: 95.589 / 3411.914
185: 152.021 / 3559.109
190: 144.883 / 3794.455
195: 105.107 / 3685.973
200: 144.764 / 3962.703
205: 122.904 / 4099.407
210: 115.182 / 4158.512
215: 172.386 / 4277.957
220: 240.916 / 4456.784
225: 146.540 / 4474.042
230: 131.276 / 4486.678
235: 151.520 / 4542.133
240: 171.146 / 4754.431
245: 84.421 / 4682.454
250: 97.255 / 4922.157
255: 123.576 / 4947.536
260: 132.240 / 4849.683
265: 129.829 / 5096.345
270: 173.453 / 5223.190
275: 167.401 / 5319.874
280: 123.620 / 5526.285
285: 154.038 / 5570.260
290: 135.443 / 5775.839
295: 138.618 / 5667.620
300: 132.015 / 5960.478
305: 97.755 / 5956.403
310: 95.089 / 6255.386
315: 125.859 / 6288.209
320: 112.844 / 6475.779
325: 112.399 / 6313.315
330: 135.931 / 6463.143
335: 160.208 / 6510.780
340: 91.807 / 6610.071
345: 134.126 / 6459.603
350: 137.479 / 6603.157
355: 104.537 / 6829.254
360: 132.744 / 6934.967
365: 71.686 / 6907.224
370: 141.044 / 7053.651
375: 79.050 / 6940.234
380: 158.517 / 7529.055
385: 125.749 / 7342.306
390: 86.852 / 7535.887
395: 157.328 / 7383.123
400: 81.227 / 7579.423
{'cayley': [{'best': 3776.039794921875, 'end': 5508.117846679687},
            {'best': 5501.281420898437, 'end': 8751.583984375},
            {'best': 2215.328649902344, 'end': 7579.422705078125}],
 'cayley_clusters': [{'best': 3197.7169921875, 'end': 5334.197119140625},
                     {'best': 5582.916357421875, 'end': 9221.503662109375},
                     {'best': 3253.177209472656, 'end': 8694.0875}]}
Loading data...
Number of training graphs: 1500
Train graph sizes: Counter({101: 46, 114: 45, 79: 44, 87: 40, 94: 39, 122: 39, 88: 37, 92: 37, 107: 36, 119: 36, 80: 34, 105: 34, 75: 33, 99: 33, 112: 33, 116: 33, 121: 33, 78: 32, 84: 32, 98: 32, 100: 32, 120: 32, 96: 31, 111: 31, 118: 31, 81: 29, 83: 28, 95: 28, 103: 28, 108: 28, 82: 27, 89: 27, 106: 27, 93: 26, 90: 26, 97: 26, 109: 25, 115: 25, 117: 25, 85: 24, 124: 24, 76: 22, 86: 22, 91: 22, 102: 22, 104: 22, 110: 22, 123: 22, 77: 19, 113: 19})
{25: [], 26: [], 27: [], 28: [], 29: [], 30: [], 31: [], 32: [], 33: [], 34: []}
Number of validation graphs: 1500
Val graph sizes: Counter({159: 12, 168: 11, 172: 11, 130: 9, 164: 9, 174: 9, 126: 8, 142: 8, 141: 8, 149: 8, 125: 7, 129: 7, 133: 7, 134: 7, 139: 7, 154: 7, 153: 7, 160: 7, 128: 6, 135: 6, 137: 6, 136: 6, 143: 6, 146: 6, 147: 6, 152: 6, 150: 6, 158: 6, 157: 6, 165: 6, 138: 5, 144: 5, 148: 5, 145: 5, 156: 5, 163: 5, 162: 5, 166: 5, 132: 4, 151: 4, 161: 4, 169: 4, 167: 4, 171: 4, 173: 4, 131: 3, 140: 3, 127: 2, 170: 2, 155: 1})
{'wandb': {'experiment_name': '', 'project': 'ColourInteract Sweeps Friday', 'entity': 'commute_opt_gnn'}, 'data': {'name': 'ColourInteract', 'dataset': 'LRGB', 'c1': 1.0, 'c2s': [0.01, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 100.0], 'num_colours': 4, 'train_size': 1500, 'val_size': 300, 'min_train_nodes': 75, 'max_train_nodes': 125, 'min_val_nodes': 125, 'max_val_nodes': 175, 'rewirers': ['cayley_clusters', 'cayley'], 'normalise': True, 'seed': 93, 'c2': 100.0}, 'model': {'hidden_channels': 8, 'num_layers': 5, 'drop_prob': 0.1, 'global_pool_aggr': 'global_mean_pool', 'approaches': ['only_original', 'interleave'], 'seeds': [17, 47, 23], 'approach': 'only_original', 'seed': 23}, 'train': {'lr': 0.0001, 'num_epochs': 400, 'print_every': 5, 'train_batch_size': 32, 'val_batch_size': 32}, 'run': {'silent': False}}
Device: cpu
No rewiring.
train targets: 3644.47 +/- 573.695
val targets: 5485.26 +/- 585.355
Rewirer None with seed 17
Using MSE Loss...
Minimum val loss at epoch 60: 368872.76875
Final val loss at epoch 400: 1786140.375
Train / Val loss by epoch
5: 19127790.000 / 29997821.800
10: 16948518.000 / 27110703.600
15: 13948073.000 / 23125627.000
20: 10447069.000 / 18224807.100
25: 7008019.500 / 12992953.200
30: 3933275.750 / 8206346.750
35: 1855417.750 / 4445153.675
40: 636281.062 / 2133933.825
45: 215234.516 / 999705.525
50: 89979.828 / 552403.397
55: 52560.945 / 416364.528
60: 44236.398 / 368872.769
65: 47816.043 / 401490.634
70: 47982.105 / 400590.455
75: 41971.250 / 407061.153
80: 46314.566 / 430271.722
85: 20913.553 / 431966.897
90: 36989.055 / 493143.819
95: 24916.451 / 512607.478
100: 27751.738 / 543664.950
105: 26456.094 / 557380.753
110: 27972.369 / 591051.581
115: 19270.275 / 583355.534
120: 17014.066 / 592103.878
125: 17026.809 / 631505.969
130: 21242.643 / 650771.269
135: 19809.346 / 664520.516
140: 16931.295 / 695463.078
145: 24431.137 / 715395.106
150: 25756.332 / 732059.416
155: 17185.355 / 748286.147
160: 19285.986 / 824801.438
165: 22526.803 / 856626.300
170: 12524.959 / 915670.181
175: 15944.068 / 903566.831
180: 14728.739 / 920298.925
185: 19145.045 / 1001420.194
190: 14032.578 / 1022557.238
195: 20782.328 / 1068797.194
200: 12856.964 / 1085885.056
205: 17016.369 / 1179623.837
210: 18527.014 / 1162350.887
215: 19166.998 / 1213583.944
220: 13872.562 / 1234168.700
225: 15356.609 / 1312060.644
230: 12169.357 / 1259077.194
235: 18128.539 / 1279101.175
240: 20432.137 / 1335567.006
245: 6768.637 / 1383424.938
250: 10642.057 / 1394767.481
255: 16947.859 / 1413475.844
260: 11296.353 / 1433576.538
265: 13274.903 / 1433383.525
270: 15213.360 / 1454985.656
275: 11346.416 / 1502223.000
280: 10898.314 / 1583658.137
285: 13011.661 / 1486329.462
290: 7894.874 / 1596951.650
295: 17672.418 / 1637838.950
300: 14583.817 / 1704150.175
305: 11842.383 / 1622198.600
310: 20198.449 / 1596897.300
315: 11549.462 / 1546474.269
320: 6890.332 / 1614059.562
325: 9407.224 / 1759374.525
330: 15162.819 / 1744188.887
335: 15928.521 / 1739967.438
340: 11104.130 / 1739831.500
345: 10350.439 / 1750799.512
350: 11685.021 / 1770087.250
355: 9573.310 / 1729507.525
360: 10638.867 / 1741496.850
365: 9357.302 / 1812706.363
370: 11281.459 / 1795551.550
375: 9959.917 / 1748156.900
380: 15198.411 / 1769526.837
385: 12320.485 / 1735892.337
390: 13880.478 / 1795707.262
395: 15857.698 / 1848842.988
400: 18640.947 / 1786140.375
Rewirer None with seed 47
Using MSE Loss...
Minimum val loss at epoch 70: 665716.5
Final val loss at epoch 400: 2146363.4375
Train / Val loss by epoch
5: 17826044.000 / 28061318.400
10: 15583795.000 / 25086404.600
15: 12619675.000 / 21028633.000
20: 9143865.000 / 16008952.500
25: 5772753.500 / 11071597.750
30: 3044274.250 / 6869098.150
35: 1231671.125 / 3886814.775
40: 449433.906 / 2124315.062
45: 123853.016 / 1237637.750
50: 51126.344 / 870573.700
55: 46138.668 / 726224.147
60: 53739.508 / 681471.050
65: 44196.219 / 669623.012
70: 28054.803 / 665716.500
75: 31887.047 / 700078.144
80: 34839.434 / 770999.806
85: 22820.549 / 818543.588
90: 21937.111 / 828845.412
95: 23880.139 / 805731.731
100: 22017.701 / 859030.219
105: 18931.703 / 878463.606
110: 16286.280 / 880804.319
115: 19478.072 / 909535.394
120: 10669.186 / 892410.400
125: 13543.075 / 912642.669
130: 17320.738 / 939223.169
135: 13576.084 / 936275.031
140: 12719.800 / 980333.688
145: 12952.462 / 996281.350
150: 13216.842 / 1040536.800
155: 19399.391 / 1071425.044
160: 15836.374 / 1062380.538
165: 13619.060 / 1089570.038
170: 6403.339 / 1141085.581
175: 11873.979 / 1163392.475
180: 8386.861 / 1215218.381
185: 11793.790 / 1265836.025
190: 16518.094 / 1224497.012
195: 20850.553 / 1233799.994
200: 13937.726 / 1292438.731
205: 15286.844 / 1349263.731
210: 14934.905 / 1334312.569
215: 7643.086 / 1382081.706
220: 22173.092 / 1446093.650
225: 12682.177 / 1458405.938
230: 9228.733 / 1460787.250
235: 14218.665 / 1447196.156
240: 11305.560 / 1499996.950
245: 13748.374 / 1569918.950
250: 12940.256 / 1521064.619
255: 16410.660 / 1551238.769
260: 13943.104 / 1647777.975
265: 11851.575 / 1630515.938
270: 12669.772 / 1638361.175
275: 10707.131 / 1705710.450
280: 12808.640 / 1688331.663
285: 12873.347 / 1746244.913
290: 17745.191 / 1787151.950
295: 9266.761 / 1733745.650
300: 10366.385 / 1796483.550
305: 12558.141 / 1840282.113
310: 8560.607 / 1828405.750
315: 10204.142 / 1846309.538
320: 13067.468 / 1840998.575
325: 12315.169 / 1883846.650
330: 14189.148 / 1871567.762
335: 14269.033 / 1963489.250
340: 10661.970 / 1894880.600
345: 9552.978 / 1971742.163
350: 13058.813 / 1962999.850
355: 9251.736 / 1982048.938
360: 15214.484 / 1981094.050
365: 15821.158 / 1972934.175
370: 12607.701 / 2032193.475
375: 12516.446 / 1995964.438
380: 13310.919 / 2017974.600
385: 11602.029 / 2042466.913
390: 11210.833 / 2062104.488
395: 12747.725 / 2082175.925
400: 14871.235 / 2146363.438
Rewirer None with seed 23
Using MSE Loss...
Minimum val loss at epoch 55: 315480.975
Final val loss at epoch 400: 2256579.8875
Train / Val loss by epoch
5: 17181594.000 / 27187297.000
10: 14412254.000 / 23379996.600
15: 11141461.000 / 18793043.400
20: 7730994.500 / 13774912.700
25: 4563363.000 / 9005137.650
30: 2298847.000 / 5058694.325
35: 901281.375 / 2447112.612
40: 297628.469 / 1110261.137
45: 90759.328 / 560896.459
50: 68476.086 / 359070.972
55: 61903.809 / 315480.975
60: 41453.543 / 335043.367
65: 44177.254 / 407013.869
70: 33724.066 / 460721.106
75: 32946.488 / 535989.256
80: 27709.732 / 567707.009
85: 24767.453 / 590948.403
90: 30656.021 / 597715.316
95: 21620.678 / 633849.988
100: 21588.861 / 626370.925
105: 15648.780 / 627307.734
110: 20841.611 / 674837.325
115: 15540.221 / 710042.869
120: 20496.553 / 766011.597
125: 17869.029 / 833099.700
130: 18448.566 / 929524.544
135: 15240.515 / 1016682.262
140: 15363.026 / 1095060.825
145: 18490.902 / 1189758.456
150: 17978.082 / 1311701.919
155: 21684.205 / 1304173.625
160: 9288.366 / 1318860.100
165: 11670.069 / 1408151.106
170: 14109.048 / 1462543.038
175: 10117.186 / 1535249.675
180: 9656.373 / 1482758.619
185: 12767.452 / 1548488.181
190: 13330.087 / 1640480.800
195: 10332.562 / 1629217.212
200: 13437.436 / 1646437.425
205: 10604.951 / 1670549.300
210: 13621.431 / 1703196.812
215: 17828.928 / 1819320.650
220: 20735.623 / 1837396.800
225: 11876.757 / 1781382.425
230: 14971.634 / 1765821.562
235: 14027.408 / 1731754.325
240: 12971.294 / 1781034.062
245: 14336.146 / 1733848.438
250: 13966.635 / 1815898.238
255: 16262.552 / 1793949.575
260: 11126.977 / 1812913.850
265: 13300.108 / 1753436.337
270: 13752.570 / 1824305.462
275: 15058.139 / 1845597.925
280: 10061.312 / 1939742.500
285: 20340.910 / 1982062.562
290: 14683.061 / 2013053.725
295: 13537.002 / 1899634.337
300: 11847.928 / 1889137.462
305: 12125.532 / 1954500.350
310: 10055.702 / 1982684.050
315: 12651.397 / 1978874.712
320: 14816.044 / 2024271.050
325: 12409.825 / 2018024.938
330: 11563.729 / 1990449.225
335: 19907.746 / 2008894.688
340: 8986.519 / 2014819.500
345: 8964.456 / 2043420.425
350: 12951.908 / 2075498.087
355: 9522.033 / 2188823.288
360: 11791.599 / 2145868.275
365: 8353.294 / 2080498.688
370: 12909.375 / 2156900.825
375: 8940.282 / 2078842.663
380: 10062.010 / 2157103.400
385: 9597.695 / 2163926.263
390: 7961.127 / 2208770.950
395: 12034.134 / 2130393.075
400: 10978.994 / 2256579.888
{None: [{'best': 368872.76875, 'end': 1786140.375},
        {'best': 665716.5, 'end': 2146363.4375},
        {'best': 315480.975, 'end': 2256579.8875}]}
{'wandb': {'experiment_name': '', 'project': 'ColourInteract Sweeps Friday', 'entity': 'commute_opt_gnn'}, 'data': {'name': 'ColourInteract', 'dataset': 'LRGB', 'c1': 1.0, 'c2s': [0.01, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 100.0], 'num_colours': 4, 'train_size': 1500, 'val_size': 300, 'min_train_nodes': 75, 'max_train_nodes': 125, 'min_val_nodes': 125, 'max_val_nodes': 175, 'rewirers': ['cayley_clusters', 'cayley'], 'normalise': True, 'seed': 93, 'c2': 100.0}, 'model': {'hidden_channels': 8, 'num_layers': 5, 'drop_prob': 0.1, 'global_pool_aggr': 'global_mean_pool', 'approaches': ['only_original', 'interleave'], 'seeds': [17, 47, 23], 'approach': 'interleave', 'seed': 23}, 'train': {'lr': 0.0001, 'num_epochs': 400, 'print_every': 5, 'train_batch_size': 32, 'val_batch_size': 32}, 'run': {'silent': False}}
Device: cpu
train targets: 3644.47 +/- 573.695
val targets: 5485.26 +/- 585.355
Rewirer cayley_clusters with seed 17
Using MSE Loss...
Minimum val loss at epoch 70: 270447.3234375
Final val loss at epoch 400: 1582885.85
Train / Val loss by epoch
5: 19117294.000 / 29959377.200
10: 16933744.000 / 27021441.200
15: 13950525.000 / 23132619.000
20: 10439221.000 / 18186600.100
25: 6993773.500 / 12848186.600
30: 3947103.250 / 7993002.650
35: 1862196.625 / 4288000.950
40: 645692.875 / 2025655.150
45: 210784.328 / 927398.447
50: 85100.117 / 492735.625
55: 54620.148 / 342675.769
60: 40628.254 / 287570.520
65: 58307.082 / 278780.686
70: 49318.039 / 270447.323
75: 39438.461 / 275965.642
80: 42668.137 / 292407.455
85: 18290.268 / 309431.748
90: 34612.426 / 349425.086
95: 23350.787 / 364310.266
100: 21818.873 / 403599.891
105: 23732.422 / 451820.056
110: 26577.982 / 518875.950
115: 18516.881 / 531559.575
120: 13632.700 / 572389.428
125: 16226.690 / 609624.975
130: 15336.438 / 628481.241
135: 16636.738 / 638949.762
140: 11475.144 / 656920.878
145: 19281.238 / 689432.162
150: 24839.605 / 722713.416
155: 16866.045 / 732202.381
160: 16394.861 / 799214.963
165: 20269.404 / 787763.509
170: 11493.882 / 821401.956
175: 12996.114 / 810793.150
180: 12282.065 / 807267.556
185: 13545.553 / 885232.150
190: 12885.328 / 892284.169
195: 20453.543 / 945953.150
200: 8664.213 / 931608.137
205: 16660.045 / 996996.438
210: 14050.894 / 991380.031
215: 18315.408 / 1010867.425
220: 16863.076 / 1038761.850
225: 12004.806 / 1084901.681
230: 11886.662 / 1073371.506
235: 15796.765 / 1080851.994
240: 16855.998 / 1122195.387
245: 7705.907 / 1152339.988
250: 8165.583 / 1192003.450
255: 14916.774 / 1190843.869
260: 10109.928 / 1205286.850
265: 10951.006 / 1223541.500
270: 11891.085 / 1215870.681
275: 12493.252 / 1282903.556
280: 8809.187 / 1283742.006
285: 11152.249 / 1276548.831
290: 8228.892 / 1340954.413
295: 19425.381 / 1372952.544
300: 14318.619 / 1440282.475
305: 9092.243 / 1353839.156
310: 18135.514 / 1349204.913
315: 9341.546 / 1308271.700
320: 6162.888 / 1379426.975
325: 9016.782 / 1493718.863
330: 14222.281 / 1482506.075
335: 15306.197 / 1491866.387
340: 9271.801 / 1478141.319
345: 8611.874 / 1486540.600
350: 8825.796 / 1529530.181
355: 6999.946 / 1491586.844
360: 8468.043 / 1549486.675
365: 7683.350 / 1578044.262
370: 10069.040 / 1520051.594
375: 11709.930 / 1522035.594
380: 12254.627 / 1524919.275
385: 10991.330 / 1523222.312
390: 10377.760 / 1576597.800
395: 14969.140 / 1614775.200
400: 13908.482 / 1582885.850
Rewirer cayley_clusters with seed 47
Using MSE Loss...
Minimum val loss at epoch 70: 720148.91875
Final val loss at epoch 400: 2025964.1375
Train / Val loss by epoch
5: 17817472.000 / 28044717.800
10: 15574363.000 / 25050449.800
15: 12608847.000 / 20971255.500
20: 9153662.000 / 16037764.900
25: 5785039.500 / 11129163.900
30: 3051926.750 / 6965335.600
35: 1237589.125 / 4013665.075
40: 451382.688 / 2245505.725
45: 124562.586 / 1352219.512
50: 50412.273 / 968626.281
55: 45596.574 / 812381.000
60: 50388.863 / 754373.131
65: 40808.965 / 733376.397
70: 24878.596 / 720148.919
75: 28894.141 / 750001.469
80: 29008.303 / 793413.988
85: 19482.816 / 832352.781
90: 20628.369 / 820967.975
95: 22156.787 / 808876.963
100: 18932.895 / 853296.887
105: 16359.912 / 869867.412
110: 15884.636 / 866459.731
115: 19714.072 / 893092.906
120: 11086.683 / 866041.900
125: 11806.997 / 882219.925
130: 16458.207 / 903352.863
135: 12672.812 / 909078.287
140: 11718.257 / 934053.344
145: 12786.154 / 939001.119
150: 12261.968 / 993407.644
155: 18467.877 / 1027712.900
160: 13839.294 / 1022447.250
165: 12223.592 / 1053208.962
170: 6362.327 / 1110866.975
175: 10398.263 / 1137332.106
180: 7322.330 / 1191692.900
185: 11188.806 / 1235983.175
190: 15093.769 / 1208781.000
195: 19630.207 / 1208366.663
200: 11761.826 / 1267763.337
205: 13767.175 / 1327665.675
210: 13024.442 / 1312732.869
215: 7755.665 / 1372149.756
220: 20206.451 / 1414801.312
225: 11912.300 / 1438700.744
230: 8454.602 / 1432289.356
235: 11984.499 / 1434098.225
240: 10779.479 / 1493539.975
245: 11403.933 / 1551585.137
250: 12578.077 / 1502736.806
255: 14620.447 / 1531526.337
260: 13148.349 / 1620992.925
265: 11341.325 / 1594225.988
270: 10619.006 / 1618917.475
275: 8865.971 / 1664707.288
280: 11616.201 / 1658176.288
285: 12007.688 / 1694813.238
290: 16557.703 / 1725499.137
295: 9390.049 / 1683817.300
300: 9936.047 / 1749349.875
305: 11457.505 / 1770128.562
310: 7671.594 / 1764013.438
315: 8761.061 / 1780865.988
320: 12631.017 / 1775179.800
325: 10051.767 / 1814559.988
330: 13442.849 / 1782869.175
335: 13102.561 / 1861050.613
340: 8861.187 / 1800162.850
345: 9926.146 / 1885490.575
350: 12032.854 / 1865653.337
355: 8386.553 / 1871493.625
360: 13986.891 / 1874064.137
365: 13224.806 / 1870525.738
370: 11960.358 / 1924319.625
375: 11912.123 / 1895012.837
380: 11449.227 / 1902718.587
385: 10788.423 / 1935999.762
390: 10914.921 / 1948905.725
395: 10927.002 / 1959519.100
400: 12161.244 / 2025964.137
Rewirer cayley_clusters with seed 23
Using MSE Loss...
Minimum val loss at epoch 55: 545291.575
Final val loss at epoch 400: 2142821.4625
Train / Val loss by epoch
5: 17174420.000 / 27182081.000
10: 14410163.000 / 23460230.900
15: 11116885.000 / 18954054.200
20: 7719952.000 / 13935894.900
25: 4569192.000 / 9191814.000
30: 2321553.250 / 5374261.400
35: 929657.562 / 2830724.562
40: 309615.031 / 1474566.038
45: 95201.078 / 852458.347
50: 67266.922 / 600591.222
55: 57732.348 / 545291.575
60: 31797.418 / 545949.006
65: 33696.094 / 548686.566
70: 28137.082 / 547607.397
75: 27862.924 / 585435.303
80: 24997.982 / 588171.275
85: 23509.740 / 631287.331
90: 33625.812 / 634487.491
95: 20292.311 / 689386.397
100: 15805.498 / 692595.628
105: 15590.635 / 733623.556
110: 16745.143 / 790527.006
115: 12685.484 / 829604.006
120: 19297.816 / 867040.438
125: 17088.693 / 913216.762
130: 17069.160 / 993211.281
135: 18600.484 / 1043824.863
140: 13203.912 / 1090982.019
145: 18938.381 / 1168193.556
150: 14437.021 / 1269646.887
155: 17547.373 / 1284256.250
160: 11704.460 / 1305092.344
165: 10790.650 / 1338984.681
170: 13045.467 / 1405872.925
175: 9626.339 / 1448908.887
180: 9421.714 / 1433431.156
185: 7804.297 / 1480587.962
190: 11009.908 / 1557486.375
195: 8915.420 / 1548650.394
200: 10560.864 / 1605241.925
205: 11274.344 / 1608525.988
210: 12574.087 / 1642010.500
215: 17057.848 / 1737311.413
220: 16141.822 / 1750257.375
225: 11444.619 / 1690626.825
230: 13412.041 / 1699988.413
235: 14033.914 / 1687793.025
240: 11545.595 / 1687632.887
245: 12458.919 / 1681499.863
250: 9626.058 / 1751272.625
255: 12437.476 / 1730258.212
260: 10417.898 / 1720170.738
265: 11144.971 / 1727182.125
270: 11802.390 / 1783473.712
275: 12470.892 / 1793353.012
280: 9909.405 / 1874848.337
285: 15997.719 / 1912485.225
290: 10433.683 / 1923013.775
295: 12626.591 / 1838404.863
300: 11237.109 / 1822821.375
305: 10956.921 / 1874203.663
310: 8229.228 / 1875983.425
315: 12246.994 / 1869142.062
320: 12115.663 / 1945828.462
325: 9505.515 / 1934183.950
330: 9601.122 / 1918783.137
335: 20086.316 / 1886006.262
340: 7926.189 / 1880870.150
345: 9687.673 / 1911410.925
350: 11792.776 / 1925237.275
355: 9446.798 / 2025899.850
360: 10423.499 / 2038125.163
365: 9054.876 / 1969773.050
370: 9906.564 / 2001356.387
375: 7464.292 / 1975046.288
380: 8269.652 / 2043936.875
385: 10204.733 / 2047172.275
390: 7166.637 / 2064214.850
395: 10052.838 / 2022414.913
400: 8471.934 / 2142821.462
Rewirer cayley with seed 17
Using MSE Loss...
Minimum val loss at epoch 80: 117395.380078125
Final val loss at epoch 400: 1409054.325
Train / Val loss by epoch
5: 19118768.000 / 30045440.400
10: 16925756.000 / 27083425.200
15: 13928015.000 / 22900810.700
20: 10377315.000 / 17559012.400
25: 6898488.000 / 11977472.500
30: 3843128.750 / 6940998.850
35: 1792691.375 / 3354670.975
40: 592880.125 / 1355045.962
45: 186806.891 / 498698.487
50: 80719.898 / 222012.341
55: 55982.074 / 147776.102
60: 53112.156 / 127426.084
65: 55819.582 / 122858.107
70: 46812.031 / 121565.281
75: 49183.719 / 119637.112
80: 49443.270 / 117395.380
85: 24085.939 / 118434.143
90: 32242.697 / 128911.252
95: 26342.746 / 145178.879
100: 26733.387 / 164191.384
105: 28544.381 / 189108.674
110: 18701.648 / 223276.794
115: 21451.635 / 246117.938
120: 15623.155 / 269533.775
125: 18264.863 / 294733.037
130: 18101.930 / 338922.816
135: 16594.619 / 356539.575
140: 20183.939 / 378390.869
145: 26088.830 / 410636.625
150: 24887.588 / 438609.647
155: 17778.496 / 446947.978
160: 17929.846 / 494955.103
165: 19698.764 / 503113.787
170: 14744.623 / 546032.447
175: 16609.180 / 532876.084
180: 15953.239 / 555600.031
185: 18049.793 / 636533.441
190: 16695.629 / 638865.416
195: 17473.064 / 698460.381
200: 16678.160 / 708496.656
205: 16311.462 / 788803.019
210: 15700.876 / 800545.428
215: 18449.814 / 854545.838
220: 14309.962 / 853311.275
225: 12251.751 / 926829.012
230: 12757.189 / 896333.938
235: 16920.125 / 933340.262
240: 22354.338 / 949078.706
245: 8871.451 / 990225.925
250: 12738.893 / 1014035.419
255: 17360.090 / 1024571.256
260: 10179.731 / 1050149.569
265: 15658.104 / 1045903.425
270: 14930.083 / 1065136.206
275: 8869.635 / 1105140.319
280: 10017.857 / 1129834.969
285: 11176.811 / 1101513.675
290: 8073.051 / 1170409.906
295: 19579.486 / 1179979.350
300: 15958.633 / 1239369.731
305: 13737.392 / 1178742.044
310: 14062.874 / 1180009.756
315: 14288.323 / 1178275.325
320: 9524.564 / 1188976.794
325: 10938.032 / 1328711.375
330: 11389.373 / 1282478.094
335: 15748.261 / 1311121.012
340: 9984.490 / 1311263.150
345: 11822.998 / 1332292.050
350: 16143.516 / 1349600.294
355: 7225.324 / 1307938.375
360: 10453.171 / 1350126.594
365: 11964.420 / 1404903.200
370: 13253.213 / 1376897.812
375: 10664.994 / 1339636.275
380: 15786.060 / 1346767.438
385: 13266.456 / 1357473.988
390: 12765.018 / 1428499.669
395: 15190.581 / 1412712.387
400: 16957.852 / 1409054.325
Rewirer cayley with seed 47
Using MSE Loss...
Minimum val loss at epoch 60: 697385.753125
Final val loss at epoch 400: 2075114.75
Train / Val loss by epoch
5: 17818510.000 / 28074669.600
10: 15571517.000 / 25017483.000
15: 12606632.000 / 20845490.700
20: 9153649.000 / 15856795.500
25: 5776148.000 / 10909812.200
30: 3045059.500 / 6771773.600
35: 1236369.375 / 3848995.650
40: 449590.219 / 2103977.025
45: 124619.367 / 1231353.663
50: 49777.152 / 866488.781
55: 45279.090 / 731504.037
60: 49284.727 / 697385.753
65: 41528.844 / 716422.175
70: 25921.379 / 753607.775
75: 29681.660 / 843286.512
80: 32910.430 / 924980.725
85: 24227.623 / 975716.012
90: 22672.354 / 979492.825
95: 24116.439 / 950554.481
100: 24359.365 / 996825.500
105: 17936.379 / 1015704.338
110: 18766.660 / 991545.406
115: 20416.764 / 1013999.425
120: 11878.278 / 993785.150
125: 13097.546 / 973925.369
130: 20296.838 / 999258.269
135: 12110.606 / 1003244.175
140: 14290.583 / 1022107.088
145: 13930.984 / 1022461.887
150: 15017.799 / 1067896.569
155: 18595.107 / 1091411.894
160: 16031.658 / 1078303.131
165: 13888.030 / 1098385.050
170: 6307.783 / 1151780.850
175: 11807.619 / 1175351.906
180: 9106.395 / 1227011.488
185: 12715.325 / 1262018.256
190: 15286.626 / 1236886.988
195: 23137.510 / 1241021.806
200: 13686.595 / 1299467.169
205: 15198.194 / 1354315.419
210: 14203.810 / 1349005.350
215: 8488.829 / 1399862.062
220: 21885.670 / 1446450.050
225: 14341.662 / 1496407.975
230: 9500.702 / 1475586.962
235: 14470.524 / 1464216.406
240: 10940.540 / 1530944.250
245: 11698.211 / 1580611.650
250: 14382.492 / 1535069.819
255: 16930.553 / 1550925.525
260: 12812.536 / 1651715.750
265: 12562.753 / 1624496.788
270: 12734.944 / 1640468.800
275: 11245.564 / 1700043.712
280: 12743.977 / 1694742.225
285: 11373.808 / 1724890.312
290: 16377.942 / 1771482.163
295: 9994.614 / 1697611.075
300: 11190.847 / 1777968.100
305: 11876.151 / 1797678.125
310: 8390.146 / 1803344.400
315: 11028.446 / 1810856.000
320: 13580.425 / 1802990.738
325: 10633.695 / 1839605.837
330: 14387.761 / 1820843.875
335: 14795.654 / 1913715.312
340: 10928.881 / 1835349.212
345: 11605.939 / 1925632.500
350: 12788.206 / 1890672.575
355: 8820.077 / 1915070.438
360: 15876.733 / 1907016.575
365: 14041.354 / 1927034.238
370: 13271.232 / 1955634.125
375: 13205.850 / 1933115.925
380: 11763.389 / 1935418.275
385: 11277.817 / 1962578.038
390: 10871.224 / 1984720.837
395: 11714.244 / 2004481.450
400: 13154.206 / 2075114.750
Rewirer cayley with seed 23
Using MSE Loss...
Minimum val loss at epoch 55: 393136.728125
Final val loss at epoch 400: 1990658.1375
Train / Val loss by epoch
5: 17180694.000 / 27134267.800
10: 14392381.000 / 23220812.600
15: 11083032.000 / 18495273.600
20: 7672902.500 / 13457952.000
25: 4516004.500 / 8730780.300
30: 2274953.250 / 4960881.250
35: 894995.312 / 2490469.200
40: 292080.781 / 1202788.925
45: 92791.867 / 644928.834
50: 71761.648 / 432517.516
55: 62668.637 / 393136.728
60: 32826.242 / 420793.847
65: 37148.520 / 474518.578
70: 31512.395 / 492542.309
75: 31734.580 / 528297.934
80: 24721.955 / 548869.213
85: 23824.170 / 569722.719
90: 27698.736 / 574509.963
95: 21012.951 / 596124.347
100: 19407.346 / 604486.169
105: 14099.109 / 617452.234
110: 19377.293 / 660519.141
115: 11712.828 / 678755.131
120: 19819.266 / 706931.781
125: 25014.225 / 728091.381
130: 21128.109 / 772934.613
135: 16713.203 / 809190.925
140: 15647.909 / 818979.606
145: 19057.275 / 861323.131
150: 15774.834 / 917470.781
155: 27363.979 / 939524.550
160: 12129.874 / 955720.738
165: 15102.173 / 1003181.756
170: 14381.877 / 1087932.144
175: 10058.585 / 1111736.175
180: 11803.422 / 1107105.675
185: 11894.359 / 1147525.919
190: 15030.392 / 1229608.469
195: 9506.333 / 1262854.181
200: 17055.139 / 1277174.094
205: 11950.906 / 1321535.144
210: 17506.551 / 1374884.175
215: 18063.770 / 1453566.800
220: 21090.115 / 1470911.538
225: 11990.036 / 1424934.631
230: 16542.395 / 1434110.206
235: 18123.213 / 1424520.975
240: 16723.434 / 1465914.562
245: 14597.156 / 1451041.650
250: 12138.872 / 1530961.525
255: 15756.088 / 1517908.381
260: 11977.284 / 1525961.900
265: 13119.835 / 1534279.806
270: 13285.278 / 1583301.925
275: 15517.789 / 1603563.325
280: 10068.000 / 1657305.288
285: 21346.229 / 1703598.188
290: 11487.853 / 1724177.750
295: 15712.333 / 1662716.675
300: 13272.712 / 1654251.750
305: 11202.972 / 1672263.413
310: 10588.846 / 1686953.550
315: 14262.050 / 1728119.800
320: 14380.062 / 1800299.525
325: 10983.276 / 1757082.825
330: 11339.483 / 1767879.300
335: 21021.885 / 1746214.663
340: 9524.695 / 1755768.425
345: 9799.885 / 1787128.125
350: 13175.800 / 1811427.950
355: 11416.758 / 1900125.163
360: 11432.748 / 1927199.663
365: 9125.220 / 1829053.163
370: 13702.054 / 1863217.613
375: 9258.715 / 1814615.087
380: 10020.505 / 1900564.350
385: 10489.883 / 1880413.875
390: 8986.645 / 1891332.038
395: 13794.318 / 1872883.387
400: 9549.516 / 1990658.137
{'cayley': [{'best': 117395.380078125, 'end': 1409054.325},
            {'best': 697385.753125, 'end': 2075114.75},
            {'best': 393136.728125, 'end': 1990658.1375}],
 'cayley_clusters': [{'best': 270447.3234375, 'end': 1582885.85},
                     {'best': 720148.91875, 'end': 2025964.1375},
                     {'best': 545291.575, 'end': 2142821.4625}]}
Time: Fri Mar  1 09:41:56 GMT 2024
