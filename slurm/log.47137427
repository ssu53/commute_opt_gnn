JobID: 47137427
======
Time: Fri Mar  1 09:51:56 GMT 2024
Running on master node: cpu-q-8
Loading data...
Number of training graphs: 1500
Train graph sizes: Counter({101: 46, 114: 45, 79: 44, 87: 40, 94: 39, 122: 39, 88: 37, 92: 37, 107: 36, 119: 36, 80: 34, 105: 34, 75: 33, 99: 33, 112: 33, 116: 33, 121: 33, 78: 32, 84: 32, 98: 32, 100: 32, 120: 32, 96: 31, 111: 31, 118: 31, 81: 29, 83: 28, 95: 28, 103: 28, 108: 28, 82: 27, 89: 27, 106: 27, 93: 26, 90: 26, 97: 26, 109: 25, 115: 25, 117: 25, 85: 24, 124: 24, 76: 22, 86: 22, 91: 22, 102: 22, 104: 22, 110: 22, 123: 22, 77: 19, 113: 19})
{25: [], 26: [], 27: [], 28: [], 29: [], 30: [], 31: [], 32: [], 33: [], 34: []}
Number of validation graphs: 1500
Val graph sizes: Counter({159: 12, 168: 11, 172: 11, 130: 9, 164: 9, 174: 9, 126: 8, 142: 8, 141: 8, 149: 8, 125: 7, 129: 7, 133: 7, 134: 7, 139: 7, 154: 7, 153: 7, 160: 7, 128: 6, 135: 6, 137: 6, 136: 6, 143: 6, 146: 6, 147: 6, 152: 6, 150: 6, 158: 6, 157: 6, 165: 6, 138: 5, 144: 5, 148: 5, 145: 5, 156: 5, 163: 5, 162: 5, 166: 5, 132: 4, 151: 4, 161: 4, 169: 4, 167: 4, 171: 4, 173: 4, 131: 3, 140: 3, 127: 2, 170: 2, 155: 1})
{'wandb': {'experiment_name': '', 'project': 'ColourInteract Sweeps Friday', 'entity': 'commute_opt_gnn'}, 'data': {'name': 'ColourInteract', 'dataset': 'LRGB', 'c1': 1.0, 'c2s': [0.01, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 100.0], 'num_colours': 4, 'train_size': 1500, 'val_size': 300, 'min_train_nodes': 75, 'max_train_nodes': 125, 'min_val_nodes': 125, 'max_val_nodes': 175, 'rewirers': ['cayley_clusters', 'cayley'], 'normalise': True, 'seed': 93, 'c2': 0.01}, 'model': {'hidden_channels': 8, 'num_layers': 5, 'drop_prob': 0.1, 'global_pool_aggr': 'global_mean_pool', 'approaches': ['only_diff'], 'seeds': [17, 47, 23], 'approach': 'only_diff'}, 'train': {'lr': 0.0001, 'num_epochs': 400, 'print_every': 5, 'train_batch_size': 32, 'val_batch_size': 32}, 'run': {'silent': False}}
Device: cpu
train targets: 3.38 +/- 0.220
val targets: 3.55 +/- 0.193
Rewirer cayley_clusters with seed 17
Using MSE Loss...
Minimum val loss at epoch 355: 0.10692550018429756
Final val loss at epoch 400: 0.31972826514393093
Train / Val loss by epoch
5: 41.272 / 92.568
10: 23.478 / 45.038
15: 21.374 / 29.202
20: 16.432 / 24.420
25: 12.594 / 19.966
30: 8.193 / 17.781
35: 8.581 / 12.966
40: 5.681 / 13.829
45: 6.857 / 9.677
50: 6.871 / 10.453
55: 4.731 / 8.434
60: 8.278 / 8.262
65: 7.442 / 8.209
70: 2.735 / 9.298
75: 4.240 / 7.901
80: 2.733 / 10.481
85: 2.800 / 10.746
90: 3.474 / 10.524
95: 3.041 / 11.712
100: 4.309 / 9.451
105: 2.788 / 9.942
110: 3.977 / 10.310
115: 2.241 / 9.822
120: 1.619 / 9.239
125: 3.138 / 10.570
130: 2.854 / 8.034
135: 2.704 / 7.310
140: 2.963 / 9.752
145: 2.936 / 7.280
150: 1.443 / 8.794
155: 1.575 / 7.017
160: 2.239 / 7.292
165: 1.124 / 6.860
170: 2.105 / 7.178
175: 3.071 / 5.661
180: 0.397 / 5.930
185: 1.716 / 6.247
190: 0.672 / 4.131
195: 1.007 / 4.179
200: 2.409 / 4.105
205: 1.081 / 3.448
210: 1.198 / 3.552
215: 0.740 / 3.244
220: 0.784 / 2.554
225: 0.671 / 1.931
230: 1.945 / 2.064
235: 1.665 / 2.225
240: 0.470 / 1.811
245: 0.416 / 1.308
250: 0.415 / 1.127
255: 0.538 / 1.093
260: 0.840 / 1.108
265: 0.592 / 0.819
270: 0.912 / 0.766
275: 0.632 / 0.594
280: 0.931 / 0.605
285: 1.637 / 0.636
290: 0.503 / 0.348
295: 0.897 / 0.456
300: 0.242 / 0.209
305: 0.511 / 0.181
310: 0.225 / 0.157
315: 0.320 / 0.158
320: 0.230 / 0.120
325: 0.202 / 0.124
330: 0.172 / 0.136
335: 0.271 / 0.108
340: 0.580 / 0.112
345: 0.527 / 0.123
350: 0.157 / 0.124
355: 0.357 / 0.107
360: 0.252 / 0.134
365: 0.297 / 0.148
370: 0.107 / 0.150
375: 0.256 / 0.153
380: 0.174 / 0.172
385: 0.138 / 0.171
390: 0.145 / 0.257
395: 0.140 / 0.210
400: 0.123 / 0.320
Rewirer cayley_clusters with seed 47
Using MSE Loss...
Minimum val loss at epoch 290: 0.2034262154251337
Final val loss at epoch 400: 0.6062113240361213
Train / Val loss by epoch
5: 14.098 / 7.956
10: 21.632 / 6.237
15: 13.587 / 5.848
20: 9.072 / 5.222
25: 13.473 / 3.489
30: 6.888 / 4.083
35: 7.304 / 2.421
40: 9.013 / 2.387
45: 9.881 / 1.852
50: 9.075 / 1.631
55: 2.722 / 1.467
60: 6.837 / 1.499
65: 4.844 / 1.468
70: 5.829 / 1.195
75: 2.171 / 1.359
80: 6.247 / 1.553
85: 3.193 / 1.724
90: 3.476 / 2.209
95: 1.776 / 2.326
100: 2.325 / 2.468
105: 3.425 / 2.011
110: 2.490 / 2.234
115: 1.971 / 2.358
120: 2.417 / 2.792
125: 1.969 / 2.639
130: 1.698 / 2.631
135: 1.509 / 2.114
140: 1.097 / 2.093
145: 1.958 / 2.481
150: 1.067 / 2.565
155: 1.284 / 2.133
160: 0.893 / 2.062
165: 1.199 / 2.435
170: 1.387 / 1.954
175: 1.289 / 1.874
180: 1.760 / 1.033
185: 1.088 / 1.759
190: 0.967 / 1.330
195: 1.026 / 1.067
200: 1.275 / 1.075
205: 0.941 / 0.839
210: 1.054 / 0.732
215: 0.664 / 0.615
220: 0.865 / 0.618
225: 0.771 / 0.480
230: 0.912 / 0.657
235: 0.639 / 0.394
240: 1.320 / 0.420
245: 0.704 / 0.342
250: 0.405 / 0.279
255: 0.696 / 0.331
260: 0.779 / 0.259
265: 0.622 / 0.242
270: 0.562 / 0.242
275: 0.444 / 0.237
280: 0.749 / 0.222
285: 0.370 / 0.209
290: 0.769 / 0.203
295: 0.424 / 0.235
300: 0.461 / 0.302
305: 0.314 / 0.238
310: 0.404 / 0.272
315: 0.354 / 0.237
320: 0.294 / 0.273
325: 0.196 / 0.316
330: 0.318 / 0.293
335: 0.222 / 0.299
340: 0.262 / 0.309
345: 0.324 / 0.292
350: 0.147 / 0.410
355: 0.239 / 0.331
360: 0.174 / 0.408
365: 0.403 / 0.297
370: 0.364 / 0.452
375: 0.111 / 0.399
380: 0.248 / 0.501
385: 0.183 / 0.462
390: 0.281 / 0.498
395: 0.091 / 0.616
400: 0.081 / 0.606
Rewirer cayley_clusters with seed 23
Using MSE Loss...
Minimum val loss at epoch 250: 0.808242167532444
Final val loss at epoch 400: 1.3964068382978438
Train / Val loss by epoch
5: 28.270 / 17.449
10: 23.937 / 13.157
15: 15.303 / 15.278
20: 17.049 / 17.264
25: 11.553 / 18.231
30: 6.176 / 18.864
35: 8.162 / 19.157
40: 8.220 / 22.072
45: 8.866 / 24.439
50: 10.602 / 23.054
55: 3.115 / 28.146
60: 6.290 / 29.128
65: 3.219 / 28.387
70: 5.749 / 31.893
75: 4.150 / 27.053
80: 4.624 / 24.059
85: 4.011 / 21.427
90: 2.825 / 20.273
95: 3.669 / 16.506
100: 2.616 / 15.890
105: 3.312 / 10.826
110: 3.904 / 9.688
115: 1.334 / 10.712
120: 4.151 / 8.424
125: 1.312 / 5.992
130: 1.154 / 5.908
135: 2.137 / 4.377
140: 1.268 / 4.063
145: 1.742 / 3.577
150: 1.323 / 2.717
155: 1.896 / 2.314
160: 4.265 / 1.632
165: 0.696 / 2.161
170: 1.685 / 1.504
175: 0.671 / 1.550
180: 0.836 / 1.096
185: 0.994 / 1.555
190: 2.249 / 1.004
195: 0.723 / 1.142
200: 0.479 / 1.274
205: 0.372 / 1.358
210: 0.450 / 1.276
215: 0.860 / 1.141
220: 0.374 / 1.024
225: 0.764 / 1.062
230: 1.146 / 1.032
235: 0.508 / 1.104
240: 0.681 / 0.849
245: 0.450 / 1.026
250: 1.166 / 0.808
255: 0.634 / 1.015
260: 0.200 / 1.046
265: 1.709 / 0.814
270: 0.424 / 0.989
275: 0.268 / 0.961
280: 0.259 / 1.646
285: 0.452 / 1.335
290: 0.845 / 1.089
295: 0.161 / 1.309
300: 0.152 / 1.386
305: 0.779 / 1.190
310: 0.241 / 1.285
315: 0.193 / 1.338
320: 0.115 / 1.263
325: 0.372 / 1.195
330: 0.135 / 1.470
335: 0.108 / 1.516
340: 0.185 / 1.279
345: 0.089 / 1.319
350: 0.267 / 1.416
355: 0.148 / 1.515
360: 0.088 / 1.590
365: 0.203 / 1.535
370: 0.173 / 1.517
375: 0.080 / 1.465
380: 0.389 / 1.476
385: 0.059 / 1.442
390: 0.051 / 1.461
395: 0.080 / 1.392
400: 0.044 / 1.396
Rewirer cayley with seed 17
Using MSE Loss...
Minimum val loss at epoch 275: 0.4010810673236847
Final val loss at epoch 400: 0.8302070565521718
Train / Val loss by epoch
5: 19.460 / 42.895
10: 35.678 / 18.963
15: 23.036 / 15.966
20: 14.061 / 13.719
25: 12.030 / 10.430
30: 8.316 / 10.125
35: 4.663 / 9.655
40: 7.124 / 11.016
45: 5.311 / 6.917
50: 4.426 / 8.958
55: 4.551 / 8.510
60: 3.951 / 9.605
65: 2.790 / 9.665
70: 5.521 / 10.970
75: 4.849 / 9.005
80: 3.172 / 10.850
85: 3.462 / 10.428
90: 2.021 / 9.744
95: 2.212 / 11.222
100: 2.714 / 11.105
105: 2.639 / 9.440
110: 3.152 / 11.127
115: 1.752 / 8.596
120: 2.130 / 8.671
125: 3.531 / 8.722
130: 2.462 / 7.777
135: 2.005 / 6.966
140: 1.838 / 6.717
145: 3.042 / 6.794
150: 1.197 / 4.630
155: 1.520 / 4.503
160: 1.088 / 4.169
165: 1.386 / 3.896
170: 3.311 / 4.242
175: 1.743 / 2.664
180: 1.059 / 3.027
185: 0.841 / 3.125
190: 1.137 / 2.151
195: 1.274 / 2.361
200: 1.340 / 2.450
205: 1.093 / 1.637
210: 1.132 / 1.710
215: 1.028 / 1.456
220: 1.009 / 1.188
225: 1.379 / 1.362
230: 1.805 / 1.063
235: 0.941 / 0.908
240: 0.683 / 0.755
245: 0.345 / 0.632
250: 0.691 / 0.516
255: 0.367 / 0.480
260: 1.910 / 0.509
265: 0.655 / 0.423
270: 1.170 / 0.426
275: 0.553 / 0.401
280: 0.749 / 0.426
285: 1.311 / 0.489
290: 0.571 / 0.409
295: 0.835 / 0.410
300: 0.411 / 0.458
305: 1.040 / 0.450
310: 0.239 / 0.423
315: 0.865 / 0.497
320: 0.373 / 0.700
325: 0.308 / 0.706
330: 0.276 / 0.685
335: 0.173 / 0.674
340: 0.499 / 0.701
345: 0.504 / 0.754
350: 0.168 / 0.809
355: 0.482 / 0.590
360: 0.180 / 0.829
365: 0.177 / 0.799
370: 0.138 / 0.895
375: 0.153 / 0.741
380: 0.159 / 0.820
385: 0.238 / 0.719
390: 0.099 / 0.786
395: 0.147 / 0.843
400: 0.147 / 0.830
Rewirer cayley with seed 47
Using MSE Loss...
Minimum val loss at epoch 360: 0.10628635697066784
Final val loss at epoch 400: 0.2766591068357229
Train / Val loss by epoch
5: 23.520 / 127.557
10: 15.271 / 115.286
15: 19.105 / 104.498
20: 10.022 / 95.211
25: 14.760 / 88.292
30: 5.526 / 74.443
35: 6.170 / 69.586
40: 11.323 / 63.205
45: 7.974 / 64.315
50: 8.493 / 59.090
55: 5.253 / 50.733
60: 6.343 / 50.539
65: 5.469 / 49.166
70: 6.146 / 37.049
75: 3.894 / 36.696
80: 3.847 / 31.110
85: 3.333 / 29.721
90: 4.299 / 28.510
95: 2.879 / 26.252
100: 4.175 / 25.204
105: 3.882 / 21.266
110: 3.293 / 22.006
115: 3.080 / 19.263
120: 1.657 / 16.478
125: 1.789 / 16.506
130: 1.640 / 13.160
135: 1.853 / 12.207
140: 1.650 / 8.971
145: 1.953 / 9.515
150: 1.974 / 8.316
155: 1.230 / 7.566
160: 1.688 / 6.960
165: 1.534 / 7.439
170: 1.432 / 5.378
175: 1.058 / 4.844
180: 1.669 / 3.929
185: 1.217 / 4.292
190: 1.085 / 4.057
195: 1.362 / 3.294
200: 1.085 / 2.282
205: 0.751 / 2.699
210: 1.301 / 2.203
215: 0.889 / 2.031
220: 0.727 / 1.843
225: 0.868 / 1.613
230: 0.900 / 2.160
235: 0.753 / 1.058
240: 0.930 / 1.263
245: 0.604 / 1.376
250: 0.424 / 0.976
255: 0.729 / 1.034
260: 0.604 / 0.541
265: 0.439 / 0.698
270: 0.531 / 0.618
275: 0.406 / 0.575
280: 0.490 / 0.556
285: 0.574 / 0.367
290: 0.270 / 0.304
295: 0.355 / 0.409
300: 0.310 / 0.180
305: 0.347 / 0.205
310: 0.370 / 0.149
315: 0.356 / 0.173
320: 0.459 / 0.119
325: 0.447 / 0.114
330: 0.510 / 0.113
335: 0.226 / 0.121
340: 0.344 / 0.110
345: 0.286 / 0.118
350: 0.313 / 0.128
355: 0.251 / 0.158
360: 0.425 / 0.106
365: 0.222 / 0.129
370: 0.199 / 0.202
375: 0.186 / 0.288
380: 0.302 / 0.224
385: 0.223 / 0.183
390: 0.225 / 0.331
395: 0.140 / 0.231
400: 0.150 / 0.277
Rewirer cayley with seed 23
Using MSE Loss...
Minimum val loss at epoch 200: 0.2867223843932152
Final val loss at epoch 400: 1.7348831921815873
Train / Val loss by epoch
5: 32.682 / 26.686
10: 38.784 / 18.635
15: 18.235 / 15.942
20: 24.513 / 13.760
25: 11.088 / 13.360
30: 7.907 / 12.979
35: 6.081 / 12.363
40: 6.451 / 12.035
45: 8.955 / 10.163
50: 10.316 / 10.253
55: 6.933 / 8.429
60: 4.491 / 7.239
65: 4.018 / 6.102
70: 4.232 / 5.185
75: 4.131 / 5.184
80: 4.254 / 4.241
85: 4.411 / 4.230
90: 4.931 / 3.654
95: 2.598 / 3.035
100: 2.501 / 2.714
105: 2.830 / 2.602
110: 3.884 / 2.244
115: 2.212 / 2.026
120: 2.921 / 1.745
125: 4.023 / 1.664
130: 1.406 / 1.394
135: 3.933 / 1.290
140: 1.775 / 1.081
145: 1.143 / 0.949
150: 0.966 / 0.861
155: 1.792 / 0.754
160: 3.361 / 0.646
165: 1.785 / 0.583
170: 1.578 / 0.532
175: 0.771 / 0.475
180: 1.568 / 0.416
185: 0.869 / 0.354
190: 2.236 / 0.308
195: 0.662 / 0.295
200: 1.601 / 0.287
205: 0.687 / 0.335
210: 0.574 / 0.327
215: 0.790 / 0.447
220: 0.200 / 0.586
225: 2.131 / 0.477
230: 1.125 / 0.557
235: 1.345 / 0.515
240: 0.485 / 0.472
245: 1.006 / 0.484
250: 0.554 / 0.529
255: 0.655 / 0.550
260: 0.243 / 0.764
265: 1.308 / 0.675
270: 1.101 / 0.681
275: 0.319 / 0.934
280: 0.105 / 1.229
285: 0.297 / 1.221
290: 0.472 / 1.139
295: 0.275 / 1.495
300: 0.208 / 1.700
305: 0.693 / 1.363
310: 0.281 / 1.431
315: 0.179 / 1.320
320: 0.153 / 1.545
325: 0.253 / 1.624
330: 0.322 / 1.764
335: 0.159 / 2.030
340: 0.145 / 1.756
345: 0.189 / 1.712
350: 0.108 / 1.883
355: 0.143 / 2.043
360: 0.073 / 1.898
365: 0.093 / 1.848
370: 0.066 / 2.018
375: 0.103 / 1.801
380: 0.127 / 1.763
385: 0.098 / 1.755
390: 0.076 / 1.866
395: 0.041 / 1.525
400: 0.100 / 1.735
{'cayley': [{'best': 0.4010810673236847, 'end': 0.8302070565521718},
            {'best': 0.10628635697066784, 'end': 0.2766591068357229},
            {'best': 0.2867223843932152, 'end': 1.7348831921815873}],
 'cayley_clusters': [{'best': 0.10692550018429756, 'end': 0.31972826514393093},
                     {'best': 0.2034262154251337, 'end': 0.6062113240361213},
                     {'best': 0.808242167532444, 'end': 1.3964068382978438}]}
Loading data...
Number of training graphs: 1500
Train graph sizes: Counter({101: 46, 114: 45, 79: 44, 87: 40, 94: 39, 122: 39, 88: 37, 92: 37, 107: 36, 119: 36, 80: 34, 105: 34, 75: 33, 99: 33, 112: 33, 116: 33, 121: 33, 78: 32, 84: 32, 98: 32, 100: 32, 120: 32, 96: 31, 111: 31, 118: 31, 81: 29, 83: 28, 95: 28, 103: 28, 108: 28, 82: 27, 89: 27, 106: 27, 93: 26, 90: 26, 97: 26, 109: 25, 115: 25, 117: 25, 85: 24, 124: 24, 76: 22, 86: 22, 91: 22, 102: 22, 104: 22, 110: 22, 123: 22, 77: 19, 113: 19})
{25: [], 26: [], 27: [], 28: [], 29: [], 30: [], 31: [], 32: [], 33: [], 34: []}
Number of validation graphs: 1500
Val graph sizes: Counter({159: 12, 168: 11, 172: 11, 130: 9, 164: 9, 174: 9, 126: 8, 142: 8, 141: 8, 149: 8, 125: 7, 129: 7, 133: 7, 134: 7, 139: 7, 154: 7, 153: 7, 160: 7, 128: 6, 135: 6, 137: 6, 136: 6, 143: 6, 146: 6, 147: 6, 152: 6, 150: 6, 158: 6, 157: 6, 165: 6, 138: 5, 144: 5, 148: 5, 145: 5, 156: 5, 163: 5, 162: 5, 166: 5, 132: 4, 151: 4, 161: 4, 169: 4, 167: 4, 171: 4, 173: 4, 131: 3, 140: 3, 127: 2, 170: 2, 155: 1})
{'wandb': {'experiment_name': '', 'project': 'ColourInteract Sweeps Friday', 'entity': 'commute_opt_gnn'}, 'data': {'name': 'ColourInteract', 'dataset': 'LRGB', 'c1': 1.0, 'c2s': [0.01, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 100.0], 'num_colours': 4, 'train_size': 1500, 'val_size': 300, 'min_train_nodes': 75, 'max_train_nodes': 125, 'min_val_nodes': 125, 'max_val_nodes': 175, 'rewirers': ['cayley_clusters', 'cayley'], 'normalise': True, 'seed': 93, 'c2': 0.1}, 'model': {'hidden_channels': 8, 'num_layers': 5, 'drop_prob': 0.1, 'global_pool_aggr': 'global_mean_pool', 'approaches': ['only_diff'], 'seeds': [17, 47, 23], 'approach': 'only_diff', 'seed': 23}, 'train': {'lr': 0.0001, 'num_epochs': 400, 'print_every': 5, 'train_batch_size': 32, 'val_batch_size': 32}, 'run': {'silent': False}}
Device: cpu
train targets: 6.66 +/- 0.654
val targets: 8.49 +/- 0.669
Rewirer cayley_clusters with seed 17
Using MSE Loss...
Minimum val loss at epoch 395: 0.13754212334752083
Final val loss at epoch 400: 0.15545164793729782
Train / Val loss by epoch
5: 45.387 / 117.770
10: 24.513 / 50.726
15: 22.144 / 33.373
20: 16.897 / 28.270
25: 13.409 / 23.270
30: 8.697 / 20.700
35: 8.812 / 15.520
40: 5.582 / 16.327
45: 6.901 / 11.737
50: 6.732 / 12.752
55: 4.606 / 10.117
60: 8.704 / 9.960
65: 7.586 / 9.943
70: 2.776 / 11.295
75: 4.383 / 9.600
80: 3.202 / 12.554
85: 2.934 / 12.987
90: 3.682 / 12.816
95: 3.316 / 14.227
100: 4.516 / 11.483
105: 3.112 / 12.335
110: 4.126 / 12.589
115: 2.407 / 11.891
120: 1.733 / 11.322
125: 3.397 / 12.725
130: 3.266 / 10.193
135: 3.058 / 9.107
140: 3.125 / 11.835
145: 2.986 / 8.955
150: 1.463 / 10.764
155: 1.559 / 8.839
160: 2.527 / 9.254
165: 1.275 / 8.610
170: 2.214 / 9.140
175: 3.239 / 7.196
180: 0.451 / 7.409
185: 1.751 / 7.804
190: 0.831 / 5.529
195: 1.229 / 5.256
200: 2.265 / 5.059
205: 1.257 / 4.552
210: 1.287 / 4.597
215: 0.848 / 4.267
220: 0.803 / 3.381
225: 0.763 / 2.798
230: 2.065 / 2.820
235: 1.725 / 2.984
240: 0.578 / 2.406
245: 0.468 / 1.861
250: 0.532 / 1.624
255: 0.666 / 1.584
260: 0.921 / 1.687
265: 0.698 / 1.216
270: 1.064 / 1.254
275: 0.660 / 0.973
280: 1.124 / 1.064
285: 1.818 / 1.101
290: 0.553 / 0.674
295: 1.009 / 0.816
300: 0.284 / 0.502
305: 0.650 / 0.483
310: 0.333 / 0.442
315: 0.492 / 0.461
320: 0.322 / 0.278
325: 0.284 / 0.222
330: 0.239 / 0.195
335: 0.357 / 0.263
340: 0.437 / 0.227
345: 0.474 / 0.203
350: 0.209 / 0.185
355: 0.462 / 0.230
360: 0.355 / 0.170
365: 0.414 / 0.158
370: 0.137 / 0.154
375: 0.222 / 0.149
380: 0.230 / 0.144
385: 0.172 / 0.140
390: 0.179 / 0.155
395: 0.203 / 0.138
400: 0.188 / 0.155
Rewirer cayley_clusters with seed 47
Using MSE Loss...
Minimum val loss at epoch 315: 0.22325308620929718
Final val loss at epoch 400: 0.4992981493473053
Train / Val loss by epoch
5: 13.978 / 7.472
10: 21.045 / 5.472
15: 13.208 / 4.619
20: 8.745 / 3.938
25: 13.344 / 2.982
30: 6.448 / 2.811
35: 6.917 / 2.206
40: 9.148 / 1.979
45: 9.555 / 1.706
50: 8.677 / 1.691
55: 2.761 / 2.008
60: 6.505 / 2.411
65: 4.530 / 2.433
70: 5.411 / 2.147
75: 1.925 / 2.489
80: 5.810 / 2.902
85: 3.248 / 3.187
90: 3.501 / 3.789
95: 1.668 / 4.199
100: 2.239 / 4.155
105: 3.601 / 3.447
110: 2.304 / 3.746
115: 1.700 / 3.836
120: 2.136 / 4.367
125: 1.966 / 4.210
130: 1.480 / 3.873
135: 1.396 / 3.473
140: 1.166 / 3.219
145: 1.725 / 3.627
150: 0.998 / 3.625
155: 1.115 / 3.246
160: 1.011 / 3.086
165: 1.206 / 3.622
170: 1.268 / 2.827
175: 1.198 / 2.665
180: 1.820 / 1.703
185: 1.127 / 2.528
190: 0.845 / 1.980
195: 1.026 / 1.625
200: 1.314 / 1.692
205: 1.145 / 1.427
210: 1.024 / 1.256
215: 0.665 / 1.032
220: 0.826 / 0.994
225: 0.812 / 0.799
230: 0.815 / 1.090
235: 0.606 / 0.719
240: 1.232 / 0.790
245: 0.782 / 0.586
250: 0.345 / 0.501
255: 0.636 / 0.537
260: 0.812 / 0.450
265: 0.619 / 0.357
270: 0.689 / 0.375
275: 0.389 / 0.355
280: 0.704 / 0.333
285: 0.365 / 0.262
290: 0.725 / 0.293
295: 0.451 / 0.232
300: 0.428 / 0.240
305: 0.348 / 0.227
310: 0.423 / 0.224
315: 0.326 / 0.223
320: 0.279 / 0.225
325: 0.248 / 0.245
330: 0.399 / 0.255
335: 0.224 / 0.242
340: 0.303 / 0.234
345: 0.306 / 0.250
350: 0.174 / 0.292
355: 0.184 / 0.278
360: 0.185 / 0.312
365: 0.366 / 0.272
370: 0.338 / 0.363
375: 0.131 / 0.360
380: 0.238 / 0.419
385: 0.164 / 0.361
390: 0.239 / 0.485
395: 0.142 / 0.492
400: 0.097 / 0.499
Rewirer cayley_clusters with seed 23
Using MSE Loss...
Minimum val loss at epoch 240: 0.6757160350680351
Final val loss at epoch 400: 0.8605461895465851
Train / Val loss by epoch
5: 27.075 / 15.238
10: 23.431 / 10.520
15: 15.093 / 12.115
20: 16.829 / 13.750
25: 11.131 / 14.414
30: 5.839 / 15.226
35: 7.951 / 15.427
40: 7.810 / 17.869
45: 8.411 / 19.793
50: 10.493 / 18.600
55: 2.984 / 22.884
60: 5.977 / 23.390
65: 3.047 / 22.527
70: 5.264 / 25.245
75: 3.874 / 21.141
80: 4.388 / 18.004
85: 3.859 / 15.593
90: 2.846 / 14.531
95: 3.340 / 11.935
100: 2.665 / 11.380
105: 3.091 / 7.327
110: 3.864 / 6.968
115: 1.238 / 7.507
120: 4.085 / 5.836
125: 1.227 / 3.858
130: 1.115 / 3.838
135: 2.128 / 2.783
140: 1.172 / 2.783
145: 1.986 / 2.510
150: 1.421 / 1.766
155: 1.722 / 1.652
160: 3.923 / 1.183
165: 0.884 / 1.667
170: 1.449 / 1.180
175: 0.683 / 1.289
180: 0.942 / 0.930
185: 0.939 / 1.282
190: 2.390 / 0.813
195: 0.949 / 0.947
200: 0.391 / 1.199
205: 0.334 / 1.219
210: 0.656 / 1.129
215: 1.340 / 0.966
220: 0.490 / 0.885
225: 1.100 / 0.832
230: 1.341 / 0.907
235: 0.744 / 0.949
240: 0.773 / 0.676
245: 0.620 / 0.921
250: 1.228 / 0.723
255: 0.718 / 0.880
260: 0.221 / 0.912
265: 1.093 / 0.759
270: 0.575 / 0.844
275: 0.269 / 0.858
280: 0.358 / 1.298
285: 0.636 / 1.050
290: 0.936 / 0.863
295: 0.220 / 0.961
300: 0.275 / 0.983
305: 0.596 / 0.800
310: 0.300 / 0.896
315: 0.431 / 0.772
320: 0.114 / 0.797
325: 0.497 / 0.765
330: 0.407 / 0.879
335: 0.221 / 0.935
340: 0.299 / 0.769
345: 0.207 / 0.797
350: 0.402 / 0.921
355: 0.381 / 0.932
360: 0.194 / 1.056
365: 0.427 / 0.869
370: 0.364 / 0.895
375: 0.250 / 0.896
380: 0.636 / 0.965
385: 0.125 / 0.918
390: 0.095 / 0.848
395: 0.107 / 0.846
400: 0.122 / 0.861
Rewirer cayley with seed 17
Using MSE Loss...
Minimum val loss at epoch 390: 0.17411259189248085
Final val loss at epoch 400: 0.18185831494629384
Train / Val loss by epoch
5: 21.079 / 57.816
10: 36.887 / 22.042
15: 23.123 / 18.446
20: 14.415 / 15.769
25: 12.374 / 11.854
30: 8.597 / 11.446
35: 4.798 / 10.967
40: 6.959 / 12.204
45: 5.204 / 7.938
50: 4.736 / 10.039
55: 4.837 / 9.505
60: 3.708 / 10.608
65: 2.696 / 10.678
70: 5.509 / 11.979
75: 5.064 / 9.982
80: 3.417 / 12.068
85: 3.743 / 11.502
90: 2.029 / 10.982
95: 2.424 / 12.565
100: 2.780 / 12.640
105: 2.842 / 10.593
110: 3.326 / 12.599
115: 2.120 / 10.075
120: 2.357 / 10.054
125: 3.673 / 10.286
130: 2.541 / 9.202
135: 2.052 / 8.617
140: 1.899 / 8.188
145: 3.112 / 8.166
150: 1.331 / 6.229
155: 1.697 / 6.035
160: 1.325 / 5.680
165: 1.468 / 5.395
170: 3.653 / 5.949
175: 1.954 / 3.977
180: 1.216 / 4.436
185: 0.922 / 4.427
190: 1.187 / 3.287
195: 1.362 / 3.596
200: 1.514 / 3.707
205: 1.380 / 2.754
210: 1.279 / 2.769
215: 1.074 / 2.402
220: 1.012 / 1.920
225: 1.368 / 2.024
230: 1.804 / 1.688
235: 1.008 / 1.480
240: 0.735 / 1.220
245: 0.402 / 1.098
250: 0.746 / 0.900
255: 0.362 / 0.762
260: 1.844 / 0.831
265: 0.722 / 0.528
270: 1.480 / 0.716
275: 0.685 / 0.516
280: 0.817 / 0.475
285: 1.053 / 0.455
290: 0.539 / 0.382
295: 0.852 / 0.405
300: 0.455 / 0.366
305: 1.190 / 0.363
310: 0.276 / 0.332
315: 0.880 / 0.314
320: 0.304 / 0.343
325: 0.228 / 0.306
330: 0.285 / 0.287
335: 0.244 / 0.273
340: 0.521 / 0.271
345: 0.353 / 0.252
350: 0.249 / 0.243
355: 0.408 / 0.209
360: 0.203 / 0.216
365: 0.173 / 0.207
370: 0.163 / 0.210
375: 0.209 / 0.185
380: 0.199 / 0.196
385: 0.278 / 0.175
390: 0.172 / 0.174
395: 0.229 / 0.182
400: 0.160 / 0.182
Rewirer cayley with seed 47
Using MSE Loss...
Minimum val loss at epoch 365: 0.1272427521646023
Final val loss at epoch 400: 0.2555388420820236
Train / Val loss by epoch
5: 23.511 / 129.614
10: 15.053 / 115.591
15: 18.970 / 105.547
20: 9.784 / 95.921
25: 14.486 / 89.710
30: 5.280 / 75.873
35: 6.083 / 70.863
40: 11.179 / 63.433
45: 7.677 / 64.378
50: 8.266 / 58.638
55: 4.875 / 50.342
60: 6.067 / 50.884
65: 5.067 / 49.166
70: 5.879 / 37.967
75: 3.476 / 36.340
80: 3.633 / 31.654
85: 3.447 / 30.053
90: 4.289 / 29.574
95: 2.876 / 27.373
100: 4.193 / 26.202
105: 3.803 / 22.550
110: 3.107 / 22.725
115: 2.773 / 19.508
120: 1.591 / 16.786
125: 1.860 / 16.693
130: 1.703 / 13.375
135: 1.994 / 12.870
140: 1.595 / 9.292
145: 1.885 / 9.739
150: 1.917 / 8.582
155: 1.177 / 7.847
160: 1.682 / 6.939
165: 1.622 / 7.803
170: 1.365 / 5.552
175: 0.992 / 5.254
180: 1.641 / 4.568
185: 1.127 / 4.466
190: 1.114 / 4.571
195: 1.456 / 3.748
200: 1.097 / 2.820
205: 0.891 / 3.514
210: 1.185 / 2.847
215: 0.925 / 2.500
220: 0.802 / 2.296
225: 0.754 / 1.868
230: 0.852 / 2.532
235: 0.720 / 1.397
240: 1.063 / 1.663
245: 0.743 / 1.561
250: 0.337 / 1.196
255: 0.741 / 1.249
260: 0.716 / 0.710
265: 0.383 / 0.769
270: 0.713 / 0.700
275: 0.325 / 0.548
280: 0.456 / 0.576
285: 0.740 / 0.453
290: 0.378 / 0.428
295: 0.451 / 0.429
300: 0.318 / 0.210
305: 0.446 / 0.239
310: 0.364 / 0.189
315: 0.305 / 0.180
320: 0.348 / 0.153
325: 0.408 / 0.132
330: 0.436 / 0.148
335: 0.251 / 0.155
340: 0.390 / 0.142
345: 0.316 / 0.142
350: 0.323 / 0.146
355: 0.218 / 0.154
360: 0.362 / 0.135
365: 0.128 / 0.127
370: 0.212 / 0.203
375: 0.219 / 0.220
380: 0.237 / 0.200
385: 0.295 / 0.218
390: 0.310 / 0.293
395: 0.182 / 0.217
400: 0.178 / 0.256
Rewirer cayley with seed 23
Using MSE Loss...
Minimum val loss at epoch 195: 0.2224063754081726
Final val loss at epoch 400: 2.2978538900613783
Train / Val loss by epoch
5: 31.857 / 28.053
10: 38.186 / 19.750
15: 17.716 / 16.967
20: 24.215 / 14.468
25: 10.825 / 13.690
30: 7.771 / 12.752
35: 5.890 / 11.824
40: 6.412 / 11.316
45: 8.368 / 9.310
50: 9.971 / 9.139
55: 6.674 / 7.352
60: 4.510 / 6.318
65: 3.991 / 5.119
70: 4.089 / 4.306
75: 3.934 / 4.334
80: 3.938 / 3.486
85: 4.466 / 3.608
90: 4.482 / 3.001
95: 2.466 / 2.524
100: 2.314 / 2.232
105: 2.679 / 2.265
110: 3.763 / 1.898
115: 2.163 / 1.574
120: 3.005 / 1.553
125: 3.741 / 1.459
130: 1.626 / 1.149
135: 3.568 / 1.134
140: 1.718 / 0.899
145: 1.132 / 0.736
150: 0.998 / 0.781
155: 1.783 / 0.660
160: 3.117 / 0.530
165: 1.627 / 0.410
170: 1.574 / 0.371
175: 0.914 / 0.329
180: 1.554 / 0.294
185: 1.045 / 0.275
190: 2.460 / 0.245
195: 0.760 / 0.222
200: 1.358 / 0.227
205: 0.753 / 0.288
210: 0.674 / 0.333
215: 0.851 / 0.462
220: 0.274 / 0.612
225: 2.131 / 0.521
230: 1.307 / 0.621
235: 1.345 / 0.646
240: 0.541 / 0.649
245: 1.044 / 0.606
250: 0.504 / 0.769
255: 0.731 / 0.753
260: 0.423 / 0.918
265: 1.447 / 0.809
270: 0.985 / 0.889
275: 0.488 / 1.070
280: 0.154 / 1.357
285: 0.399 / 1.379
290: 0.797 / 1.299
295: 0.627 / 1.375
300: 0.330 / 1.745
305: 0.869 / 1.444
310: 0.332 / 1.646
315: 0.286 / 1.494
320: 0.197 / 1.739
325: 0.388 / 1.829
330: 0.884 / 1.804
335: 0.240 / 1.950
340: 0.627 / 1.891
345: 0.318 / 1.922
350: 0.101 / 2.144
355: 0.227 / 2.310
360: 0.116 / 2.156
365: 0.222 / 1.892
370: 0.215 / 2.196
375: 0.277 / 2.046
380: 0.175 / 2.198
385: 0.240 / 1.971
390: 0.168 / 2.259
395: 0.114 / 2.149
400: 0.181 / 2.298
{'cayley': [{'best': 0.17411259189248085, 'end': 0.18185831494629384},
            {'best': 0.1272427521646023, 'end': 0.2555388420820236},
            {'best': 0.2224063754081726, 'end': 2.2978538900613783}],
 'cayley_clusters': [{'best': 0.13754212334752083, 'end': 0.15545164793729782},
                     {'best': 0.22325308620929718, 'end': 0.4992981493473053},
                     {'best': 0.6757160350680351, 'end': 0.8605461895465851}]}
Loading data...
Number of training graphs: 1500
Train graph sizes: Counter({101: 46, 114: 45, 79: 44, 87: 40, 94: 39, 122: 39, 88: 37, 92: 37, 107: 36, 119: 36, 80: 34, 105: 34, 75: 33, 99: 33, 112: 33, 116: 33, 121: 33, 78: 32, 84: 32, 98: 32, 100: 32, 120: 32, 96: 31, 111: 31, 118: 31, 81: 29, 83: 28, 95: 28, 103: 28, 108: 28, 82: 27, 89: 27, 106: 27, 93: 26, 90: 26, 97: 26, 109: 25, 115: 25, 117: 25, 85: 24, 124: 24, 76: 22, 86: 22, 91: 22, 102: 22, 104: 22, 110: 22, 123: 22, 77: 19, 113: 19})
{25: [], 26: [], 27: [], 28: [], 29: [], 30: [], 31: [], 32: [], 33: [], 34: []}
Number of validation graphs: 1500
Val graph sizes: Counter({159: 12, 168: 11, 172: 11, 130: 9, 164: 9, 174: 9, 126: 8, 142: 8, 141: 8, 149: 8, 125: 7, 129: 7, 133: 7, 134: 7, 139: 7, 154: 7, 153: 7, 160: 7, 128: 6, 135: 6, 137: 6, 136: 6, 143: 6, 146: 6, 147: 6, 152: 6, 150: 6, 158: 6, 157: 6, 165: 6, 138: 5, 144: 5, 148: 5, 145: 5, 156: 5, 163: 5, 162: 5, 166: 5, 132: 4, 151: 4, 161: 4, 169: 4, 167: 4, 171: 4, 173: 4, 131: 3, 140: 3, 127: 2, 170: 2, 155: 1})
{'wandb': {'experiment_name': '', 'project': 'ColourInteract Sweeps Friday', 'entity': 'commute_opt_gnn'}, 'data': {'name': 'ColourInteract', 'dataset': 'LRGB', 'c1': 1.0, 'c2s': [0.01, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 100.0], 'num_colours': 4, 'train_size': 1500, 'val_size': 300, 'min_train_nodes': 75, 'max_train_nodes': 125, 'min_val_nodes': 125, 'max_val_nodes': 175, 'rewirers': ['cayley_clusters', 'cayley'], 'normalise': True, 'seed': 93, 'c2': 0.2}, 'model': {'hidden_channels': 8, 'num_layers': 5, 'drop_prob': 0.1, 'global_pool_aggr': 'global_mean_pool', 'approaches': ['only_diff'], 'seeds': [17, 47, 23], 'approach': 'only_diff', 'seed': 23}, 'train': {'lr': 0.0001, 'num_epochs': 400, 'print_every': 5, 'train_batch_size': 32, 'val_batch_size': 32}, 'run': {'silent': False}}
Device: cpu
train targets: 10.30 +/- 1.214
val targets: 13.97 +/- 1.246
Rewirer cayley_clusters with seed 17
Using MSE Loss...
Minimum val loss at epoch 400: 0.2561991147696972
Final val loss at epoch 400: 0.2561991147696972
Train / Val loss by epoch
5: 52.321 / 154.394
10: 25.681 / 57.372
15: 23.106 / 38.414
20: 17.505 / 32.793
25: 14.360 / 27.405
30: 9.410 / 24.294
35: 9.177 / 18.560
40: 5.534 / 19.427
45: 7.019 / 14.349
50: 6.695 / 15.596
55: 4.506 / 12.161
60: 9.366 / 12.021
65: 7.766 / 11.964
70: 2.813 / 13.730
75: 4.664 / 11.669
80: 3.766 / 15.013
85: 3.039 / 15.519
90: 4.034 / 15.481
95: 3.646 / 17.020
100: 4.830 / 13.953
105: 3.603 / 14.639
110: 4.289 / 15.502
115: 2.625 / 14.123
120: 1.941 / 13.444
125: 3.676 / 15.097
130: 3.853 / 12.369
135: 3.456 / 10.889
140: 3.377 / 13.852
145: 3.165 / 10.629
150: 1.514 / 12.656
155: 1.631 / 10.410
160: 2.933 / 11.203
165: 1.471 / 10.297
170: 2.410 / 11.063
175: 3.358 / 8.857
180: 0.610 / 8.889
185: 1.867 / 9.228
190: 1.155 / 7.108
195: 1.485 / 6.886
200: 2.220 / 6.523
205: 1.560 / 6.001
210: 1.405 / 6.007
215: 0.991 / 5.630
220: 0.820 / 4.693
225: 0.984 / 4.059
230: 2.254 / 3.902
235: 1.825 / 4.207
240: 0.770 / 3.342
245: 0.576 / 2.854
250: 0.681 / 2.536
255: 0.845 / 2.436
260: 1.056 / 2.688
265: 0.784 / 2.144
270: 1.275 / 2.134
275: 0.823 / 1.729
280: 1.282 / 1.842
285: 2.055 / 1.992
290: 0.705 / 1.419
295: 1.160 / 1.543
300: 0.401 / 1.111
305: 0.768 / 1.089
310: 0.557 / 0.970
315: 0.700 / 1.127
320: 0.411 / 0.697
325: 0.419 / 0.608
330: 0.346 / 0.523
335: 0.604 / 0.697
340: 0.449 / 0.618
345: 0.364 / 0.501
350: 0.275 / 0.528
355: 0.597 / 0.597
360: 0.478 / 0.458
365: 0.540 / 0.436
370: 0.238 / 0.419
375: 0.329 / 0.393
380: 0.345 / 0.373
385: 0.301 / 0.389
390: 0.346 / 0.260
395: 0.334 / 0.320
400: 0.250 / 0.256
Rewirer cayley_clusters with seed 47
Using MSE Loss...
Minimum val loss at epoch 340: 0.2527165420353413
Final val loss at epoch 400: 0.3598563775420189
Train / Val loss by epoch
5: 13.987 / 7.571
10: 20.390 / 5.238
15: 12.889 / 4.002
20: 8.412 / 3.401
25: 13.043 / 3.462
30: 6.081 / 2.426
35: 6.307 / 2.894
40: 9.425 / 2.335
45: 9.275 / 2.280
50: 8.089 / 2.670
55: 2.934 / 3.444
60: 6.272 / 4.362
65: 4.283 / 4.384
70: 5.052 / 4.175
75: 1.669 / 4.630
80: 5.450 / 5.064
85: 3.431 / 5.379
90: 3.563 / 6.177
95: 1.683 / 6.916
100: 2.137 / 6.431
105: 3.825 / 5.717
110: 2.070 / 5.844
115: 1.470 / 5.766
120: 1.934 / 6.467
125: 1.933 / 6.307
130: 1.339 / 5.524
135: 1.474 / 5.287
140: 1.270 / 4.672
145: 1.539 / 4.955
150: 0.964 / 4.879
155: 0.984 / 4.428
160: 1.115 / 4.143
165: 1.377 / 4.640
170: 1.153 / 3.873
175: 1.165 / 3.494
180: 1.992 / 2.477
185: 1.310 / 3.516
190: 0.793 / 2.850
195: 1.163 / 2.129
200: 1.378 / 2.280
205: 1.382 / 1.887
210: 1.112 / 1.858
215: 0.621 / 1.504
220: 0.864 / 1.375
225: 0.992 / 1.192
230: 0.844 / 1.544
235: 0.524 / 1.128
240: 1.296 / 1.219
245: 0.860 / 0.902
250: 0.348 / 0.809
255: 0.589 / 0.879
260: 0.927 / 0.754
265: 0.659 / 0.576
270: 0.766 / 0.608
275: 0.420 / 0.630
280: 0.660 / 0.469
285: 0.382 / 0.404
290: 0.726 / 0.437
295: 0.469 / 0.330
300: 0.399 / 0.269
305: 0.391 / 0.320
310: 0.384 / 0.276
315: 0.282 / 0.300
320: 0.309 / 0.260
325: 0.312 / 0.254
330: 0.412 / 0.265
335: 0.331 / 0.262
340: 0.325 / 0.253
345: 0.337 / 0.257
350: 0.198 / 0.270
355: 0.189 / 0.268
360: 0.283 / 0.290
365: 0.411 / 0.284
370: 0.330 / 0.323
375: 0.194 / 0.303
380: 0.255 / 0.317
385: 0.209 / 0.306
390: 0.180 / 0.377
395: 0.180 / 0.362
400: 0.236 / 0.360
Rewirer cayley_clusters with seed 23
Using MSE Loss...
Minimum val loss at epoch 315: 0.33946832865476606
Final val loss at epoch 400: 0.4641239017248154
Train / Val loss by epoch
5: 25.803 / 13.637
10: 22.959 / 8.526
15: 14.972 / 9.374
20: 16.643 / 10.522
25: 10.694 / 11.028
30: 5.559 / 12.236
35: 7.619 / 12.326
40: 7.441 / 14.315
45: 8.075 / 15.484
50: 10.457 / 14.275
55: 2.890 / 18.086
60: 5.724 / 18.394
65: 2.980 / 17.089
70: 4.867 / 19.187
75: 3.557 / 15.987
80: 4.287 / 12.654
85: 3.903 / 10.768
90: 2.908 / 9.632
95: 3.134 / 7.902
100: 2.733 / 7.901
105: 2.877 / 4.814
110: 3.679 / 4.561
115: 1.267 / 5.158
120: 4.279 / 4.085
125: 1.207 / 2.518
130: 1.081 / 2.563
135: 2.164 / 1.914
140: 1.203 / 2.137
145: 2.122 / 1.988
150: 1.442 / 1.314
155: 1.721 / 1.360
160: 3.743 / 1.021
165: 1.049 / 1.334
170: 1.314 / 0.904
175: 0.881 / 0.983
180: 1.064 / 0.775
185: 0.945 / 0.922
190: 2.853 / 0.594
195: 1.177 / 0.702
200: 0.456 / 0.882
205: 0.387 / 0.830
210: 0.579 / 0.762
215: 1.653 / 0.620
220: 0.587 / 0.626
225: 1.434 / 0.454
230: 1.573 / 0.502
235: 1.246 / 0.455
240: 0.802 / 0.389
245: 0.798 / 0.476
250: 1.480 / 0.380
255: 0.747 / 0.453
260: 0.358 / 0.524
265: 0.988 / 0.399
270: 0.774 / 0.412
275: 0.264 / 0.458
280: 0.436 / 0.645
285: 0.924 / 0.559
290: 0.861 / 0.416
295: 0.380 / 0.523
300: 0.389 / 0.537
305: 0.528 / 0.434
310: 0.375 / 0.443
315: 0.510 / 0.339
320: 0.212 / 0.348
325: 0.636 / 0.365
330: 0.622 / 0.364
335: 0.371 / 0.438
340: 0.600 / 0.376
345: 0.323 / 0.394
350: 0.556 / 0.469
355: 0.554 / 0.410
360: 0.342 / 0.475
365: 0.722 / 0.396
370: 0.557 / 0.442
375: 0.489 / 0.434
380: 0.677 / 0.515
385: 0.202 / 0.444
390: 0.315 / 0.384
395: 0.220 / 0.409
400: 0.193 / 0.464
Rewirer cayley with seed 17
Using MSE Loss...
Minimum val loss at epoch 400: 0.3533505991101265
Final val loss at epoch 400: 0.3533505991101265
Train / Val loss by epoch
5: 24.786 / 81.315
10: 38.134 / 25.925
15: 23.260 / 21.384
20: 14.825 / 18.421
25: 12.861 / 13.775
30: 9.007 / 13.059
35: 5.057 / 12.534
40: 6.865 / 13.659
45: 5.219 / 9.150
50: 5.176 / 11.390
55: 5.222 / 10.654
60: 3.497 / 11.713
65: 2.642 / 12.037
70: 5.599 / 13.260
75: 5.348 / 11.267
80: 3.735 / 13.525
85: 4.133 / 13.064
90: 2.148 / 12.810
95: 2.711 / 14.761
100: 2.949 / 14.561
105: 3.146 / 12.398
110: 3.568 / 14.597
115: 2.581 / 12.311
120: 2.673 / 12.362
125: 3.809 / 13.278
130: 2.701 / 11.934
135: 2.248 / 11.219
140: 2.117 / 10.790
145: 3.448 / 10.620
150: 1.503 / 8.064
155: 2.011 / 7.654
160: 1.581 / 7.818
165: 1.645 / 7.100
170: 3.931 / 8.043
175: 2.268 / 5.768
180: 1.420 / 6.076
185: 1.047 / 5.917
190: 1.355 / 4.580
195: 1.540 / 4.866
200: 1.712 / 4.990
205: 1.742 / 3.896
210: 1.445 / 3.959
215: 1.171 / 3.468
220: 1.092 / 2.808
225: 1.472 / 3.018
230: 1.908 / 2.553
235: 1.145 / 2.331
240: 0.801 / 1.764
245: 0.484 / 1.694
250: 0.883 / 1.516
255: 0.423 / 1.294
260: 1.594 / 1.437
265: 0.912 / 0.944
270: 1.781 / 1.386
275: 0.832 / 0.938
280: 0.880 / 0.818
285: 1.027 / 0.766
290: 0.586 / 0.835
295: 0.920 / 0.921
300: 0.639 / 0.763
305: 1.514 / 0.815
310: 0.423 / 0.770
315: 1.058 / 0.780
320: 0.380 / 0.624
325: 0.222 / 0.582
330: 0.369 / 0.556
335: 0.414 / 0.614
340: 0.707 / 0.630
345: 0.456 / 0.587
350: 0.344 / 0.475
355: 0.577 / 0.645
360: 0.362 / 0.512
365: 0.259 / 0.514
370: 0.266 / 0.409
375: 0.431 / 0.534
380: 0.376 / 0.445
385: 0.564 / 0.480
390: 0.288 / 0.388
395: 0.359 / 0.404
400: 0.241 / 0.353
Rewirer cayley with seed 47
Using MSE Loss...
Minimum val loss at epoch 380: 0.1940372370183468
Final val loss at epoch 400: 0.2155241012573242
Train / Val loss by epoch
5: 23.582 / 131.728
10: 14.898 / 116.307
15: 18.947 / 107.835
20: 9.602 / 98.191
25: 14.186 / 90.805
30: 5.021 / 77.075
35: 5.932 / 71.213
40: 11.134 / 62.279
45: 7.462 / 62.392
50: 8.041 / 57.573
55: 4.593 / 48.681
60: 5.659 / 49.095
65: 4.737 / 47.107
70: 5.533 / 37.510
75: 3.160 / 35.852
80: 3.444 / 31.256
85: 3.521 / 29.498
90: 4.492 / 29.217
95: 2.984 / 26.978
100: 4.324 / 25.346
105: 3.915 / 21.694
110: 2.881 / 21.414
115: 2.542 / 18.678
120: 1.676 / 16.355
125: 1.899 / 16.978
130: 2.004 / 13.488
135: 2.067 / 12.673
140: 1.716 / 9.599
145: 2.022 / 10.034
150: 2.001 / 8.917
155: 1.173 / 8.534
160: 1.674 / 7.320
165: 1.549 / 8.082
170: 1.209 / 5.537
175: 1.044 / 5.492
180: 1.679 / 4.722
185: 0.998 / 4.454
190: 1.064 / 4.553
195: 1.366 / 3.886
200: 1.274 / 3.201
205: 1.053 / 3.544
210: 1.172 / 3.197
215: 1.068 / 2.426
220: 0.870 / 2.257
225: 0.666 / 1.864
230: 0.887 / 2.648
235: 0.714 / 1.576
240: 1.142 / 1.580
245: 0.801 / 1.466
250: 0.402 / 1.291
255: 0.922 / 1.232
260: 0.772 / 0.790
265: 0.413 / 0.865
270: 0.705 / 0.906
275: 0.337 / 0.623
280: 0.413 / 0.712
285: 0.769 / 0.590
290: 0.442 / 0.658
295: 0.450 / 0.543
300: 0.310 / 0.322
305: 0.459 / 0.459
310: 0.518 / 0.364
315: 0.345 / 0.406
320: 0.348 / 0.301
325: 0.384 / 0.282
330: 0.443 / 0.240
335: 0.370 / 0.224
340: 0.500 / 0.219
345: 0.386 / 0.231
350: 0.299 / 0.213
355: 0.183 / 0.215
360: 0.393 / 0.218
365: 0.181 / 0.206
370: 0.277 / 0.201
375: 0.289 / 0.204
380: 0.392 / 0.194
385: 0.315 / 0.196
390: 0.517 / 0.247
395: 0.221 / 0.218
400: 0.336 / 0.216
Rewirer cayley with seed 23
Using MSE Loss...
Minimum val loss at epoch 190: 0.29367875009775163
Final val loss at epoch 400: 2.2131927460432053
Train / Val loss by epoch
5: 31.062 / 30.392
10: 37.591 / 21.827
15: 17.308 / 18.652
20: 23.932 / 15.628
25: 10.580 / 14.394
30: 7.597 / 12.393
35: 5.788 / 10.945
40: 6.460 / 10.256
45: 7.800 / 8.514
50: 9.721 / 8.020
55: 6.482 / 6.285
60: 4.633 / 5.358
65: 4.028 / 4.362
70: 4.041 / 3.559
75: 3.748 / 3.522
80: 3.899 / 2.831
85: 4.623 / 3.097
90: 4.139 / 2.477
95: 2.357 / 2.091
100: 2.158 / 1.828
105: 2.540 / 1.917
110: 3.746 / 1.614
115: 2.100 / 1.275
120: 3.240 / 1.370
125: 3.411 / 1.286
130: 1.753 / 0.985
135: 3.369 / 0.898
140: 1.724 / 0.685
145: 1.254 / 0.549
150: 1.055 / 0.566
155: 1.887 / 0.492
160: 2.906 / 0.417
165: 1.788 / 0.388
170: 1.634 / 0.329
175: 0.926 / 0.380
180: 1.632 / 0.309
185: 1.042 / 0.300
190: 2.539 / 0.294
195: 0.875 / 0.376
200: 1.130 / 0.402
205: 1.019 / 0.410
210: 0.763 / 0.515
215: 1.037 / 0.634
220: 0.322 / 0.813
225: 1.793 / 0.595
230: 1.787 / 0.692
235: 1.455 / 0.591
240: 0.744 / 0.721
245: 0.806 / 0.725
250: 0.706 / 0.848
255: 0.819 / 0.930
260: 0.687 / 1.142
265: 1.486 / 1.069
270: 0.699 / 1.159
275: 0.521 / 1.379
280: 0.235 / 1.622
285: 0.375 / 1.477
290: 0.864 / 1.286
295: 0.621 / 1.403
300: 0.377 / 1.783
305: 0.955 / 1.592
310: 0.379 / 1.709
315: 0.627 / 1.548
320: 0.294 / 1.662
325: 0.415 / 1.682
330: 1.380 / 1.717
335: 0.527 / 1.800
340: 0.985 / 1.663
345: 0.599 / 1.758
350: 0.352 / 1.948
355: 0.387 / 2.115
360: 0.233 / 2.134
365: 0.476 / 1.789
370: 0.487 / 2.146
375: 0.495 / 2.075
380: 0.464 / 2.077
385: 0.375 / 1.780
390: 0.322 / 2.052
395: 0.360 / 2.037
400: 0.184 / 2.213
{'cayley': [{'best': 0.3533505991101265, 'end': 0.3533505991101265},
            {'best': 0.1940372370183468, 'end': 0.2155241012573242},
            {'best': 0.29367875009775163, 'end': 2.2131927460432053}],
 'cayley_clusters': [{'best': 0.2561991147696972, 'end': 0.2561991147696972},
                     {'best': 0.2527165420353413, 'end': 0.3598563775420189},
                     {'best': 0.33946832865476606, 'end': 0.4641239017248154}]}
Loading data...
Number of training graphs: 1500
Train graph sizes: Counter({101: 46, 114: 45, 79: 44, 87: 40, 94: 39, 122: 39, 88: 37, 92: 37, 107: 36, 119: 36, 80: 34, 105: 34, 75: 33, 99: 33, 112: 33, 116: 33, 121: 33, 78: 32, 84: 32, 98: 32, 100: 32, 120: 32, 96: 31, 111: 31, 118: 31, 81: 29, 83: 28, 95: 28, 103: 28, 108: 28, 82: 27, 89: 27, 106: 27, 93: 26, 90: 26, 97: 26, 109: 25, 115: 25, 117: 25, 85: 24, 124: 24, 76: 22, 86: 22, 91: 22, 102: 22, 104: 22, 110: 22, 123: 22, 77: 19, 113: 19})
{25: [], 26: [], 27: [], 28: [], 29: [], 30: [], 31: [], 32: [], 33: [], 34: []}
Number of validation graphs: 1500
Val graph sizes: Counter({159: 12, 168: 11, 172: 11, 130: 9, 164: 9, 174: 9, 126: 8, 142: 8, 141: 8, 149: 8, 125: 7, 129: 7, 133: 7, 134: 7, 139: 7, 154: 7, 153: 7, 160: 7, 128: 6, 135: 6, 137: 6, 136: 6, 143: 6, 146: 6, 147: 6, 152: 6, 150: 6, 158: 6, 157: 6, 165: 6, 138: 5, 144: 5, 148: 5, 145: 5, 156: 5, 163: 5, 162: 5, 166: 5, 132: 4, 151: 4, 161: 4, 169: 4, 167: 4, 171: 4, 173: 4, 131: 3, 140: 3, 127: 2, 170: 2, 155: 1})
{'wandb': {'experiment_name': '', 'project': 'ColourInteract Sweeps Friday', 'entity': 'commute_opt_gnn'}, 'data': {'name': 'ColourInteract', 'dataset': 'LRGB', 'c1': 1.0, 'c2s': [0.01, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 100.0], 'num_colours': 4, 'train_size': 1500, 'val_size': 300, 'min_train_nodes': 75, 'max_train_nodes': 125, 'min_val_nodes': 125, 'max_val_nodes': 175, 'rewirers': ['cayley_clusters', 'cayley'], 'normalise': True, 'seed': 93, 'c2': 0.5}, 'model': {'hidden_channels': 8, 'num_layers': 5, 'drop_prob': 0.1, 'global_pool_aggr': 'global_mean_pool', 'approaches': ['only_diff'], 'seeds': [17, 47, 23], 'approach': 'only_diff', 'seed': 23}, 'train': {'lr': 0.0001, 'num_epochs': 400, 'print_every': 5, 'train_batch_size': 32, 'val_batch_size': 32}, 'run': {'silent': False}}
Device: cpu
train targets: 21.22 +/- 2.926
val targets: 30.41 +/- 2.997
Rewirer cayley_clusters with seed 17
Using MSE Loss...
Minimum val loss at epoch 400: 3.030128335952759
Final val loss at epoch 400: 3.030128335952759
Train / Val loss by epoch
5: 98.380 / 338.160
10: 29.708 / 82.328
15: 26.404 / 56.637
20: 19.485 / 49.198
25: 17.671 / 41.547
30: 12.329 / 36.980
35: 11.205 / 29.502
40: 5.778 / 30.541
45: 8.041 / 23.814
50: 7.525 / 26.112
55: 4.557 / 20.033
60: 11.911 / 20.118
65: 9.019 / 19.548
70: 3.281 / 22.258
75: 5.873 / 19.157
80: 6.454 / 22.013
85: 3.767 / 22.551
90: 5.300 / 22.900
95: 5.018 / 24.843
100: 5.965 / 20.252
105: 5.482 / 21.186
110: 5.201 / 22.643
115: 3.354 / 20.171
120: 3.003 / 19.741
125: 5.202 / 21.705
130: 5.569 / 18.265
135: 4.895 / 16.246
140: 4.228 / 19.518
145: 4.438 / 16.225
150: 2.178 / 18.145
155: 2.312 / 15.877
160: 4.501 / 17.258
165: 1.803 / 15.331
170: 3.486 / 16.766
175: 3.966 / 13.996
180: 1.557 / 13.675
185: 3.059 / 13.857
190: 2.307 / 11.589
195: 2.011 / 11.063
200: 2.103 / 11.373
205: 2.000 / 10.693
210: 2.051 / 10.939
215: 1.443 / 10.276
220: 1.251 / 8.821
225: 1.879 / 8.863
230: 3.110 / 8.293
235: 1.950 / 9.750
240: 1.296 / 7.510
245: 1.119 / 7.879
250: 1.158 / 7.205
255: 1.619 / 6.840
260: 1.636 / 7.103
265: 1.275 / 6.498
270: 2.244 / 6.959
275: 1.833 / 6.362
280: 1.560 / 6.117
285: 2.637 / 6.813
290: 1.437 / 6.332
295: 1.891 / 6.391
300: 1.313 / 5.680
305: 1.531 / 5.294
310: 1.813 / 4.865
315: 1.119 / 6.123
320: 0.976 / 4.780
325: 1.110 / 4.420
330: 0.829 / 4.325
335: 1.384 / 4.610
340: 1.121 / 4.118
345: 1.114 / 4.100
350: 0.903 / 4.105
355: 1.037 / 4.483
360: 0.904 / 4.142
365: 0.756 / 3.925
370: 0.647 / 3.533
375: 1.091 / 4.123
380: 0.878 / 3.945
385: 0.713 / 4.160
390: 0.923 / 3.341
395: 0.817 / 3.495
400: 0.654 / 3.030
Rewirer cayley_clusters with seed 47
Using MSE Loss...
Minimum val loss at epoch 400: 0.7333817154169082
Final val loss at epoch 400: 0.7333817154169082
Train / Val loss by epoch
5: 14.849 / 12.369
10: 18.769 / 8.979
15: 12.500 / 6.628
20: 7.862 / 6.900
25: 10.639 / 8.684
30: 5.430 / 6.598
35: 5.056 / 10.400
40: 9.736 / 9.104
45: 8.095 / 9.732
50: 6.327 / 12.571
55: 3.584 / 12.850
60: 6.354 / 15.460
65: 4.053 / 15.555
70: 4.517 / 15.893
75: 1.139 / 15.380
80: 4.767 / 16.220
85: 4.416 / 16.290
90: 4.281 / 16.788
95: 2.372 / 18.470
100: 1.803 / 16.781
105: 4.431 / 16.034
110: 2.304 / 15.660
115: 1.161 / 15.997
120: 2.272 / 14.729
125: 2.273 / 14.641
130: 1.477 / 12.657
135: 2.023 / 12.260
140: 1.630 / 10.945
145: 1.590 / 10.312
150: 1.361 / 9.583
155: 1.170 / 9.909
160: 0.854 / 8.818
165: 2.393 / 9.223
170: 1.329 / 7.674
175: 1.589 / 7.491
180: 1.725 / 6.251
185: 1.527 / 7.394
190: 1.076 / 6.548
195: 1.381 / 4.909
200: 1.122 / 4.807
205: 1.791 / 4.651
210: 1.135 / 4.500
215: 1.058 / 3.823
220: 1.222 / 4.063
225: 1.453 / 3.334
230: 1.455 / 4.057
235: 1.543 / 3.536
240: 1.672 / 3.068
245: 1.154 / 2.397
250: 0.926 / 2.284
255: 1.046 / 2.850
260: 1.265 / 2.584
265: 0.935 / 2.346
270: 1.327 / 2.030
275: 0.780 / 2.045
280: 0.846 / 1.787
285: 1.198 / 1.738
290: 0.989 / 1.874
295: 1.040 / 1.586
300: 0.548 / 1.286
305: 1.083 / 1.425
310: 0.731 / 1.241
315: 0.493 / 1.320
320: 0.671 / 1.122
325: 0.822 / 1.130
330: 0.608 / 0.924
335: 0.979 / 0.918
340: 0.628 / 0.952
345: 0.576 / 0.998
350: 0.750 / 1.023
355: 0.634 / 0.885
360: 0.845 / 0.971
365: 0.723 / 0.891
370: 0.709 / 0.736
375: 0.466 / 0.825
380: 0.490 / 0.734
385: 0.547 / 0.998
390: 0.335 / 0.789
395: 0.426 / 0.746
400: 0.466 / 0.733
Rewirer cayley_clusters with seed 23
Using MSE Loss...
Minimum val loss at epoch 400: 0.6101273044943809
Final val loss at epoch 400: 0.6101273044943809
Train / Val loss by epoch
5: 22.324 / 13.393
10: 22.050 / 6.543
15: 14.607 / 4.736
20: 15.511 / 4.686
25: 9.620 / 4.741
30: 5.208 / 5.861
35: 6.910 / 5.579
40: 7.404 / 5.871
45: 8.126 / 5.728
50: 11.275 / 5.329
55: 2.970 / 7.173
60: 5.794 / 7.148
65: 3.083 / 5.930
70: 3.895 / 6.898
75: 2.601 / 5.617
80: 3.957 / 3.586
85: 3.892 / 3.103
90: 3.983 / 2.663
95: 2.481 / 2.321
100: 2.697 / 2.416
105: 2.152 / 1.381
110: 3.824 / 1.289
115: 1.961 / 1.561
120: 5.103 / 1.411
125: 1.684 / 0.923
130: 0.855 / 0.973
135: 3.041 / 0.886
140: 1.541 / 0.869
145: 2.133 / 0.845
150: 1.687 / 0.833
155: 2.017 / 0.806
160: 3.760 / 0.869
165: 1.674 / 0.760
170: 1.638 / 0.817
175: 1.403 / 0.780
180: 1.523 / 0.890
185: 1.274 / 0.893
190: 4.289 / 1.094
195: 1.807 / 0.895
200: 1.169 / 0.888
205: 0.872 / 0.864
210: 1.002 / 0.799
215: 1.913 / 0.798
220: 1.150 / 0.791
225: 2.072 / 1.084
230: 2.848 / 0.879
235: 1.798 / 0.967
240: 1.497 / 1.068
245: 1.404 / 0.777
250: 2.571 / 0.966
255: 1.095 / 0.949
260: 1.151 / 0.784
265: 1.728 / 1.051
270: 1.497 / 0.920
275: 0.841 / 0.815
280: 0.955 / 0.648
285: 1.807 / 0.702
290: 1.288 / 0.930
295: 0.735 / 0.826
300: 0.695 / 0.682
305: 0.936 / 0.791
310: 0.599 / 0.788
315: 1.325 / 0.826
320: 0.484 / 0.822
325: 1.107 / 0.959
330: 1.498 / 0.858
335: 0.548 / 0.700
340: 1.578 / 0.791
345: 0.690 / 0.680
350: 1.149 / 0.709
355: 0.895 / 0.819
360: 1.239 / 0.742
365: 1.336 / 0.701
370: 1.187 / 0.643
375: 0.926 / 0.626
380: 0.896 / 0.614
385: 0.711 / 0.665
390: 0.704 / 0.768
395: 0.809 / 0.715
400: 0.661 / 0.610
Rewirer cayley with seed 17
Using MSE Loss...
Minimum val loss at epoch 400: 2.690872776508331
Final val loss at epoch 400: 2.690872776508331
Train / Val loss by epoch
5: 57.854 / 215.762
10: 41.854 / 41.595
15: 24.318 / 33.582
20: 16.032 / 28.293
25: 14.648 / 20.804
30: 10.643 / 19.247
35: 6.090 / 18.692
40: 7.118 / 19.543
45: 5.876 / 14.197
50: 7.000 / 16.556
55: 6.533 / 15.358
60: 3.250 / 16.193
65: 3.075 / 16.430
70: 6.510 / 17.465
75: 6.623 / 15.449
80: 5.273 / 17.805
85: 5.532 / 17.520
90: 3.050 / 18.012
95: 4.036 / 20.820
100: 3.864 / 20.833
105: 4.298 / 18.161
110: 4.269 / 21.759
115: 4.101 / 19.120
120: 3.862 / 18.427
125: 4.548 / 20.786
130: 3.591 / 18.527
135: 3.061 / 17.362
140: 3.433 / 17.202
145: 4.823 / 17.034
150: 2.109 / 13.500
155: 3.004 / 12.791
160: 2.470 / 13.930
165: 2.371 / 12.475
170: 4.616 / 14.591
175: 3.138 / 10.826
180: 2.409 / 10.938
185: 1.663 / 9.903
190: 2.133 / 8.362
195: 2.162 / 9.082
200: 2.556 / 8.721
205: 2.843 / 7.446
210: 1.903 / 7.423
215: 1.803 / 7.166
220: 1.780 / 6.201
225: 2.452 / 6.552
230: 2.511 / 5.864
235: 2.034 / 5.654
240: 1.305 / 4.903
245: 0.841 / 4.701
250: 1.648 / 4.655
255: 0.798 / 4.276
260: 1.685 / 4.725
265: 1.452 / 3.934
270: 2.580 / 5.170
275: 1.528 / 4.151
280: 1.313 / 3.518
285: 1.705 / 3.329
290: 1.256 / 3.796
295: 1.789 / 3.953
300: 1.498 / 3.812
305: 2.842 / 3.812
310: 1.009 / 3.616
315: 2.509 / 3.979
320: 0.814 / 3.699
325: 0.855 / 3.442
330: 0.861 / 3.291
335: 1.121 / 3.498
340: 1.699 / 3.320
345: 1.786 / 3.532
350: 0.766 / 2.859
355: 1.431 / 3.798
360: 1.319 / 3.353
365: 0.801 / 3.166
370: 0.598 / 2.889
375: 1.367 / 3.624
380: 0.900 / 3.285
385: 1.248 / 3.410
390: 1.181 / 3.264
395: 0.696 / 2.981
400: 0.849 / 2.691
Rewirer cayley with seed 47
Using MSE Loss...
Minimum val loss at epoch 375: 1.2044750571250915
Final val loss at epoch 400: 1.3806698203086853
Train / Val loss by epoch
5: 23.886 / 143.679
10: 14.703 / 119.167
15: 18.929 / 108.304
20: 8.640 / 98.289
25: 12.610 / 85.177
30: 4.798 / 74.490
35: 5.465 / 69.470
40: 10.902 / 60.740
45: 7.283 / 62.062
50: 7.184 / 59.967
55: 4.235 / 49.985
60: 4.614 / 52.149
65: 4.844 / 51.595
70: 5.017 / 45.850
75: 2.810 / 41.273
80: 3.278 / 39.122
85: 3.929 / 36.755
90: 4.873 / 34.795
95: 3.302 / 32.929
100: 5.050 / 32.005
105: 3.819 / 29.273
110: 2.995 / 28.317
115: 2.161 / 23.780
120: 2.169 / 22.406
125: 2.470 / 22.448
130: 2.310 / 18.283
135: 1.976 / 17.525
140: 2.374 / 14.368
145: 2.083 / 14.250
150: 2.275 / 12.749
155: 1.259 / 13.321
160: 1.301 / 10.780
165: 1.670 / 10.633
170: 1.376 / 8.342
175: 1.664 / 8.196
180: 1.514 / 6.718
185: 1.309 / 6.540
190: 1.107 / 6.894
195: 1.651 / 5.732
200: 1.678 / 5.603
205: 2.078 / 5.781
210: 1.254 / 5.110
215: 1.762 / 4.524
220: 0.889 / 3.850
225: 1.149 / 3.664
230: 1.711 / 4.420
235: 1.577 / 3.509
240: 1.722 / 3.056
245: 1.074 / 3.027
250: 0.631 / 3.097
255: 1.197 / 2.915
260: 1.126 / 2.228
265: 1.021 / 2.713
270: 0.836 / 2.779
275: 1.004 / 2.246
280: 0.713 / 2.331
285: 1.233 / 1.979
290: 0.790 / 2.593
295: 0.581 / 2.051
300: 0.683 / 1.663
305: 0.709 / 2.011
310: 1.128 / 1.818
315: 0.631 / 2.021
320: 0.670 / 1.640
325: 0.677 / 1.640
330: 1.012 / 1.389
335: 1.052 / 1.330
340: 1.119 / 1.276
345: 0.872 / 1.315
350: 0.743 / 1.453
355: 0.504 / 1.475
360: 1.045 / 1.497
365: 0.486 / 1.536
370: 0.703 / 1.362
375: 0.853 / 1.204
380: 0.700 / 1.235
385: 0.469 / 1.488
390: 0.584 / 1.277
395: 0.614 / 1.250
400: 0.619 / 1.381
Rewirer cayley with seed 23
Using MSE Loss...
Minimum val loss at epoch 125: 0.9301871359348297
Final val loss at epoch 400: 1.6322282642126082
Train / Val loss by epoch
5: 29.267 / 41.090
10: 35.450 / 31.200
15: 16.148 / 23.148
20: 21.763 / 17.065
25: 9.804 / 15.396
30: 8.175 / 10.683
35: 6.006 / 7.440
40: 6.770 / 6.818
45: 6.576 / 6.806
50: 9.775 / 5.153
55: 6.655 / 3.289
60: 5.075 / 2.697
65: 4.414 / 2.325
70: 4.038 / 1.835
75: 3.633 / 1.594
80: 3.896 / 1.382
85: 4.606 / 1.362
90: 3.796 / 1.166
95: 2.960 / 1.108
100: 1.953 / 1.067
105: 2.431 / 0.955
110: 3.374 / 1.091
115: 2.291 / 1.120
120: 4.106 / 0.977
125: 3.185 / 0.930
130: 1.768 / 1.054
135: 2.974 / 1.113
140: 1.958 / 1.314
145: 1.677 / 1.623
150: 1.683 / 1.205
155: 2.320 / 1.399
160: 3.474 / 1.411
165: 1.852 / 1.812
170: 1.533 / 1.751
175: 1.358 / 2.015
180: 1.451 / 1.522
185: 1.330 / 1.366
190: 3.496 / 1.323
195: 1.126 / 1.657
200: 1.003 / 1.710
205: 1.735 / 1.677
210: 0.775 / 2.004
215: 1.709 / 2.158
220: 1.036 / 2.214
225: 1.677 / 1.828
230: 3.194 / 1.730
235: 1.818 / 1.773
240: 1.251 / 1.886
245: 0.913 / 1.795
250: 0.840 / 1.941
255: 0.592 / 2.051
260: 1.538 / 2.135
265: 1.570 / 2.135
270: 0.957 / 1.877
275: 1.227 / 1.754
280: 0.647 / 2.261
285: 0.945 / 2.017
290: 1.421 / 1.779
295: 1.075 / 1.689
300: 0.703 / 2.173
305: 0.889 / 1.861
310: 0.623 / 1.992
315: 1.261 / 1.809
320: 0.580 / 1.873
325: 0.779 / 1.722
330: 1.899 / 1.753
335: 1.282 / 1.744
340: 1.283 / 1.816
345: 0.812 / 1.864
350: 0.889 / 1.936
355: 0.681 / 1.877
360: 0.790 / 1.692
365: 0.837 / 1.531
370: 1.639 / 1.641
375: 0.782 / 1.826
380: 1.231 / 1.532
385: 0.845 / 1.571
390: 0.490 / 1.716
395: 0.667 / 1.668
400: 0.523 / 1.632
{'cayley': [{'best': 2.690872776508331, 'end': 2.690872776508331},
            {'best': 1.2044750571250915, 'end': 1.3806698203086853},
            {'best': 0.9301871359348297, 'end': 1.6322282642126082}],
 'cayley_clusters': [{'best': 3.030128335952759, 'end': 3.030128335952759},
                     {'best': 0.7333817154169082, 'end': 0.7333817154169082},
                     {'best': 0.6101273044943809, 'end': 0.6101273044943809}]}
Loading data...
Number of training graphs: 1500
Train graph sizes: Counter({101: 46, 114: 45, 79: 44, 87: 40, 94: 39, 122: 39, 88: 37, 92: 37, 107: 36, 119: 36, 80: 34, 105: 34, 75: 33, 99: 33, 112: 33, 116: 33, 121: 33, 78: 32, 84: 32, 98: 32, 100: 32, 120: 32, 96: 31, 111: 31, 118: 31, 81: 29, 83: 28, 95: 28, 103: 28, 108: 28, 82: 27, 89: 27, 106: 27, 93: 26, 90: 26, 97: 26, 109: 25, 115: 25, 117: 25, 85: 24, 124: 24, 76: 22, 86: 22, 91: 22, 102: 22, 104: 22, 110: 22, 123: 22, 77: 19, 113: 19})
{25: [], 26: [], 27: [], 28: [], 29: [], 30: [], 31: [], 32: [], 33: [], 34: []}
Number of validation graphs: 1500
Val graph sizes: Counter({159: 12, 168: 11, 172: 11, 130: 9, 164: 9, 174: 9, 126: 8, 142: 8, 141: 8, 149: 8, 125: 7, 129: 7, 133: 7, 134: 7, 139: 7, 154: 7, 153: 7, 160: 7, 128: 6, 135: 6, 137: 6, 136: 6, 143: 6, 146: 6, 147: 6, 152: 6, 150: 6, 158: 6, 157: 6, 165: 6, 138: 5, 144: 5, 148: 5, 145: 5, 156: 5, 163: 5, 162: 5, 166: 5, 132: 4, 151: 4, 161: 4, 169: 4, 167: 4, 171: 4, 173: 4, 131: 3, 140: 3, 127: 2, 170: 2, 155: 1})
{'wandb': {'experiment_name': '', 'project': 'ColourInteract Sweeps Friday', 'entity': 'commute_opt_gnn'}, 'data': {'name': 'ColourInteract', 'dataset': 'LRGB', 'c1': 1.0, 'c2s': [0.01, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 100.0], 'num_colours': 4, 'train_size': 1500, 'val_size': 300, 'min_train_nodes': 75, 'max_train_nodes': 125, 'min_val_nodes': 125, 'max_val_nodes': 175, 'rewirers': ['cayley_clusters', 'cayley'], 'normalise': True, 'seed': 93, 'c2': 1.0}, 'model': {'hidden_channels': 8, 'num_layers': 5, 'drop_prob': 0.1, 'global_pool_aggr': 'global_mean_pool', 'approaches': ['only_diff'], 'seeds': [17, 47, 23], 'approach': 'only_diff', 'seed': 23}, 'train': {'lr': 0.0001, 'num_epochs': 400, 'print_every': 5, 'train_batch_size': 32, 'val_batch_size': 32}, 'run': {'silent': False}}
Device: cpu
train targets: 39.43 +/- 5.791
val targets: 57.83 +/- 5.922
Rewirer cayley_clusters with seed 17
Using MSE Loss...
Minimum val loss at epoch 400: 16.95755777359009
Final val loss at epoch 400: 16.95755777359009
Train / Val loss by epoch
5: 337.162 / 1022.022
10: 36.369 / 146.202
15: 32.292 / 102.479
20: 23.093 / 90.652
25: 26.034 / 78.210
30: 19.048 / 69.326
35: 17.098 / 57.691
40: 8.290 / 57.426
45: 12.728 / 47.162
50: 11.155 / 51.226
55: 6.017 / 40.043
60: 17.498 / 41.020
65: 12.828 / 38.693
70: 5.099 / 42.333
75: 9.079 / 36.648
80: 12.838 / 38.038
85: 7.437 / 38.799
90: 7.851 / 38.406
95: 7.454 / 39.040
100: 8.377 / 33.271
105: 9.464 / 33.404
110: 7.291 / 35.067
115: 5.307 / 31.671
120: 6.019 / 29.981
125: 8.754 / 32.500
130: 7.983 / 28.624
135: 8.624 / 24.475
140: 5.950 / 28.628
145: 8.373 / 26.658
150: 4.315 / 29.593
155: 4.691 / 26.371
160: 7.703 / 29.860
165: 3.605 / 24.908
170: 5.382 / 27.686
175: 6.841 / 24.799
180: 3.549 / 23.715
185: 5.783 / 24.903
190: 4.789 / 22.521
195: 4.010 / 20.647
200: 3.042 / 21.500
205: 3.132 / 22.841
210: 3.412 / 24.501
215: 3.280 / 22.400
220: 3.369 / 20.014
225: 3.694 / 21.601
230: 5.241 / 20.562
235: 2.761 / 22.875
240: 3.358 / 19.846
245: 2.449 / 21.673
250: 2.249 / 20.353
255: 3.456 / 19.268
260: 2.914 / 21.509
265: 2.571 / 20.257
270: 4.186 / 20.576
275: 3.280 / 21.106
280: 2.842 / 18.835
285: 3.403 / 21.902
290: 2.437 / 19.767
295: 4.944 / 21.275
300: 3.082 / 21.493
305: 3.248 / 19.671
310: 5.766 / 19.703
315: 2.262 / 21.979
320: 1.837 / 19.331
325: 2.596 / 19.115
330: 2.244 / 18.922
335: 2.936 / 19.296
340: 2.442 / 17.422
345: 2.914 / 18.807
350: 1.923 / 18.156
355: 2.673 / 19.255
360: 2.407 / 18.043
365: 1.294 / 18.449
370: 1.992 / 18.368
375: 2.650 / 19.354
380: 2.418 / 19.818
385: 1.697 / 18.988
390: 2.350 / 18.596
395: 2.010 / 18.435
400: 2.220 / 16.958
Rewirer cayley_clusters with seed 47
Using MSE Loss...
Minimum val loss at epoch 225: 12.31708574295044
Final val loss at epoch 400: 16.497990703582765
Train / Val loss by epoch
5: 16.965 / 25.382
10: 15.598 / 24.514
15: 12.276 / 24.257
20: 7.579 / 25.747
25: 8.990 / 28.520
30: 4.766 / 29.520
35: 4.236 / 37.760
40: 10.342 / 33.826
45: 7.533 / 34.179
50: 6.900 / 39.453
55: 3.976 / 36.939
60: 7.421 / 38.763
65: 4.646 / 40.342
70: 4.613 / 39.629
75: 2.405 / 34.816
80: 4.940 / 33.144
85: 4.565 / 32.243
90: 5.471 / 29.845
95: 4.291 / 32.914
100: 1.999 / 29.209
105: 4.806 / 28.583
110: 2.961 / 25.929
115: 1.855 / 24.569
120: 2.891 / 22.949
125: 2.548 / 22.147
130: 2.772 / 20.747
135: 3.187 / 20.818
140: 2.450 / 19.911
145: 2.499 / 18.625
150: 2.553 / 18.500
155: 2.459 / 18.985
160: 1.535 / 16.561
165: 2.850 / 17.092
170: 2.251 / 15.651
175: 2.562 / 16.304
180: 2.063 / 14.404
185: 2.230 / 16.185
190: 2.595 / 15.384
195: 2.787 / 13.179
200: 2.024 / 13.136
205: 3.193 / 13.485
210: 1.730 / 14.157
215: 2.100 / 13.367
220: 3.062 / 13.821
225: 2.215 / 12.317
230: 3.087 / 14.564
235: 3.146 / 14.570
240: 2.834 / 13.160
245: 2.426 / 12.846
250: 1.765 / 13.587
255: 2.514 / 14.387
260: 1.827 / 13.420
265: 1.829 / 14.452
270: 2.731 / 14.160
275: 1.727 / 15.341
280: 2.033 / 13.449
285: 2.210 / 15.149
290: 2.316 / 14.540
295: 2.694 / 14.131
300: 1.424 / 14.343
305: 2.015 / 16.046
310: 1.986 / 14.454
315: 1.452 / 15.410
320: 2.384 / 13.827
325: 2.379 / 15.620
330: 2.094 / 14.816
335: 2.604 / 14.307
340: 1.429 / 14.955
345: 1.670 / 16.620
350: 1.747 / 16.923
355: 1.432 / 16.348
360: 2.188 / 16.779
365: 1.301 / 15.610
370: 1.506 / 14.651
375: 1.543 / 15.208
380: 1.964 / 16.655
385: 1.185 / 17.575
390: 0.851 / 15.525
395: 1.188 / 16.135
400: 1.770 / 16.498
Rewirer cayley_clusters with seed 23
Using MSE Loss...
Minimum val loss at epoch 75: 3.3441876649856566
Final val loss at epoch 400: 6.3333234786987305
Train / Val loss by epoch
5: 17.763 / 25.402
10: 18.496 / 13.898
15: 14.195 / 9.547
20: 15.052 / 6.959
25: 9.898 / 6.308
30: 5.468 / 5.248
35: 8.058 / 5.410
40: 8.589 / 4.739
45: 8.725 / 4.489
50: 12.337 / 4.463
55: 4.746 / 3.720
60: 6.062 / 3.688
65: 4.068 / 3.881
70: 4.857 / 3.655
75: 2.676 / 3.344
80: 4.129 / 4.556
85: 3.771 / 4.723
90: 5.444 / 4.640
95: 3.599 / 4.896
100: 3.530 / 4.831
105: 3.037 / 5.988
110: 4.935 / 6.084
115: 3.086 / 5.213
120: 6.585 / 4.670
125: 3.071 / 6.752
130: 1.897 / 6.161
135: 5.848 / 6.238
140: 2.990 / 6.476
145: 2.356 / 5.776
150: 2.450 / 7.223
155: 3.366 / 5.572
160: 4.430 / 7.206
165: 3.537 / 7.507
170: 3.260 / 6.747
175: 2.351 / 6.435
180: 2.925 / 7.122
185: 3.066 / 7.645
190: 5.613 / 8.187
195: 3.086 / 7.201
200: 2.540 / 8.325
205: 2.171 / 7.680
210: 1.705 / 6.274
215: 3.475 / 6.191
220: 3.021 / 6.128
225: 3.317 / 7.692
230: 3.859 / 6.597
235: 3.687 / 6.688
240: 3.183 / 7.293
245: 2.422 / 5.712
250: 3.446 / 6.582
255: 2.021 / 6.318
260: 2.821 / 5.594
265: 3.190 / 6.746
270: 2.674 / 6.121
275: 2.876 / 6.474
280: 2.685 / 5.161
285: 3.171 / 5.153
290: 2.500 / 5.762
295: 1.739 / 5.126
300: 1.231 / 4.783
305: 2.226 / 5.482
310: 1.580 / 5.676
315: 3.392 / 5.555
320: 1.164 / 5.567
325: 2.530 / 6.884
330: 2.143 / 5.954
335: 1.524 / 5.453
340: 2.701 / 5.504
345: 1.918 / 5.420
350: 2.714 / 6.331
355: 1.761 / 6.737
360: 2.104 / 5.978
365: 2.383 / 6.118
370: 2.344 / 5.923
375: 1.868 / 5.734
380: 1.885 / 6.048
385: 1.442 / 6.474
390: 1.432 / 7.051
395: 1.247 / 6.921
400: 1.626 / 6.333
Rewirer cayley with seed 17
Using MSE Loss...
Minimum val loss at epoch 350: 7.927302503585816
Final val loss at epoch 400: 8.244167709350586
Train / Val loss by epoch
5: 265.063 / 783.671
10: 48.061 / 87.046
15: 27.443 / 66.333
20: 19.498 / 55.269
25: 18.536 / 40.769
30: 14.812 / 34.919
35: 9.762 / 33.320
40: 8.934 / 32.342
45: 8.457 / 24.855
50: 11.657 / 27.507
55: 9.861 / 24.674
60: 3.992 / 24.554
65: 5.765 / 24.603
70: 9.802 / 25.385
75: 9.239 / 22.821
80: 9.883 / 24.776
85: 8.827 / 23.930
90: 6.113 / 25.741
95: 6.712 / 28.677
100: 6.119 / 29.218
105: 7.018 / 25.791
110: 6.333 / 30.927
115: 6.575 / 28.331
120: 6.246 / 25.138
125: 6.658 / 29.494
130: 6.176 / 24.605
135: 4.831 / 24.323
140: 5.984 / 24.610
145: 7.271 / 23.567
150: 4.215 / 20.830
155: 4.115 / 18.073
160: 4.457 / 19.796
165: 3.777 / 17.383
170: 5.508 / 20.786
175: 6.136 / 16.803
180: 5.820 / 16.621
185: 3.379 / 14.722
190: 5.100 / 13.838
195: 3.481 / 15.179
200: 4.275 / 14.471
205: 4.119 / 13.499
210: 2.609 / 12.926
215: 3.066 / 13.817
220: 3.172 / 12.204
225: 4.538 / 14.011
230: 3.735 / 12.034
235: 4.618 / 12.305
240: 2.543 / 11.837
245: 1.603 / 9.671
250: 3.061 / 10.831
255: 1.789 / 9.129
260: 2.340 / 10.033
265: 2.254 / 9.936
270: 4.638 / 12.618
275: 2.388 / 10.850
280: 2.259 / 9.169
285: 3.112 / 8.954
290: 2.259 / 9.746
295: 3.363 / 9.428
300: 3.531 / 10.264
305: 4.686 / 9.407
310: 2.590 / 9.366
315: 4.435 / 10.320
320: 1.374 / 10.156
325: 1.674 / 8.583
330: 1.919 / 8.621
335: 3.031 / 9.225
340: 2.861 / 8.941
345: 4.044 / 8.633
350: 1.337 / 7.927
355: 2.845 / 9.116
360: 2.571 / 8.928
365: 1.477 / 8.882
370: 1.798 / 8.281
375: 2.861 / 9.491
380: 2.021 / 9.880
385: 2.042 / 9.258
390: 3.149 / 10.375
395: 2.422 / 9.455
400: 3.016 / 8.244
Rewirer cayley with seed 47
Using MSE Loss...
Minimum val loss at epoch 220: 15.70277509689331
Final val loss at epoch 400: 18.41920394897461
Train / Val loss by epoch
5: 25.276 / 134.617
10: 13.626 / 107.655
15: 20.032 / 107.448
20: 8.153 / 100.481
25: 11.170 / 89.817
30: 5.885 / 87.997
35: 5.926 / 91.826
40: 10.055 / 79.347
45: 7.584 / 78.679
50: 7.677 / 75.417
55: 5.310 / 64.145
60: 4.861 / 63.225
65: 6.024 / 61.733
70: 5.661 / 55.607
75: 4.636 / 50.230
80: 5.498 / 47.271
85: 4.682 / 44.756
90: 6.452 / 39.685
95: 5.015 / 41.399
100: 5.954 / 38.521
105: 4.854 / 36.372
110: 4.042 / 36.389
115: 3.032 / 31.634
120: 2.841 / 31.990
125: 4.198 / 29.090
130: 3.127 / 26.213
135: 3.185 / 26.073
140: 3.371 / 23.855
145: 2.692 / 23.272
150: 3.844 / 22.729
155: 2.670 / 24.288
160: 1.614 / 21.673
165: 2.800 / 18.864
170: 1.917 / 18.286
175: 2.914 / 19.850
180: 2.428 / 17.445
185: 2.627 / 17.212
190: 2.204 / 17.873
195: 3.060 / 16.378
200: 3.049 / 18.092
205: 4.218 / 17.367
210: 1.725 / 17.315
215: 3.070 / 17.058
220: 2.482 / 15.703
225: 2.942 / 15.883
230: 2.736 / 18.376
235: 3.444 / 18.004
240: 2.675 / 17.836
245: 2.964 / 16.478
250: 1.684 / 17.351
255: 3.094 / 18.436
260: 2.441 / 16.829
265: 2.329 / 17.403
270: 1.986 / 17.079
275: 2.712 / 17.934
280: 1.965 / 16.264
285: 2.330 / 16.841
290: 2.396 / 19.310
295: 1.907 / 17.604
300: 2.138 / 16.340
305: 1.906 / 18.401
310: 2.148 / 16.972
315: 2.116 / 18.643
320: 1.934 / 17.844
325: 1.497 / 18.262
330: 2.561 / 17.378
335: 2.917 / 18.338
340: 1.751 / 18.506
345: 2.061 / 18.938
350: 1.684 / 18.565
355: 0.970 / 19.717
360: 2.641 / 19.668
365: 1.315 / 19.019
370: 2.247 / 18.011
375: 2.142 / 17.246
380: 1.672 / 19.391
385: 1.322 / 18.767
390: 1.362 / 18.061
395: 1.752 / 18.077
400: 1.452 / 18.419
Rewirer cayley with seed 23
Using MSE Loss...
Minimum val loss at epoch 345: 2.791967511177063
Final val loss at epoch 400: 2.9335064589977264
Train / Val loss by epoch
5: 26.473 / 57.579
10: 30.168 / 35.355
15: 14.580 / 27.824
20: 20.640 / 15.544
25: 11.396 / 13.288
30: 10.511 / 8.987
35: 6.697 / 5.426
40: 7.522 / 4.579
45: 5.932 / 4.405
50: 10.018 / 3.768
55: 5.927 / 3.708
60: 5.154 / 3.936
65: 5.000 / 4.011
70: 3.990 / 4.785
75: 4.531 / 4.742
80: 4.879 / 4.255
85: 4.383 / 3.935
90: 3.750 / 4.516
95: 5.093 / 4.516
100: 2.661 / 4.594
105: 3.056 / 4.359
110: 4.299 / 4.603
115: 3.611 / 4.975
120: 6.331 / 4.858
125: 3.905 / 4.345
130: 2.281 / 4.594
135: 3.709 / 4.864
140: 3.645 / 4.789
145: 2.420 / 5.047
150: 2.802 / 4.608
155: 3.184 / 4.852
160: 5.103 / 4.413
165: 2.879 / 4.476
170: 2.717 / 4.687
175: 3.337 / 4.735
180: 2.270 / 4.411
185: 3.044 / 4.040
190: 4.053 / 3.914
195: 2.056 / 4.157
200: 1.460 / 3.830
205: 3.063 / 3.748
210: 2.214 / 3.674
215: 1.941 / 4.032
220: 2.516 / 4.145
225: 2.633 / 3.431
230: 4.739 / 3.659
235: 3.460 / 3.404
240: 2.600 / 3.400
245: 1.715 / 3.357
250: 2.164 / 3.240
255: 2.380 / 3.353
260: 2.833 / 3.372
265: 3.539 / 3.161
270: 2.475 / 2.969
275: 3.134 / 2.987
280: 2.014 / 2.905
285: 1.945 / 2.942
290: 2.847 / 3.006
295: 2.230 / 2.987
300: 1.767 / 2.844
305: 2.061 / 2.986
310: 1.671 / 2.891
315: 2.792 / 2.927
320: 1.382 / 2.893
325: 2.110 / 3.035
330: 2.345 / 2.984
335: 1.874 / 2.876
340: 1.813 / 2.892
345: 2.261 / 2.792
350: 2.967 / 3.050
355: 1.488 / 3.191
360: 1.920 / 3.046
365: 1.552 / 3.090
370: 2.233 / 3.035
375: 1.516 / 2.850
380: 1.999 / 3.342
385: 1.556 / 3.006
390: 0.830 / 2.840
395: 1.769 / 2.984
400: 1.168 / 2.934
{'cayley': [{'best': 7.927302503585816, 'end': 8.244167709350586},
            {'best': 15.70277509689331, 'end': 18.41920394897461},
            {'best': 2.791967511177063, 'end': 2.9335064589977264}],
 'cayley_clusters': [{'best': 16.95755777359009, 'end': 16.95755777359009},
                     {'best': 12.31708574295044, 'end': 16.497990703582765},
                     {'best': 3.3441876649856566, 'end': 6.3333234786987305}]}
Loading data...
Number of training graphs: 1500
Train graph sizes: Counter({101: 46, 114: 45, 79: 44, 87: 40, 94: 39, 122: 39, 88: 37, 92: 37, 107: 36, 119: 36, 80: 34, 105: 34, 75: 33, 99: 33, 112: 33, 116: 33, 121: 33, 78: 32, 84: 32, 98: 32, 100: 32, 120: 32, 96: 31, 111: 31, 118: 31, 81: 29, 83: 28, 95: 28, 103: 28, 108: 28, 82: 27, 89: 27, 106: 27, 93: 26, 90: 26, 97: 26, 109: 25, 115: 25, 117: 25, 85: 24, 124: 24, 76: 22, 86: 22, 91: 22, 102: 22, 104: 22, 110: 22, 123: 22, 77: 19, 113: 19})
{25: [], 26: [], 27: [], 28: [], 29: [], 30: [], 31: [], 32: [], 33: [], 34: []}
Number of validation graphs: 1500
Val graph sizes: Counter({159: 12, 168: 11, 172: 11, 130: 9, 164: 9, 174: 9, 126: 8, 142: 8, 141: 8, 149: 8, 125: 7, 129: 7, 133: 7, 134: 7, 139: 7, 154: 7, 153: 7, 160: 7, 128: 6, 135: 6, 137: 6, 136: 6, 143: 6, 146: 6, 147: 6, 152: 6, 150: 6, 158: 6, 157: 6, 165: 6, 138: 5, 144: 5, 148: 5, 145: 5, 156: 5, 163: 5, 162: 5, 166: 5, 132: 4, 151: 4, 161: 4, 169: 4, 167: 4, 171: 4, 173: 4, 131: 3, 140: 3, 127: 2, 170: 2, 155: 1})
{'wandb': {'experiment_name': '', 'project': 'ColourInteract Sweeps Friday', 'entity': 'commute_opt_gnn'}, 'data': {'name': 'ColourInteract', 'dataset': 'LRGB', 'c1': 1.0, 'c2s': [0.01, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 100.0], 'num_colours': 4, 'train_size': 1500, 'val_size': 300, 'min_train_nodes': 75, 'max_train_nodes': 125, 'min_val_nodes': 125, 'max_val_nodes': 175, 'rewirers': ['cayley_clusters', 'cayley'], 'normalise': True, 'seed': 93, 'c2': 2.0}, 'model': {'hidden_channels': 8, 'num_layers': 5, 'drop_prob': 0.1, 'global_pool_aggr': 'global_mean_pool', 'approaches': ['only_diff'], 'seeds': [17, 47, 23], 'approach': 'only_diff', 'seed': 23}, 'train': {'lr': 0.0001, 'num_epochs': 400, 'print_every': 5, 'train_batch_size': 32, 'val_batch_size': 32}, 'run': {'silent': False}}
Device: cpu
train targets: 75.85 +/- 11.526
val targets: 112.65 +/- 11.774
Rewirer cayley_clusters with seed 17
Using MSE Loss...
Minimum val loss at epoch 135: 78.77164993286132
Final val loss at epoch 400: 118.43637237548828
Train / Val loss by epoch
5: 2047.323 / 4715.759
10: 55.229 / 466.962
15: 41.981 / 274.105
20: 33.744 / 249.634
25: 48.216 / 217.387
30: 35.973 / 196.855
35: 36.534 / 174.348
40: 16.974 / 170.992
45: 29.918 / 149.137
50: 24.920 / 157.329
55: 15.044 / 135.360
60: 30.854 / 137.524
65: 21.323 / 135.033
70: 18.980 / 135.083
75: 25.023 / 123.183
80: 34.737 / 112.517
85: 21.930 / 118.224
90: 18.130 / 111.872
95: 12.492 / 107.405
100: 15.456 / 95.739
105: 20.546 / 94.360
110: 15.166 / 93.240
115: 11.704 / 92.753
120: 17.950 / 89.912
125: 17.366 / 95.061
130: 13.882 / 95.684
135: 17.316 / 78.772
140: 11.472 / 82.573
145: 8.588 / 89.966
150: 7.872 / 95.262
155: 10.648 / 94.906
160: 13.379 / 104.363
165: 10.501 / 95.111
170: 13.119 / 104.222
175: 13.898 / 91.235
180: 9.889 / 98.546
185: 12.985 / 103.730
190: 10.171 / 91.108
195: 12.564 / 94.636
200: 11.495 / 102.556
205: 8.534 / 96.239
210: 9.555 / 102.707
215: 5.868 / 108.629
220: 8.013 / 100.551
225: 9.639 / 105.421
230: 9.658 / 101.881
235: 8.853 / 109.176
240: 10.523 / 104.995
245: 5.685 / 112.526
250: 8.236 / 110.298
255: 8.204 / 98.089
260: 4.381 / 100.762
265: 5.898 / 108.686
270: 10.786 / 113.295
275: 6.957 / 113.589
280: 11.037 / 112.817
285: 7.150 / 111.881
290: 4.141 / 112.453
295: 11.737 / 119.573
300: 11.151 / 126.451
305: 7.527 / 117.730
310: 14.310 / 119.646
315: 8.291 / 121.079
320: 6.717 / 121.805
325: 6.412 / 117.392
330: 11.701 / 116.138
335: 9.465 / 122.573
340: 5.632 / 110.333
345: 8.479 / 115.956
350: 7.869 / 122.223
355: 5.807 / 126.718
360: 7.480 / 118.617
365: 5.379 / 129.317
370: 5.164 / 112.800
375: 6.923 / 125.208
380: 6.870 / 123.010
385: 6.887 / 123.219
390: 4.960 / 117.335
395: 6.810 / 118.396
400: 8.721 / 118.436
Rewirer cayley_clusters with seed 47
Using MSE Loss...
Minimum val loss at epoch 130: 98.5734748840332
Final val loss at epoch 400: 138.20707168579102
Train / Val loss by epoch
5: 34.062 / 127.124
10: 32.034 / 132.530
15: 26.359 / 130.885
20: 17.913 / 139.519
25: 19.975 / 148.350
30: 12.561 / 128.784
35: 11.198 / 144.564
40: 26.768 / 128.518
45: 16.510 / 130.460
50: 16.368 / 134.622
55: 11.608 / 122.730
60: 16.186 / 131.405
65: 14.819 / 141.738
70: 12.474 / 141.585
75: 9.672 / 126.425
80: 13.998 / 122.562
85: 13.309 / 118.975
90: 15.499 / 122.083
95: 12.223 / 124.232
100: 9.147 / 113.932
105: 16.030 / 121.786
110: 11.380 / 116.473
115: 6.209 / 111.551
120: 8.220 / 115.974
125: 8.060 / 102.510
130: 9.336 / 98.573
135: 10.509 / 107.414
140: 9.795 / 108.379
145: 7.877 / 102.198
150: 8.314 / 107.571
155: 9.324 / 114.716
160: 4.940 / 108.250
165: 6.862 / 107.324
170: 5.869 / 100.465
175: 9.652 / 106.200
180: 7.208 / 107.307
185: 6.848 / 111.901
190: 10.580 / 107.892
195: 7.848 / 105.558
200: 5.011 / 109.824
205: 8.944 / 108.237
210: 6.426 / 116.936
215: 5.496 / 108.994
220: 9.041 / 109.620
225: 8.427 / 107.591
230: 9.372 / 117.050
235: 8.637 / 117.701
240: 6.919 / 114.267
245: 8.117 / 115.893
250: 6.170 / 121.373
255: 8.162 / 125.418
260: 4.980 / 123.290
265: 7.497 / 123.584
270: 8.479 / 115.867
275: 4.907 / 127.440
280: 7.182 / 117.962
285: 7.621 / 123.125
290: 9.521 / 128.653
295: 7.361 / 127.119
300: 5.156 / 124.726
305: 7.220 / 128.914
310: 5.962 / 126.993
315: 5.914 / 126.896
320: 8.775 / 126.923
325: 8.213 / 134.109
330: 5.447 / 125.486
335: 8.833 / 128.322
340: 5.962 / 133.107
345: 6.497 / 133.924
350: 6.169 / 142.182
355: 4.115 / 135.751
360: 7.481 / 133.604
365: 3.826 / 131.144
370: 6.309 / 125.455
375: 6.696 / 126.409
380: 6.440 / 136.835
385: 5.183 / 139.314
390: 3.581 / 132.700
395: 5.714 / 133.204
400: 6.855 / 138.207
Rewirer cayley_clusters with seed 23
Using MSE Loss...
Minimum val loss at epoch 75: 36.382835960388185
Final val loss at epoch 400: 77.95004844665527
Train / Val loss by epoch
5: 34.348 / 138.469
10: 35.177 / 108.356
15: 26.418 / 78.581
20: 28.692 / 69.313
25: 23.479 / 62.237
30: 14.690 / 54.144
35: 11.948 / 52.857
40: 13.429 / 48.808
45: 16.264 / 48.687
50: 20.170 / 49.151
55: 13.543 / 43.028
60: 9.520 / 42.152
65: 8.888 / 37.803
70: 9.941 / 37.331
75: 8.727 / 36.383
80: 9.465 / 42.516
85: 9.822 / 44.853
90: 11.097 / 41.020
95: 9.094 / 44.570
100: 7.866 / 45.029
105: 8.621 / 50.009
110: 9.538 / 45.330
115: 9.349 / 43.261
120: 13.095 / 42.357
125: 6.881 / 51.046
130: 9.478 / 49.620
135: 11.993 / 46.932
140: 9.745 / 49.288
145: 7.546 / 44.251
150: 5.153 / 50.777
155: 8.419 / 41.400
160: 7.196 / 46.843
165: 8.612 / 52.893
170: 7.303 / 47.326
175: 4.854 / 45.777
180: 8.148 / 49.057
185: 7.320 / 51.118
190: 10.272 / 52.671
195: 6.676 / 52.742
200: 5.867 / 53.426
205: 5.853 / 52.761
210: 6.423 / 45.280
215: 7.376 / 46.515
220: 7.812 / 46.131
225: 7.355 / 53.639
230: 6.209 / 53.026
235: 7.701 / 51.133
240: 7.476 / 53.912
245: 6.313 / 47.879
250: 7.638 / 51.793
255: 5.381 / 48.801
260: 6.924 / 49.044
265: 7.454 / 54.383
270: 5.331 / 53.560
275: 7.696 / 54.785
280: 10.337 / 51.130
285: 11.671 / 55.688
290: 5.117 / 51.817
295: 7.345 / 51.678
300: 4.092 / 49.187
305: 7.301 / 54.918
310: 4.507 / 60.718
315: 7.670 / 57.204
320: 5.075 / 59.264
325: 7.261 / 61.748
330: 3.734 / 62.629
335: 5.937 / 61.718
340: 5.470 / 60.094
345: 6.420 / 61.275
350: 6.214 / 64.185
355: 4.090 / 69.852
360: 8.234 / 67.313
365: 5.375 / 69.172
370: 5.761 / 65.954
375: 3.642 / 70.477
380: 6.111 / 74.221
385: 4.357 / 71.099
390: 4.239 / 75.005
395: 4.300 / 74.755
400: 5.130 / 77.950
Rewirer cayley with seed 17
Using MSE Loss...
Minimum val loss at epoch 260: 50.968353271484375
Final val loss at epoch 400: 76.93490943908691
Train / Val loss by epoch
5: 1878.863 / 4200.281
10: 68.543 / 346.808
15: 37.375 / 204.321
20: 31.560 / 174.635
25: 33.838 / 135.171
30: 28.326 / 114.373
35: 25.834 / 109.016
40: 21.587 / 103.188
45: 23.412 / 83.109
50: 30.182 / 90.245
55: 24.127 / 82.678
60: 15.157 / 80.864
65: 18.020 / 75.130
70: 19.997 / 73.391
75: 12.459 / 65.797
80: 25.605 / 64.379
85: 19.053 / 60.573
90: 15.305 / 60.357
95: 13.376 / 61.218
100: 18.398 / 59.713
105: 14.071 / 55.839
110: 8.112 / 60.251
115: 13.133 / 58.437
120: 15.025 / 53.434
125: 12.198 / 64.389
130: 11.012 / 56.193
135: 8.455 / 58.457
140: 11.590 / 58.963
145: 14.696 / 63.590
150: 10.457 / 63.024
155: 6.861 / 56.917
160: 14.609 / 60.414
165: 10.357 / 57.369
170: 10.929 / 69.905
175: 12.474 / 56.010
180: 10.159 / 62.632
185: 8.215 / 63.359
190: 9.621 / 59.447
195: 7.629 / 62.795
200: 10.440 / 61.476
205: 10.278 / 57.913
210: 8.022 / 56.349
215: 5.781 / 62.226
220: 8.791 / 57.385
225: 8.586 / 65.535
230: 10.535 / 61.051
235: 13.454 / 60.616
240: 11.167 / 59.085
245: 4.843 / 54.256
250: 7.777 / 57.200
255: 5.765 / 52.700
260: 4.826 / 50.968
265: 6.789 / 52.306
270: 7.418 / 56.304
275: 6.965 / 58.603
280: 5.887 / 53.334
285: 6.215 / 58.886
290: 6.394 / 57.052
295: 11.281 / 58.645
300: 11.009 / 65.943
305: 7.929 / 62.535
310: 8.088 / 61.637
315: 9.998 / 68.926
320: 4.701 / 63.604
325: 6.919 / 60.597
330: 7.185 / 63.025
335: 9.412 / 66.006
340: 6.804 / 61.743
345: 7.118 / 60.069
350: 6.599 / 62.780
355: 6.796 / 67.939
360: 6.610 / 63.228
365: 5.104 / 69.660
370: 5.785 / 67.861
375: 5.909 / 75.749
380: 6.941 / 81.115
385: 4.822 / 84.720
390: 7.299 / 77.084
395: 7.023 / 79.948
400: 8.644 / 76.935
Rewirer cayley with seed 47
Using MSE Loss...
Minimum val loss at epoch 195: 106.00835800170898
Final val loss at epoch 400: 151.45950775146486
Train / Val loss by epoch
5: 52.183 / 256.421
10: 34.637 / 250.701
15: 39.651 / 245.470
20: 19.861 / 245.151
25: 19.246 / 250.136
30: 11.141 / 222.728
35: 17.468 / 228.621
40: 25.047 / 205.240
45: 18.300 / 202.585
50: 18.401 / 193.740
55: 11.557 / 171.016
60: 14.574 / 174.413
65: 18.715 / 169.356
70: 14.178 / 157.572
75: 14.741 / 155.981
80: 12.433 / 140.586
85: 12.022 / 136.189
90: 17.617 / 132.907
95: 11.156 / 139.205
100: 15.286 / 130.426
105: 16.323 / 132.064
110: 11.445 / 133.478
115: 7.372 / 122.352
120: 10.586 / 126.986
125: 10.557 / 120.046
130: 9.264 / 113.166
135: 10.177 / 115.280
140: 11.469 / 109.893
145: 9.174 / 111.082
150: 11.327 / 113.671
155: 9.740 / 124.392
160: 6.036 / 111.207
165: 7.959 / 108.605
170: 5.721 / 109.329
175: 8.029 / 113.542
180: 7.540 / 109.919
185: 8.081 / 107.737
190: 7.754 / 110.163
195: 8.916 / 106.008
200: 8.092 / 114.041
205: 11.259 / 110.060
210: 5.527 / 114.419
215: 8.980 / 113.401
220: 10.340 / 107.075
225: 8.783 / 110.388
230: 7.384 / 119.089
235: 8.358 / 121.410
240: 6.565 / 123.533
245: 9.431 / 113.738
250: 4.781 / 123.001
255: 9.442 / 125.788
260: 6.544 / 119.887
265: 7.417 / 120.911
270: 6.936 / 118.275
275: 8.450 / 126.140
280: 7.177 / 122.258
285: 7.028 / 123.288
290: 7.612 / 126.609
295: 5.504 / 124.998
300: 6.593 / 122.410
305: 7.220 / 130.011
310: 7.643 / 122.707
315: 6.944 / 131.821
320: 7.481 / 131.078
325: 6.191 / 136.285
330: 8.494 / 131.490
335: 9.289 / 129.974
340: 5.338 / 143.041
345: 6.152 / 139.298
350: 4.593 / 141.416
355: 4.389 / 145.220
360: 9.480 / 139.120
365: 4.413 / 144.603
370: 7.223 / 140.820
375: 6.998 / 137.917
380: 3.861 / 150.872
385: 6.083 / 144.821
390: 4.889 / 145.339
395: 5.349 / 142.389
400: 6.461 / 151.460
Rewirer cayley with seed 23
Using MSE Loss...
Minimum val loss at epoch 145: 13.273101854324342
Final val loss at epoch 400: 34.956657791137694
Train / Val loss by epoch
5: 45.363 / 173.077
10: 48.376 / 113.009
15: 28.786 / 84.832
20: 36.003 / 58.917
25: 23.050 / 46.306
30: 28.748 / 33.503
35: 16.205 / 25.364
40: 17.534 / 23.281
45: 13.590 / 21.535
50: 19.069 / 18.867
55: 15.952 / 16.635
60: 12.712 / 16.655
65: 13.002 / 15.813
70: 9.136 / 15.620
75: 10.824 / 15.340
80: 9.774 / 15.075
85: 8.073 / 15.020
90: 7.427 / 14.528
95: 12.605 / 14.763
100: 7.846 / 14.507
105: 9.112 / 14.452
110: 10.826 / 14.328
115: 14.065 / 13.786
120: 13.125 / 13.970
125: 9.331 / 14.910
130: 6.014 / 14.515
135: 8.240 / 13.856
140: 9.817 / 13.276
145: 9.082 / 13.273
150: 5.388 / 14.210
155: 9.273 / 13.386
160: 6.817 / 14.121
165: 7.587 / 14.339
170: 7.624 / 13.352
175: 7.027 / 13.442
180: 5.022 / 14.103
185: 7.644 / 15.795
190: 8.155 / 14.960
195: 4.969 / 15.153
200: 2.661 / 15.726
205: 7.151 / 15.229
210: 7.235 / 14.131
215: 5.280 / 13.755
220: 6.502 / 14.201
225: 6.414 / 15.263
230: 9.936 / 16.188
235: 6.826 / 17.207
240: 5.195 / 17.320
245: 5.298 / 16.591
250: 4.370 / 16.763
255: 9.229 / 16.075
260: 6.588 / 16.439
265: 6.798 / 17.372
270: 5.746 / 19.888
275: 7.609 / 20.066
280: 8.480 / 17.486
285: 7.532 / 20.391
290: 5.313 / 20.208
295: 5.354 / 20.195
300: 4.236 / 19.139
305: 5.488 / 23.051
310: 4.448 / 24.481
315: 6.722 / 25.734
320: 4.577 / 24.741
325: 6.175 / 26.405
330: 4.777 / 24.858
335: 6.081 / 26.451
340: 3.500 / 27.127
345: 5.988 / 26.383
350: 6.836 / 29.334
355: 2.974 / 32.587
360: 5.298 / 30.917
365: 4.085 / 31.048
370: 4.742 / 29.263
375: 4.430 / 30.187
380: 4.324 / 33.014
385: 4.449 / 32.142
390: 3.037 / 32.159
395: 5.052 / 32.063
400: 4.204 / 34.957
{'cayley': [{'best': 50.968353271484375, 'end': 76.93490943908691},
            {'best': 106.00835800170898, 'end': 151.45950775146486},
            {'best': 13.273101854324342, 'end': 34.956657791137694}],
 'cayley_clusters': [{'best': 78.77164993286132, 'end': 118.43637237548828},
                     {'best': 98.5734748840332, 'end': 138.20707168579102},
                     {'best': 36.382835960388185, 'end': 77.95004844665527}]}
Loading data...
Number of training graphs: 1500
Train graph sizes: Counter({101: 46, 114: 45, 79: 44, 87: 40, 94: 39, 122: 39, 88: 37, 92: 37, 107: 36, 119: 36, 80: 34, 105: 34, 75: 33, 99: 33, 112: 33, 116: 33, 121: 33, 78: 32, 84: 32, 98: 32, 100: 32, 120: 32, 96: 31, 111: 31, 118: 31, 81: 29, 83: 28, 95: 28, 103: 28, 108: 28, 82: 27, 89: 27, 106: 27, 93: 26, 90: 26, 97: 26, 109: 25, 115: 25, 117: 25, 85: 24, 124: 24, 76: 22, 86: 22, 91: 22, 102: 22, 104: 22, 110: 22, 123: 22, 77: 19, 113: 19})
{25: [], 26: [], 27: [], 28: [], 29: [], 30: [], 31: [], 32: [], 33: [], 34: []}
Number of validation graphs: 1500
Val graph sizes: Counter({159: 12, 168: 11, 172: 11, 130: 9, 164: 9, 174: 9, 126: 8, 142: 8, 141: 8, 149: 8, 125: 7, 129: 7, 133: 7, 134: 7, 139: 7, 154: 7, 153: 7, 160: 7, 128: 6, 135: 6, 137: 6, 136: 6, 143: 6, 146: 6, 147: 6, 152: 6, 150: 6, 158: 6, 157: 6, 165: 6, 138: 5, 144: 5, 148: 5, 145: 5, 156: 5, 163: 5, 162: 5, 166: 5, 132: 4, 151: 4, 161: 4, 169: 4, 167: 4, 171: 4, 173: 4, 131: 3, 140: 3, 127: 2, 170: 2, 155: 1})
{'wandb': {'experiment_name': '', 'project': 'ColourInteract Sweeps Friday', 'entity': 'commute_opt_gnn'}, 'data': {'name': 'ColourInteract', 'dataset': 'LRGB', 'c1': 1.0, 'c2s': [0.01, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 100.0], 'num_colours': 4, 'train_size': 1500, 'val_size': 300, 'min_train_nodes': 75, 'max_train_nodes': 125, 'min_val_nodes': 125, 'max_val_nodes': 175, 'rewirers': ['cayley_clusters', 'cayley'], 'normalise': True, 'seed': 93, 'c2': 5.0}, 'model': {'hidden_channels': 8, 'num_layers': 5, 'drop_prob': 0.1, 'global_pool_aggr': 'global_mean_pool', 'approaches': ['only_diff'], 'seeds': [17, 47, 23], 'approach': 'only_diff', 'seed': 23}, 'train': {'lr': 0.0001, 'num_epochs': 400, 'print_every': 5, 'train_batch_size': 32, 'val_batch_size': 32}, 'run': {'silent': False}}
Device: cpu
train targets: 185.09 +/- 28.734
val targets: 277.12 +/- 29.332
Rewirer cayley_clusters with seed 17
Using MSE Loss...
Minimum val loss at epoch 140: 900.43740234375
Final val loss at epoch 400: 1791.3495239257813
Train / Val loss by epoch
5: 24649.400 / 44715.096
10: 1903.603 / 7598.853
15: 94.863 / 1933.743
20: 103.318 / 1466.560
25: 164.483 / 1304.340
30: 113.546 / 1243.491
35: 136.914 / 1199.891
40: 52.999 / 1186.997
45: 107.957 / 1105.514
50: 110.991 / 1127.798
55: 78.878 / 1056.248
60: 93.646 / 1069.809
65: 85.315 / 1047.121
70: 104.974 / 1025.568
75: 103.600 / 992.019
80: 141.251 / 996.416
85: 84.257 / 1010.795
90: 79.038 / 959.267
95: 48.203 / 1001.292
100: 70.380 / 975.873
105: 92.898 / 975.929
110: 73.678 / 944.451
115: 33.417 / 970.780
120: 54.517 / 978.217
125: 76.430 / 981.881
130: 43.667 / 985.246
135: 56.381 / 926.178
140: 49.502 / 900.437
145: 44.234 / 1001.237
150: 37.610 / 1001.113
155: 51.293 / 1026.114
160: 47.595 / 1070.313
165: 45.813 / 1042.685
170: 51.442 / 1123.147
175: 37.622 / 1077.775
180: 37.503 / 1094.889
185: 55.822 / 1128.649
190: 38.977 / 1113.727
195: 55.642 / 1202.008
200: 33.635 / 1246.282
205: 39.827 / 1235.151
210: 36.836 / 1205.383
215: 44.449 / 1267.241
220: 22.710 / 1239.592
225: 38.590 / 1321.282
230: 37.678 / 1329.029
235: 39.456 / 1343.605
240: 50.626 / 1327.611
245: 25.474 / 1401.420
250: 37.296 / 1361.077
255: 72.797 / 1298.117
260: 35.809 / 1329.471
265: 30.677 / 1407.977
270: 57.537 / 1423.810
275: 45.466 / 1479.240
280: 34.308 / 1473.040
285: 30.527 / 1489.906
290: 24.092 / 1486.394
295: 51.113 / 1573.991
300: 44.993 / 1592.722
305: 37.124 / 1530.668
310: 53.815 / 1592.497
315: 43.709 / 1533.961
320: 29.741 / 1640.711
325: 37.559 / 1638.305
330: 43.808 / 1661.928
335: 46.079 / 1675.463
340: 21.621 / 1638.034
345: 35.104 / 1637.150
350: 35.179 / 1737.518
355: 33.188 / 1738.556
360: 42.418 / 1725.791
365: 26.504 / 1753.540
370: 29.280 / 1711.030
375: 38.745 / 1792.733
380: 38.143 / 1836.479
385: 33.695 / 1818.280
390: 23.341 / 1806.729
395: 53.020 / 1865.280
400: 56.344 / 1791.350
Rewirer cayley_clusters with seed 47
Using MSE Loss...
Minimum val loss at epoch 15: 920.3936645507813
Final val loss at epoch 400: 2045.5480712890626
Train / Val loss by epoch
5: 1222.942 / 4413.219
10: 126.421 / 1035.373
15: 141.451 / 920.394
20: 94.587 / 934.157
25: 81.457 / 944.704
30: 63.538 / 947.999
35: 75.743 / 937.185
40: 113.649 / 977.405
45: 55.764 / 974.864
50: 58.133 / 997.819
55: 58.962 / 981.796
60: 81.411 / 1000.044
65: 63.968 / 1034.587
70: 56.974 / 983.368
75: 40.932 / 978.958
80: 46.446 / 1001.580
85: 77.450 / 982.841
90: 69.188 / 984.774
95: 53.406 / 1027.053
100: 51.118 / 1008.604
105: 78.342 / 1103.863
110: 59.908 / 1123.657
115: 43.883 / 1170.717
120: 38.452 / 1165.159
125: 36.476 / 1147.644
130: 52.363 / 1164.736
135: 46.320 / 1130.110
140: 44.038 / 1145.762
145: 48.960 / 1180.207
150: 47.724 / 1186.864
155: 46.860 / 1246.822
160: 35.670 / 1265.092
165: 51.092 / 1173.593
170: 33.750 / 1230.738
175: 36.516 / 1304.232
180: 29.719 / 1306.334
185: 47.326 / 1294.229
190: 49.945 / 1355.384
195: 39.630 / 1336.043
200: 16.453 / 1366.395
205: 41.657 / 1354.532
210: 28.420 / 1416.189
215: 29.342 / 1448.068
220: 32.585 / 1399.570
225: 39.164 / 1485.113
230: 43.244 / 1564.318
235: 33.844 / 1497.501
240: 35.043 / 1568.373
245: 39.995 / 1523.005
250: 28.449 / 1627.952
255: 41.493 / 1661.492
260: 22.710 / 1690.452
265: 52.690 / 1592.739
270: 34.650 / 1620.014
275: 32.212 / 1716.583
280: 30.614 / 1675.029
285: 34.160 / 1693.623
290: 45.434 / 1736.570
295: 35.324 / 1775.507
300: 31.191 / 1780.964
305: 31.890 / 1816.483
310: 27.268 / 1778.195
315: 29.069 / 1852.353
320: 38.467 / 1817.740
325: 35.151 / 1873.154
330: 34.551 / 1772.153
335: 37.983 / 1852.471
340: 32.383 / 1832.939
345: 32.397 / 1927.059
350: 33.732 / 1937.609
355: 28.235 / 1901.611
360: 42.902 / 1971.098
365: 27.889 / 1919.383
370: 41.276 / 1940.278
375: 33.030 / 1933.038
380: 37.304 / 1936.243
385: 36.314 / 1931.474
390: 17.582 / 1973.490
395: 28.074 / 1864.727
400: 38.885 / 2045.548
Rewirer cayley_clusters with seed 23
Using MSE Loss...
Minimum val loss at epoch 90: 730.4976531982422
Final val loss at epoch 400: 1560.9613647460938
Train / Val loss by epoch
5: 268.407 / 1937.865
10: 141.261 / 1123.070
15: 105.250 / 1073.661
20: 113.515 / 1009.632
25: 111.914 / 946.918
30: 90.213 / 925.928
35: 61.214 / 892.762
40: 73.454 / 886.070
45: 65.437 / 844.709
50: 111.530 / 843.132
55: 83.853 / 810.950
60: 40.850 / 815.117
65: 61.148 / 777.600
70: 68.243 / 738.292
75: 40.836 / 768.223
80: 61.613 / 786.444
85: 41.534 / 787.836
90: 41.102 / 730.498
95: 50.894 / 783.015
100: 54.616 / 744.063
105: 54.013 / 790.501
110: 54.720 / 763.682
115: 39.636 / 730.651
120: 52.487 / 763.053
125: 31.953 / 786.416
130: 48.025 / 812.429
135: 38.920 / 783.813
140: 49.304 / 793.268
145: 43.859 / 831.686
150: 29.433 / 863.831
155: 34.058 / 825.864
160: 30.926 / 866.817
165: 40.253 / 943.129
170: 33.146 / 890.505
175: 21.508 / 866.602
180: 33.332 / 932.652
185: 35.429 / 948.494
190: 50.103 / 1000.868
195: 36.379 / 965.801
200: 33.775 / 1007.488
205: 26.767 / 1034.662
210: 35.337 / 984.069
215: 39.897 / 975.365
220: 46.422 / 1028.496
225: 28.628 / 1041.151
230: 32.150 / 1019.712
235: 32.354 / 1063.849
240: 39.582 / 1107.384
245: 37.530 / 1115.373
250: 33.612 / 1084.096
255: 34.777 / 1099.365
260: 35.671 / 1062.005
265: 28.612 / 1120.068
270: 35.457 / 1157.627
275: 39.386 / 1172.491
280: 46.418 / 1154.785
285: 48.200 / 1192.230
290: 34.178 / 1189.815
295: 39.751 / 1187.100
300: 31.356 / 1256.301
305: 40.507 / 1251.953
310: 18.568 / 1322.791
315: 29.912 / 1283.878
320: 39.584 / 1311.090
325: 35.991 / 1315.242
330: 17.849 / 1354.794
335: 36.147 / 1351.407
340: 21.372 / 1332.008
345: 34.527 / 1374.390
350: 30.037 / 1420.838
355: 24.270 / 1433.294
360: 33.700 / 1433.312
365: 23.582 / 1484.641
370: 32.970 / 1456.730
375: 21.454 / 1501.641
380: 36.353 / 1518.617
385: 27.002 / 1448.119
390: 30.444 / 1550.606
395: 29.414 / 1518.033
400: 35.260 / 1560.961
Rewirer cayley with seed 17
Using MSE Loss...
Minimum val loss at epoch 175: 535.8186096191406
Final val loss at epoch 400: 995.9110473632812
Train / Val loss by epoch
5: 24075.131 / 43187.104
10: 1864.063 / 7104.479
15: 108.546 / 1723.763
20: 109.213 / 1244.346
25: 117.985 / 1102.301
30: 95.183 / 1006.151
35: 124.881 / 1013.238
40: 104.854 / 1006.515
45: 106.641 / 900.761
50: 97.448 / 913.786
55: 93.648 / 846.139
60: 85.073 / 832.266
65: 94.724 / 794.697
70: 84.625 / 785.222
75: 56.914 / 753.545
80: 101.751 / 730.921
85: 79.409 / 687.923
90: 54.622 / 650.169
95: 58.253 / 653.048
100: 75.134 / 625.697
105: 72.223 / 621.003
110: 46.275 / 598.349
115: 40.489 / 562.973
120: 41.375 / 556.623
125: 76.903 / 591.497
130: 50.036 / 578.667
135: 46.441 / 548.905
140: 33.213 / 572.037
145: 64.039 / 560.508
150: 66.868 / 573.747
155: 28.825 / 574.636
160: 58.431 / 571.986
165: 56.961 / 566.067
170: 41.564 / 620.023
175: 39.498 / 535.819
180: 39.010 / 572.602
185: 37.337 / 592.405
190: 58.283 / 554.750
195: 51.458 / 618.745
200: 50.142 / 613.591
205: 49.693 / 613.967
210: 33.797 / 598.558
215: 23.601 / 641.693
220: 27.610 / 620.373
225: 42.554 / 693.225
230: 50.516 / 672.657
235: 54.578 / 683.986
240: 38.063 / 655.138
245: 29.717 / 675.149
250: 33.032 / 683.702
255: 54.646 / 667.373
260: 36.954 / 688.652
265: 37.932 / 699.557
270: 41.285 / 684.317
275: 39.064 / 722.405
280: 29.217 / 731.293
285: 26.623 / 759.280
290: 30.152 / 759.745
295: 45.171 / 820.861
300: 50.618 / 856.257
305: 36.603 / 772.558
310: 45.089 / 831.656
315: 42.704 / 838.383
320: 22.922 / 831.215
325: 18.244 / 878.313
330: 27.599 / 882.136
335: 44.663 / 893.853
340: 28.807 / 879.496
345: 37.037 / 873.622
350: 31.104 / 874.231
355: 33.178 / 883.853
360: 44.339 / 905.346
365: 35.668 / 980.585
370: 30.810 / 920.096
375: 26.738 / 921.807
380: 50.986 / 939.113
385: 28.287 / 983.121
390: 33.804 / 959.622
395: 50.672 / 992.043
400: 58.187 / 995.911
Rewirer cayley with seed 47
Using MSE Loss...
Minimum val loss at epoch 15: 816.1120239257813
Final val loss at epoch 400: 1834.2987670898438
Train / Val loss by epoch
5: 1274.669 / 4305.305
10: 156.774 / 885.933
15: 172.983 / 816.112
20: 128.327 / 840.248
25: 106.483 / 907.505
30: 72.560 / 907.126
35: 109.376 / 950.555
40: 135.320 / 1044.297
45: 79.025 / 1123.186
50: 80.714 / 1182.449
55: 61.587 / 1203.499
60: 91.513 / 1182.873
65: 81.559 / 1257.198
70: 65.634 / 1163.198
75: 46.735 / 1165.454
80: 56.634 / 1170.798
85: 67.433 / 1091.397
90: 86.156 / 1080.371
95: 52.442 / 1052.844
100: 64.603 / 1022.630
105: 99.085 / 1060.319
110: 45.043 / 1070.288
115: 36.089 / 1027.492
120: 43.223 / 1072.142
125: 40.458 / 1044.730
130: 56.364 / 992.376
135: 43.049 / 1008.869
140: 73.234 / 994.511
145: 52.966 / 1017.376
150: 53.689 / 1000.213
155: 45.591 / 1101.760
160: 33.270 / 1067.727
165: 37.368 / 1002.422
170: 24.140 / 1085.334
175: 33.234 / 1099.755
180: 41.074 / 1076.265
185: 41.933 / 1080.815
190: 50.200 / 1125.194
195: 51.379 / 1114.823
200: 30.810 / 1135.938
205: 51.365 / 1122.066
210: 29.729 / 1180.732
215: 38.518 / 1189.886
220: 62.759 / 1186.327
225: 48.059 / 1265.715
230: 37.942 / 1340.721
235: 44.409 / 1314.028
240: 33.730 / 1375.853
245: 43.745 / 1271.103
250: 24.413 / 1394.816
255: 58.566 / 1401.553
260: 30.516 / 1448.346
265: 41.764 / 1434.866
270: 42.235 / 1400.703
275: 43.097 / 1478.841
280: 31.365 / 1443.106
285: 34.877 / 1473.577
290: 45.283 / 1507.947
295: 30.569 / 1530.908
300: 29.236 / 1538.280
305: 39.022 / 1598.468
310: 26.834 / 1572.882
315: 35.963 / 1624.760
320: 46.896 / 1621.286
325: 27.268 / 1676.163
330: 51.741 / 1654.416
335: 42.549 / 1678.300
340: 31.771 / 1722.824
345: 32.706 / 1784.299
350: 21.610 / 1777.262
355: 31.788 / 1771.412
360: 51.845 / 1789.801
365: 28.502 / 1752.675
370: 38.859 / 1798.694
375: 35.914 / 1767.413
380: 27.878 / 1848.235
385: 31.957 / 1810.982
390: 27.761 / 1871.520
395: 33.105 / 1747.105
400: 34.825 / 1834.299
Rewirer cayley with seed 23
Using MSE Loss...
Minimum val loss at epoch 90: 275.3428070068359
Final val loss at epoch 400: 839.7964294433593
Train / Val loss by epoch
5: 295.854 / 1860.311
10: 156.732 / 871.091
15: 118.285 / 741.566
20: 145.300 / 645.474
25: 119.818 / 553.749
30: 158.933 / 527.620
35: 71.475 / 463.800
40: 89.320 / 455.726
45: 88.913 / 420.340
50: 111.304 / 392.902
55: 101.014 / 355.400
60: 61.813 / 336.315
65: 71.609 / 317.436
70: 61.501 / 278.134
75: 43.531 / 276.397
80: 42.011 / 303.586
85: 57.267 / 297.918
90: 41.931 / 275.343
95: 69.895 / 302.089
100: 59.928 / 310.652
105: 51.434 / 314.470
110: 61.952 / 320.105
115: 54.428 / 286.573
120: 50.898 / 319.992
125: 44.192 / 329.723
130: 49.293 / 346.431
135: 37.835 / 369.856
140: 49.639 / 362.558
145: 37.005 / 385.243
150: 37.220 / 397.858
155: 30.364 / 371.141
160: 24.101 / 423.338
165: 41.284 / 424.223
170: 39.845 / 436.953
175: 25.491 / 423.254
180: 31.357 / 416.890
185: 28.816 / 437.662
190: 30.225 / 466.173
195: 40.630 / 434.159
200: 26.932 / 484.356
205: 29.511 / 475.297
210: 30.991 / 474.474
215: 26.317 / 469.663
220: 37.030 / 479.080
225: 28.321 / 524.593
230: 41.375 / 508.238
235: 33.599 / 535.534
240: 43.363 / 553.512
245: 26.026 / 558.408
250: 24.681 / 562.236
255: 32.232 / 560.136
260: 30.815 / 544.904
265: 25.597 / 561.407
270: 42.442 / 597.679
275: 40.253 / 622.982
280: 43.399 / 591.432
285: 51.430 / 599.666
290: 29.999 / 639.277
295: 29.122 / 615.100
300: 26.974 / 656.793
305: 33.228 / 669.585
310: 20.348 / 731.946
315: 27.000 / 714.333
320: 32.791 / 690.228
325: 33.724 / 689.492
330: 27.611 / 707.037
335: 43.045 / 705.153
340: 20.741 / 715.203
345: 32.890 / 752.492
350: 40.478 / 763.915
355: 23.861 / 806.565
360: 24.555 / 788.934
365: 28.062 / 818.350
370: 28.215 / 804.690
375: 23.535 / 804.346
380: 35.110 / 851.190
385: 20.734 / 800.137
390: 27.194 / 881.624
395: 35.237 / 831.185
400: 29.312 / 839.796
{'cayley': [{'best': 535.8186096191406, 'end': 995.9110473632812},
            {'best': 816.1120239257813, 'end': 1834.2987670898438},
            {'best': 275.3428070068359, 'end': 839.7964294433593}],
 'cayley_clusters': [{'best': 900.43740234375, 'end': 1791.3495239257813},
                     {'best': 920.3936645507813, 'end': 2045.5480712890626},
                     {'best': 730.4976531982422, 'end': 1560.9613647460938}]}
Loading data...
Number of training graphs: 1500
Train graph sizes: Counter({101: 46, 114: 45, 79: 44, 87: 40, 94: 39, 122: 39, 88: 37, 92: 37, 107: 36, 119: 36, 80: 34, 105: 34, 75: 33, 99: 33, 112: 33, 116: 33, 121: 33, 78: 32, 84: 32, 98: 32, 100: 32, 120: 32, 96: 31, 111: 31, 118: 31, 81: 29, 83: 28, 95: 28, 103: 28, 108: 28, 82: 27, 89: 27, 106: 27, 93: 26, 90: 26, 97: 26, 109: 25, 115: 25, 117: 25, 85: 24, 124: 24, 76: 22, 86: 22, 91: 22, 102: 22, 104: 22, 110: 22, 123: 22, 77: 19, 113: 19})
{25: [], 26: [], 27: [], 28: [], 29: [], 30: [], 31: [], 32: [], 33: [], 34: []}
Number of validation graphs: 1500
Val graph sizes: Counter({159: 12, 168: 11, 172: 11, 130: 9, 164: 9, 174: 9, 126: 8, 142: 8, 141: 8, 149: 8, 125: 7, 129: 7, 133: 7, 134: 7, 139: 7, 154: 7, 153: 7, 160: 7, 128: 6, 135: 6, 137: 6, 136: 6, 143: 6, 146: 6, 147: 6, 152: 6, 150: 6, 158: 6, 157: 6, 165: 6, 138: 5, 144: 5, 148: 5, 145: 5, 156: 5, 163: 5, 162: 5, 166: 5, 132: 4, 151: 4, 161: 4, 169: 4, 167: 4, 171: 4, 173: 4, 131: 3, 140: 3, 127: 2, 170: 2, 155: 1})
{'wandb': {'experiment_name': '', 'project': 'ColourInteract Sweeps Friday', 'entity': 'commute_opt_gnn'}, 'data': {'name': 'ColourInteract', 'dataset': 'LRGB', 'c1': 1.0, 'c2s': [0.01, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 100.0], 'num_colours': 4, 'train_size': 1500, 'val_size': 300, 'min_train_nodes': 75, 'max_train_nodes': 125, 'min_val_nodes': 125, 'max_val_nodes': 175, 'rewirers': ['cayley_clusters', 'cayley'], 'normalise': True, 'seed': 93, 'c2': 10.0}, 'model': {'hidden_channels': 8, 'num_layers': 5, 'drop_prob': 0.1, 'global_pool_aggr': 'global_mean_pool', 'approaches': ['only_diff'], 'seeds': [17, 47, 23], 'approach': 'only_diff', 'seed': 23}, 'train': {'lr': 0.0001, 'num_epochs': 400, 'print_every': 5, 'train_batch_size': 32, 'val_batch_size': 32}, 'run': {'silent': False}}
Device: cpu
train targets: 367.16 +/- 57.416
val targets: 551.23 +/- 58.596
Rewirer cayley_clusters with seed 17
Using MSE Loss...
Minimum val loss at epoch 90: 4773.51845703125
Final val loss at epoch 400: 10565.7568359375
Train / Val loss by epoch
5: 136027.859 / 227847.812
10: 37209.152 / 82234.671
15: 3519.817 / 21043.297
20: 402.290 / 8550.563
25: 508.389 / 6417.932
30: 364.381 / 6082.945
35: 464.736 / 5871.208
40: 178.478 / 5757.237
45: 342.558 / 5418.673
50: 391.152 / 5388.157
55: 281.802 / 5150.161
60: 268.180 / 5126.320
65: 333.544 / 5038.336
70: 344.498 / 4880.439
75: 272.850 / 4915.850
80: 437.197 / 4972.735
85: 237.704 / 4973.418
90: 269.645 / 4773.518
95: 170.482 / 5000.507
100: 248.936 / 5004.219
105: 279.587 / 4925.849
110: 269.374 / 4788.405
115: 122.245 / 4901.406
120: 194.587 / 5078.808
125: 237.507 / 5028.608
130: 175.026 / 5038.238
135: 196.687 / 4980.337
140: 153.921 / 4905.495
145: 170.865 / 5327.032
150: 145.167 / 5403.386
155: 235.719 / 5470.782
160: 173.653 / 5843.578
165: 197.640 / 5741.901
170: 172.082 / 6044.811
175: 98.738 / 5861.171
180: 104.775 / 6006.634
185: 165.483 / 6179.595
190: 152.080 / 6238.207
195: 194.847 / 6719.920
200: 122.314 / 6900.846
205: 137.125 / 7102.280
210: 144.956 / 7001.464
215: 192.164 / 7160.552
220: 99.571 / 7206.665
225: 162.278 / 7733.053
230: 157.943 / 7695.687
235: 174.161 / 7858.430
240: 184.289 / 7660.360
245: 77.290 / 7971.178
250: 123.430 / 7999.185
255: 276.926 / 7906.093
260: 131.791 / 8038.706
265: 120.340 / 8554.611
270: 221.709 / 8467.131
275: 170.312 / 8936.878
280: 120.348 / 8592.577
285: 106.402 / 8752.847
290: 79.020 / 8829.292
295: 183.842 / 9350.218
300: 153.049 / 9399.288
305: 128.461 / 9008.187
310: 200.373 / 9469.674
315: 115.603 / 9052.295
320: 118.040 / 9547.495
325: 125.301 / 9869.215
330: 170.999 / 9769.901
335: 174.366 / 9888.420
340: 82.640 / 9837.470
345: 110.423 / 9851.571
350: 134.869 / 10258.062
355: 112.070 / 10287.531
360: 143.191 / 10323.463
365: 103.216 / 10374.114
370: 113.675 / 10318.703
375: 136.272 / 10664.346
380: 168.856 / 10692.541
385: 102.652 / 10671.479
390: 94.304 / 10605.392
395: 211.604 / 10844.013
400: 215.448 / 10565.757
Rewirer cayley_clusters with seed 47
Using MSE Loss...
Minimum val loss at epoch 100: 5859.895532226563
Final val loss at epoch 400: 15185.27607421875
Train / Val loss by epoch
5: 49762.848 / 94975.669
10: 6282.921 / 23264.549
15: 712.891 / 7728.487
20: 450.628 / 5985.314
25: 321.058 / 5932.623
30: 291.527 / 5954.691
35: 345.108 / 5991.080
40: 324.973 / 6137.341
45: 252.087 / 6092.760
50: 169.814 / 6122.475
55: 237.018 / 6022.673
60: 288.792 / 6084.835
65: 246.635 / 6145.114
70: 192.538 / 6058.418
75: 164.228 / 6078.640
80: 161.924 / 6061.204
85: 228.976 / 5892.898
90: 218.906 / 5892.507
95: 195.600 / 5873.705
100: 217.101 / 5859.896
105: 226.144 / 6188.807
110: 194.681 / 6288.108
115: 177.370 / 6398.348
120: 171.152 / 6622.905
125: 147.433 / 6567.164
130: 184.502 / 6626.914
135: 157.165 / 6626.661
140: 127.536 / 6755.861
145: 185.101 / 6822.833
150: 189.504 / 6843.245
155: 196.051 / 7484.086
160: 144.207 / 7379.126
165: 202.692 / 7120.377
170: 106.094 / 7529.005
175: 140.417 / 8124.255
180: 121.762 / 8085.689
185: 128.815 / 8176.247
190: 197.078 / 8476.576
195: 169.582 / 8592.347
200: 84.425 / 8635.454
205: 172.406 / 9003.394
210: 120.688 / 9369.610
215: 150.817 / 9525.688
220: 163.051 / 9493.181
225: 113.069 / 10094.048
230: 141.470 / 10545.152
235: 114.966 / 10173.715
240: 146.706 / 10584.918
245: 197.869 / 10491.954
250: 166.889 / 10807.904
255: 185.773 / 11044.518
260: 120.609 / 11616.528
265: 154.174 / 10875.812
270: 151.577 / 11354.436
275: 131.488 / 11578.778
280: 124.304 / 11306.232
285: 131.430 / 11636.338
290: 184.900 / 11997.762
295: 145.336 / 11924.940
300: 129.403 / 12261.554
305: 138.937 / 12598.332
310: 109.436 / 12525.472
315: 111.303 / 12884.137
320: 115.278 / 12766.648
325: 138.861 / 12970.270
330: 151.249 / 12919.720
335: 169.955 / 13176.201
340: 150.824 / 12712.237
345: 136.015 / 13854.647
350: 136.561 / 13678.110
355: 127.537 / 13439.096
360: 193.941 / 13917.833
365: 108.260 / 13478.315
370: 120.675 / 13952.675
375: 157.007 / 13716.053
380: 160.112 / 14099.839
385: 133.832 / 14424.175
390: 66.648 / 14721.133
395: 134.635 / 14399.678
400: 135.019 / 15185.276
Rewirer cayley_clusters with seed 23
Using MSE Loss...
Minimum val loss at epoch 35: 5271.642846679688
Final val loss at epoch 400: 9867.16982421875
Train / Val loss by epoch
5: 27849.877 / 60764.356
10: 1620.243 / 11419.700
15: 402.025 / 5789.557
20: 347.661 / 5352.668
25: 479.665 / 5422.454
30: 448.444 / 5320.261
35: 264.798 / 5271.643
40: 352.699 / 5378.261
45: 243.288 / 5313.379
50: 378.333 / 5274.585
55: 320.441 / 5444.508
60: 156.236 / 5491.860
65: 235.494 / 5411.957
70: 221.804 / 5312.759
75: 125.833 / 5409.625
80: 249.250 / 5512.631
85: 157.437 / 5632.595
90: 210.843 / 5454.646
95: 201.327 / 5719.932
100: 191.112 / 5775.116
105: 188.669 / 5872.384
110: 169.184 / 6044.275
115: 140.758 / 5834.973
120: 162.320 / 5904.614
125: 128.272 / 6333.016
130: 162.776 / 6197.675
135: 115.793 / 6307.484
140: 147.646 / 6267.871
145: 177.544 / 6364.391
150: 138.147 / 6605.351
155: 139.105 / 6240.678
160: 102.483 / 6362.602
165: 121.276 / 6701.235
170: 123.570 / 6570.752
175: 81.938 / 6537.090
180: 96.124 / 6585.880
185: 112.478 / 6734.752
190: 135.686 / 7244.653
195: 118.555 / 6911.412
200: 122.711 / 7228.353
205: 117.198 / 7328.708
210: 105.792 / 7287.648
215: 147.767 / 7171.937
220: 215.976 / 7451.063
225: 104.455 / 7554.550
230: 109.055 / 7403.940
235: 126.783 / 7401.276
240: 142.280 / 7776.376
245: 148.206 / 7693.113
250: 120.491 / 7770.056
255: 121.836 / 7702.952
260: 102.090 / 7523.898
265: 107.506 / 7702.723
270: 159.446 / 8057.971
275: 147.430 / 8216.779
280: 131.330 / 8282.365
285: 169.883 / 8431.322
290: 149.917 / 8387.758
295: 118.768 / 8321.564
300: 105.425 / 8473.881
305: 145.747 / 8540.803
310: 77.262 / 8861.214
315: 103.370 / 8727.000
320: 126.773 / 9141.849
325: 115.823 / 8733.647
330: 85.486 / 8848.998
335: 164.161 / 8826.356
340: 78.205 / 9024.116
345: 117.393 / 8892.880
350: 113.225 / 9185.194
355: 94.761 / 9413.600
360: 98.172 / 9390.292
365: 98.369 / 9386.504
370: 120.995 / 9279.243
375: 94.222 / 9377.781
380: 101.340 / 9745.106
385: 89.853 / 9298.877
390: 104.418 / 9755.368
395: 128.276 / 9593.326
400: 126.603 / 9867.170
Rewirer cayley with seed 17
Using MSE Loss...
Minimum val loss at epoch 115: 2941.087048339844
Final val loss at epoch 400: 7363.403173828125
Train / Val loss by epoch
5: 134683.250 / 224429.616
10: 36564.840 / 79937.714
15: 3592.379 / 18400.632
20: 477.097 / 6413.393
25: 450.859 / 4752.497
30: 420.059 / 4390.986
35: 453.920 / 4436.466
40: 393.468 / 4513.312
45: 445.968 / 4250.421
50: 335.457 / 4199.593
55: 339.474 / 3956.205
60: 311.266 / 3825.623
65: 357.808 / 3629.420
70: 300.425 / 3423.646
75: 230.739 / 3298.038
80: 341.390 / 3377.795
85: 226.213 / 3337.827
90: 195.098 / 3281.720
95: 200.313 / 3294.523
100: 225.188 / 3236.222
105: 226.547 / 3198.877
110: 151.694 / 3119.403
115: 132.341 / 2941.087
120: 131.534 / 3017.081
125: 252.260 / 3074.361
130: 220.960 / 3046.379
135: 150.501 / 3038.031
140: 114.072 / 3121.494
145: 230.762 / 3173.856
150: 252.820 / 3126.484
155: 128.402 / 3245.844
160: 179.111 / 3350.521
165: 203.690 / 3285.545
170: 128.686 / 3540.728
175: 104.466 / 3260.666
180: 126.009 / 3527.345
185: 147.734 / 3556.298
190: 160.471 / 3557.966
195: 161.552 / 3856.068
200: 186.528 / 3844.170
205: 145.440 / 3998.990
210: 121.063 / 4072.296
215: 103.987 / 4258.422
220: 135.296 / 4156.544
225: 150.576 / 4608.904
230: 146.925 / 4482.823
235: 166.967 / 4609.057
240: 179.819 / 4545.933
245: 118.212 / 4770.911
250: 132.746 / 4795.354
255: 248.635 / 4883.043
260: 130.695 / 5009.013
265: 132.755 / 5066.272
270: 147.927 / 5042.544
275: 163.570 / 5284.663
280: 109.789 / 5436.165
285: 104.989 / 5633.916
290: 81.525 / 5633.535
295: 155.599 / 6039.316
300: 167.106 / 6050.096
305: 135.931 / 5840.728
310: 160.202 / 6120.970
315: 130.666 / 5889.413
320: 94.020 / 5920.555
325: 64.762 / 6607.961
330: 118.303 / 6552.876
335: 183.981 / 6384.125
340: 106.266 / 6539.190
345: 138.948 / 6604.225
350: 135.381 / 6698.737
355: 122.182 / 6632.085
360: 152.526 / 6879.557
365: 139.239 / 7115.503
370: 140.028 / 6954.757
375: 88.954 / 6975.796
380: 164.981 / 7091.414
385: 88.073 / 7332.322
390: 123.408 / 7345.311
395: 192.668 / 7435.448
400: 200.857 / 7363.403
Rewirer cayley with seed 47
Using MSE Loss...
Minimum val loss at epoch 20: 4486.556787109375
Final val loss at epoch 400: 13244.42509765625
Train / Val loss by epoch
5: 49885.945 / 93120.961
10: 6418.274 / 20245.620
15: 878.082 / 5924.736
20: 573.390 / 4486.557
25: 436.508 / 4681.890
30: 337.616 / 4894.559
35: 410.627 / 5387.180
40: 422.841 / 6086.072
45: 273.118 / 6641.364
50: 154.790 / 7114.189
55: 240.491 / 7289.430
60: 323.003 / 7536.062
65: 255.816 / 7507.517
70: 182.989 / 7323.975
75: 194.877 / 7401.181
80: 209.256 / 7397.231
85: 243.254 / 7027.891
90: 296.256 / 6877.350
95: 201.827 / 6703.123
100: 187.147 / 6748.239
105: 334.090 / 6786.344
110: 146.130 / 6702.855
115: 133.433 / 6524.494
120: 183.763 / 6649.340
125: 126.962 / 6477.362
130: 209.392 / 6409.876
135: 169.968 / 6552.263
140: 185.904 / 6486.038
145: 193.096 / 6435.761
150: 170.962 / 6503.596
155: 227.742 / 6820.369
160: 137.921 / 6856.873
165: 148.827 / 6605.941
170: 91.171 / 6962.953
175: 128.106 / 7224.638
180: 124.492 / 7276.670
185: 118.700 / 7387.308
190: 194.674 / 7858.076
195: 196.297 / 7722.253
200: 115.882 / 7739.577
205: 172.305 / 8141.673
210: 141.709 / 8236.372
215: 160.737 / 8514.164
220: 235.330 / 8523.434
225: 153.489 / 8885.197
230: 143.579 / 9343.504
235: 125.109 / 9093.340
240: 131.545 / 9252.887
245: 191.768 / 9054.169
250: 153.238 / 9608.743
255: 211.364 / 9716.555
260: 142.856 / 10134.971
265: 149.987 / 9879.039
270: 179.109 / 10107.611
275: 171.675 / 10410.309
280: 130.162 / 9919.904
285: 122.843 / 10354.136
290: 168.499 / 10794.119
295: 135.826 / 10469.514
300: 149.392 / 10922.668
305: 144.037 / 10965.226
310: 84.023 / 11029.433
315: 161.583 / 11272.384
320: 157.232 / 11138.513
325: 98.777 / 11787.259
330: 181.034 / 11464.752
335: 162.447 / 11802.765
340: 162.262 / 11715.964
345: 133.846 / 12268.526
350: 114.071 / 12207.135
355: 114.258 / 12154.999
360: 229.711 / 12615.663
365: 104.531 / 12368.242
370: 132.915 / 12659.314
375: 151.967 / 12415.311
380: 104.820 / 12645.807
385: 121.768 / 12805.595
390: 80.145 / 12843.238
395: 127.073 / 12659.893
400: 122.465 / 13244.425
Rewirer cayley with seed 23
Using MSE Loss...
Minimum val loss at epoch 25: 2736.0415893554687
Final val loss at epoch 400: 6240.35927734375
Train / Val loss by epoch
5: 28085.699 / 59320.823
10: 1568.833 / 8398.191
15: 511.902 / 3302.758
20: 489.989 / 2833.589
25: 484.309 / 2736.042
30: 478.915 / 2758.948
35: 314.750 / 2737.411
40: 330.489 / 2932.037
45: 283.834 / 2992.764
50: 412.586 / 3002.419
55: 436.651 / 3126.866
60: 178.382 / 3039.958
65: 184.559 / 3035.945
70: 284.839 / 3040.165
75: 185.556 / 2945.496
80: 219.604 / 3108.363
85: 215.342 / 3116.676
90: 160.793 / 3085.259
95: 253.190 / 3217.644
100: 267.083 / 3371.455
105: 145.988 / 3295.650
110: 174.308 / 3388.795
115: 178.315 / 3195.381
120: 162.095 / 3516.062
125: 148.366 / 3495.516
130: 167.115 / 3741.678
135: 118.174 / 3902.026
140: 171.963 / 3822.041
145: 142.510 / 3841.060
150: 147.527 / 3916.660
155: 133.757 / 3786.292
160: 107.053 / 3869.469
165: 137.336 / 4081.265
170: 138.284 / 4089.997
175: 87.351 / 3952.165
180: 105.863 / 3944.446
185: 104.971 / 3993.560
190: 109.060 / 4296.395
195: 114.648 / 3957.548
200: 127.748 / 4195.899
205: 133.552 / 4222.583
210: 117.937 / 4343.727
215: 145.040 / 4241.528
220: 203.832 / 4483.864
225: 130.922 / 4594.926
230: 145.781 / 4493.564
235: 106.185 / 4509.732
240: 140.114 / 4660.706
245: 126.712 / 4673.978
250: 125.732 / 4701.246
255: 123.228 / 4735.892
260: 96.429 / 4577.398
265: 95.055 / 4692.763
270: 152.569 / 4970.923
275: 182.581 / 5047.014
280: 144.744 / 5008.160
285: 160.484 / 5094.252
290: 124.531 / 4986.181
295: 108.398 / 5104.892
300: 123.535 / 5198.703
305: 116.213 / 5201.698
310: 85.541 / 5541.552
315: 111.105 / 5555.273
320: 135.120 / 5593.034
325: 123.786 / 5430.978
330: 108.075 / 5561.467
335: 185.047 / 5659.592
340: 88.008 / 5715.943
345: 117.435 / 5705.977
350: 123.775 / 5769.853
355: 96.166 / 5918.938
360: 103.197 / 5970.977
365: 100.151 / 5891.127
370: 116.719 / 5819.031
375: 119.130 / 5826.602
380: 138.329 / 6311.366
385: 78.676 / 5887.496
390: 81.325 / 6228.825
395: 124.875 / 6089.663
400: 115.344 / 6240.359
{'cayley': [{'best': 2941.087048339844, 'end': 7363.403173828125},
            {'best': 4486.556787109375, 'end': 13244.42509765625},
            {'best': 2736.0415893554687, 'end': 6240.35927734375}],
 'cayley_clusters': [{'best': 4773.51845703125, 'end': 10565.7568359375},
                     {'best': 5859.895532226563, 'end': 15185.27607421875},
                     {'best': 5271.642846679688, 'end': 9867.16982421875}]}
Loading data...
Number of training graphs: 1500
Train graph sizes: Counter({101: 46, 114: 45, 79: 44, 87: 40, 94: 39, 122: 39, 88: 37, 92: 37, 107: 36, 119: 36, 80: 34, 105: 34, 75: 33, 99: 33, 112: 33, 116: 33, 121: 33, 78: 32, 84: 32, 98: 32, 100: 32, 120: 32, 96: 31, 111: 31, 118: 31, 81: 29, 83: 28, 95: 28, 103: 28, 108: 28, 82: 27, 89: 27, 106: 27, 93: 26, 90: 26, 97: 26, 109: 25, 115: 25, 117: 25, 85: 24, 124: 24, 76: 22, 86: 22, 91: 22, 102: 22, 104: 22, 110: 22, 123: 22, 77: 19, 113: 19})
{25: [], 26: [], 27: [], 28: [], 29: [], 30: [], 31: [], 32: [], 33: [], 34: []}
Number of validation graphs: 1500
Val graph sizes: Counter({159: 12, 168: 11, 172: 11, 130: 9, 164: 9, 174: 9, 126: 8, 142: 8, 141: 8, 149: 8, 125: 7, 129: 7, 133: 7, 134: 7, 139: 7, 154: 7, 153: 7, 160: 7, 128: 6, 135: 6, 137: 6, 136: 6, 143: 6, 146: 6, 147: 6, 152: 6, 150: 6, 158: 6, 157: 6, 165: 6, 138: 5, 144: 5, 148: 5, 145: 5, 156: 5, 163: 5, 162: 5, 166: 5, 132: 4, 151: 4, 161: 4, 169: 4, 167: 4, 171: 4, 173: 4, 131: 3, 140: 3, 127: 2, 170: 2, 155: 1})
{'wandb': {'experiment_name': '', 'project': 'ColourInteract Sweeps Friday', 'entity': 'commute_opt_gnn'}, 'data': {'name': 'ColourInteract', 'dataset': 'LRGB', 'c1': 1.0, 'c2s': [0.01, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 100.0], 'num_colours': 4, 'train_size': 1500, 'val_size': 300, 'min_train_nodes': 75, 'max_train_nodes': 125, 'min_val_nodes': 125, 'max_val_nodes': 175, 'rewirers': ['cayley_clusters', 'cayley'], 'normalise': True, 'seed': 93, 'c2': 100.0}, 'model': {'hidden_channels': 8, 'num_layers': 5, 'drop_prob': 0.1, 'global_pool_aggr': 'global_mean_pool', 'approaches': ['only_diff'], 'seeds': [17, 47, 23], 'approach': 'only_diff', 'seed': 23}, 'train': {'lr': 0.0001, 'num_epochs': 400, 'print_every': 5, 'train_batch_size': 32, 'val_batch_size': 32}, 'run': {'silent': False}}
Device: cpu
train targets: 3644.47 +/- 573.695
val targets: 5485.26 +/- 585.355
Rewirer cayley_clusters with seed 17
Using MSE Loss...
Minimum val loss at epoch 80: 215800.32890625
Final val loss at epoch 400: 1317288.3375
Train / Val loss by epoch
5: 19132876.000 / 29983967.200
10: 16938508.000 / 27034635.800
15: 13953186.000 / 23126014.300
20: 10483035.000 / 18325875.300
25: 7010264.500 / 13035548.100
30: 3922228.750 / 8007262.800
35: 1847302.000 / 4281433.775
40: 611532.000 / 2013110.363
45: 194375.031 / 909563.881
50: 65351.324 / 469378.031
55: 50117.594 / 316345.930
60: 30902.703 / 255216.588
65: 49885.695 / 238555.970
70: 44950.883 / 223668.777
75: 36118.652 / 218058.188
80: 45751.168 / 215800.329
85: 28003.750 / 224303.765
90: 23969.479 / 231711.591
95: 21931.811 / 257581.131
100: 20241.707 / 280489.430
105: 23257.076 / 270307.138
110: 22482.701 / 292422.792
115: 15591.682 / 310682.064
120: 22293.385 / 325588.416
125: 22585.467 / 308589.341
130: 16641.920 / 336312.991
135: 15644.272 / 358429.458
140: 15633.680 / 378806.520
145: 15944.778 / 389715.050
150: 20475.826 / 413228.209
155: 21724.846 / 419465.713
160: 15339.915 / 446616.722
165: 20068.871 / 480784.769
170: 15669.440 / 502843.056
175: 9202.515 / 509133.825
180: 10196.390 / 514053.044
185: 15943.005 / 538204.363
190: 13884.294 / 564517.762
195: 19672.043 / 602689.278
200: 11216.253 / 612447.691
205: 16701.691 / 631642.550
210: 16923.275 / 634727.497
215: 17826.338 / 665456.084
220: 16870.523 / 672900.297
225: 13773.675 / 712591.991
230: 14684.461 / 703831.856
235: 15079.419 / 739448.113
240: 13658.616 / 762203.338
245: 11693.023 / 767872.381
250: 10935.077 / 820497.644
255: 21843.271 / 806033.053
260: 10358.289 / 857055.781
265: 14229.780 / 859931.475
270: 15875.906 / 861925.069
275: 19024.111 / 889742.050
280: 9624.896 / 901022.738
285: 14485.876 / 936432.525
290: 12252.183 / 936426.175
295: 20122.021 / 992669.731
300: 20247.516 / 1050409.306
305: 10217.748 / 991878.056
310: 17487.176 / 1002276.469
315: 9535.259 / 1006766.444
320: 10666.214 / 1042025.531
325: 9032.453 / 1131339.306
330: 16725.818 / 1141083.456
335: 18997.205 / 1158779.125
340: 9611.394 / 1149394.631
345: 10056.140 / 1159734.281
350: 13607.533 / 1220299.244
355: 10282.607 / 1217715.863
360: 11459.836 / 1238687.756
365: 9031.565 / 1224500.925
370: 10773.780 / 1231180.556
375: 11450.841 / 1271427.400
380: 15270.187 / 1262325.775
385: 12598.349 / 1281757.312
390: 11187.685 / 1289936.669
395: 15232.320 / 1352453.613
400: 14674.125 / 1317288.337
Rewirer cayley_clusters with seed 47
Using MSE Loss...
Minimum val loss at epoch 75: 634229.5375
Final val loss at epoch 400: 2232166.55
Train / Val loss by epoch
5: 17814552.000 / 28082765.000
10: 15588718.000 / 25113338.800
15: 12652158.000 / 21180740.700
20: 9196514.000 / 16255253.300
25: 5802945.500 / 11248955.800
30: 3054886.250 / 7026621.050
35: 1235340.375 / 4030286.550
40: 448383.031 / 2221813.100
45: 123609.539 / 1311223.344
50: 50260.258 / 923789.225
55: 45962.434 / 761881.350
60: 50632.191 / 690062.512
65: 44086.277 / 651622.166
70: 28854.848 / 637258.328
75: 35383.918 / 634229.537
80: 33528.316 / 649738.100
85: 21789.771 / 678115.787
90: 19439.078 / 683160.838
95: 18938.152 / 708823.200
100: 16034.677 / 751770.088
105: 17914.553 / 796624.481
110: 12944.469 / 817449.894
115: 18268.609 / 861391.238
120: 9780.667 / 873581.512
125: 14446.791 / 906979.406
130: 14003.323 / 917601.319
135: 9848.334 / 946840.125
140: 9300.044 / 979385.863
145: 13859.702 / 1005745.675
150: 12603.819 / 1029365.812
155: 17880.467 / 1112834.500
160: 12967.794 / 1088703.688
165: 13126.240 / 1114817.894
170: 6279.505 / 1179876.419
175: 12265.471 / 1219131.069
180: 6147.711 / 1289697.913
185: 9797.890 / 1303006.444
190: 14957.925 / 1303651.238
195: 17950.383 / 1326187.562
200: 10323.549 / 1373987.831
205: 14633.862 / 1427304.975
210: 10662.522 / 1434241.387
215: 7310.852 / 1513061.700
220: 15830.055 / 1562557.425
225: 10465.267 / 1575109.419
230: 8248.188 / 1577674.856
235: 12651.159 / 1578894.825
240: 11215.841 / 1632281.775
245: 10674.014 / 1676712.500
250: 12820.708 / 1641881.587
255: 14873.357 / 1656243.062
260: 12864.634 / 1740823.950
265: 9759.367 / 1737659.538
270: 11537.711 / 1729787.075
275: 10416.048 / 1787380.438
280: 12069.658 / 1789313.100
285: 13994.823 / 1843533.050
290: 17772.336 / 1838268.175
295: 9008.556 / 1853179.625
300: 8972.876 / 1892046.350
305: 11898.083 / 1926996.025
310: 10474.868 / 1903743.750
315: 7503.398 / 1938672.550
320: 12276.969 / 1968130.700
325: 9917.474 / 1989991.837
330: 12698.811 / 1956105.762
335: 13180.970 / 2041152.900
340: 8060.571 / 1990634.950
345: 9225.689 / 2051418.850
350: 12288.732 / 2067075.188
355: 9273.101 / 2054198.500
360: 13557.803 / 2079398.725
365: 14265.748 / 2096112.825
370: 13054.804 / 2117177.913
375: 10458.183 / 2100671.475
380: 12138.964 / 2120507.663
385: 10078.749 / 2156225.862
390: 8370.424 / 2174672.462
395: 12246.033 / 2186573.888
400: 12892.749 / 2232166.550
Rewirer cayley_clusters with seed 23
Using MSE Loss...
Minimum val loss at epoch 80: 740418.871875
Final val loss at epoch 400: 2113584.7625
Train / Val loss by epoch
5: 17180416.000 / 27204007.000
10: 14418399.000 / 23553703.600
15: 11120515.000 / 19272361.000
20: 7750661.000 / 14608869.700
25: 4588489.500 / 10144534.250
30: 2296400.500 / 6270661.625
35: 903157.688 / 3507576.550
40: 292595.562 / 1961782.863
45: 85280.539 / 1197100.137
50: 63833.277 / 895493.016
55: 53602.445 / 799558.522
60: 24743.438 / 777180.841
65: 29062.709 / 761634.241
70: 25664.375 / 749818.672
75: 24305.113 / 757681.213
80: 25106.529 / 740418.872
85: 22405.240 / 746127.953
90: 32583.139 / 741806.231
95: 14437.802 / 760604.938
100: 11339.998 / 764025.825
105: 16714.066 / 767213.412
110: 16055.055 / 835121.409
115: 9910.180 / 847252.250
120: 15385.649 / 862106.775
125: 14859.245 / 894112.438
130: 14326.938 / 960874.975
135: 15187.155 / 981243.919
140: 13224.901 / 1023128.681
145: 14550.951 / 1071479.969
150: 15793.142 / 1120431.556
155: 16480.309 / 1149512.150
160: 11005.896 / 1172632.606
165: 10601.354 / 1223429.300
170: 12985.757 / 1235397.975
175: 10231.969 / 1292657.637
180: 9419.091 / 1303118.231
185: 9927.933 / 1340699.850
190: 18168.129 / 1417166.225
195: 10634.892 / 1431262.775
200: 12328.186 / 1444247.556
205: 10631.005 / 1454245.387
210: 14506.938 / 1472863.762
215: 14596.944 / 1565788.969
220: 17689.654 / 1597686.019
225: 11598.208 / 1534324.781
230: 10883.172 / 1618123.062
235: 14958.401 / 1608410.481
240: 14652.417 / 1593776.438
245: 13362.261 / 1596249.663
250: 13095.417 / 1671222.950
255: 15692.006 / 1678540.925
260: 9221.276 / 1683807.950
265: 11658.888 / 1692502.238
270: 13339.306 / 1756748.625
275: 13617.585 / 1748558.800
280: 11356.463 / 1816554.387
285: 11960.031 / 1896280.462
290: 13130.687 / 1851639.225
295: 15330.686 / 1813879.575
300: 10376.268 / 1835602.200
305: 13174.523 / 1846335.712
310: 8874.490 / 1854194.850
315: 11855.831 / 1802239.188
320: 10918.376 / 1948160.913
325: 9315.144 / 1918469.650
330: 8697.519 / 1908632.988
335: 17605.295 / 1850996.125
340: 7552.112 / 1929759.050
345: 9758.649 / 1914844.250
350: 11754.966 / 1926704.425
355: 11396.713 / 1981388.387
360: 10309.622 / 2015518.613
365: 9553.210 / 1979303.363
370: 9366.312 / 2000067.925
375: 10273.034 / 1976355.988
380: 9486.492 / 2049999.613
385: 10460.540 / 1962905.962
390: 7447.741 / 2038420.413
395: 9222.398 / 2007062.488
400: 13309.064 / 2113584.763
Rewirer cayley with seed 17
Using MSE Loss...
Minimum val loss at epoch 85: 98782.49765625
Final val loss at epoch 400: 745283.8375
Train / Val loss by epoch
5: 19116896.000 / 29946958.600
10: 16914542.000 / 26956733.000
15: 13920007.000 / 22712216.000
20: 10380981.000 / 17249316.700
25: 6849245.000 / 11600496.550
30: 3793922.250 / 6747484.700
35: 1749245.000 / 3304054.650
40: 582849.312 / 1353971.238
45: 171163.766 / 503255.684
50: 79276.312 / 226908.342
55: 53122.031 / 143404.709
60: 46855.820 / 120430.349
65: 47904.586 / 110694.491
70: 41096.980 / 104046.850
75: 44247.992 / 100029.034
80: 39297.055 / 98906.323
85: 17478.350 / 98782.498
90: 29451.881 / 101261.777
95: 30874.660 / 99701.977
100: 28574.217 / 102094.150
105: 21186.678 / 100194.181
110: 17018.154 / 107129.899
115: 18881.189 / 110510.886
120: 15546.593 / 109581.438
125: 22010.340 / 108973.718
130: 21137.842 / 116303.950
135: 20369.410 / 116535.022
140: 19440.623 / 117494.759
145: 24184.412 / 123200.482
150: 26305.875 / 128117.716
155: 15609.897 / 135000.682
160: 13477.698 / 134309.473
165: 22952.111 / 135733.048
170: 14930.561 / 154742.220
175: 12378.634 / 165319.560
180: 17307.510 / 175377.453
185: 14318.469 / 195284.764
190: 16527.068 / 198100.202
195: 15178.352 / 214611.733
200: 13651.489 / 219527.520
205: 15029.486 / 251458.081
210: 12387.739 / 250012.913
215: 17258.166 / 280423.253
220: 16838.043 / 285460.633
225: 13666.694 / 316505.844
230: 15000.533 / 318825.261
235: 18737.951 / 325588.959
240: 21213.293 / 341915.030
245: 11608.292 / 374735.791
250: 14478.084 / 385974.422
255: 18615.215 / 396698.678
260: 13866.398 / 416661.119
265: 14793.883 / 419687.303
270: 17106.459 / 423902.906
275: 14229.266 / 448772.622
280: 9800.654 / 464654.653
285: 14052.300 / 474094.119
290: 9914.963 / 500215.184
295: 19315.500 / 523105.109
300: 18142.012 / 529823.100
305: 14739.937 / 507543.812
310: 15228.706 / 517989.550
315: 13164.117 / 525112.916
320: 9592.177 / 522237.013
325: 7633.646 / 609007.656
330: 13615.594 / 585499.697
335: 19080.510 / 588756.075
340: 13663.055 / 617090.363
345: 11895.023 / 631350.534
350: 16135.416 / 647256.091
355: 10094.159 / 643109.906
360: 11589.756 / 657250.825
365: 14372.603 / 709469.775
370: 18421.967 / 679114.469
375: 11313.026 / 690818.000
380: 17993.385 / 697957.256
385: 13582.606 / 706141.822
390: 13111.556 / 743192.488
395: 13828.155 / 746735.944
400: 22936.623 / 745283.838
Rewirer cayley with seed 47
Using MSE Loss...
Minimum val loss at epoch 60: 611296.053125
Final val loss at epoch 400: 2087311.7
Train / Val loss by epoch
5: 17815490.000 / 28034850.400
10: 15586813.000 / 24957267.400
15: 12661553.000 / 20892887.600
20: 9217945.000 / 15852467.300
25: 5814313.500 / 10771693.400
30: 3056490.500 / 6526089.650
35: 1239277.125 / 3585979.150
40: 448932.906 / 1871201.425
45: 124339.609 / 1047905.488
50: 50410.457 / 723090.938
55: 45586.352 / 614890.103
60: 53286.035 / 611296.053
65: 46315.574 / 671259.100
70: 29311.725 / 764797.541
75: 34775.996 / 902040.781
80: 38348.227 / 1033405.525
85: 29548.209 / 1147646.831
90: 24956.910 / 1159371.306
95: 21801.576 / 1157805.538
100: 24981.469 / 1174197.681
105: 17299.654 / 1176255.988
110: 18034.816 / 1122339.231
115: 13682.204 / 1110315.031
120: 12156.221 / 1095502.331
125: 12561.054 / 1066613.075
130: 15086.772 / 1081554.575
135: 10277.845 / 1091853.519
140: 13481.477 / 1110209.906
145: 14257.531 / 1128026.819
150: 16136.284 / 1143607.519
155: 17552.273 / 1187629.387
160: 15564.516 / 1166730.750
165: 12999.670 / 1178338.131
170: 5689.857 / 1243892.219
175: 10750.717 / 1256430.000
180: 8442.514 / 1314765.306
185: 10871.703 / 1303773.775
190: 15037.458 / 1283828.438
195: 22083.561 / 1316823.769
200: 13453.576 / 1361761.337
205: 14009.458 / 1404121.694
210: 13517.580 / 1409943.988
215: 9343.698 / 1471145.731
220: 19773.795 / 1484317.488
225: 13739.519 / 1547182.550
230: 9003.897 / 1505937.931
235: 15485.778 / 1501491.900
240: 11624.102 / 1556794.569
245: 9724.078 / 1588606.544
250: 13694.659 / 1574366.150
255: 18096.076 / 1566754.694
260: 13561.547 / 1632173.350
265: 10589.187 / 1639761.131
270: 13722.284 / 1632343.075
275: 12762.744 / 1686155.875
280: 12289.670 / 1701551.488
285: 14379.760 / 1723997.150
290: 16187.432 / 1746382.137
295: 11095.897 / 1690428.462
300: 10890.470 / 1778665.100
305: 12164.561 / 1799868.400
310: 9474.875 / 1791220.925
315: 12610.312 / 1819280.850
320: 15347.624 / 1804014.188
325: 10628.110 / 1866152.988
330: 16160.495 / 1828081.212
335: 14668.169 / 1920336.750
340: 11034.972 / 1873674.850
345: 10497.454 / 1937403.150
350: 14491.854 / 1937205.288
355: 8683.960 / 1930455.275
360: 14374.476 / 1935821.562
365: 14314.781 / 1983002.038
370: 13856.054 / 1976094.775
375: 12159.642 / 1971190.150
380: 9687.049 / 1991636.975
385: 11236.054 / 1993378.663
390: 9777.688 / 2030445.788
395: 11679.929 / 2055066.975
400: 12865.956 / 2087311.700
Rewirer cayley with seed 23
Using MSE Loss...
Minimum val loss at epoch 55: 267741.52265625
Final val loss at epoch 400: 1594118.325
Train / Val loss by epoch
5: 17182468.000 / 27154095.800
10: 14393166.000 / 23152494.800
15: 11088654.000 / 18281345.800
20: 7688345.500 / 13118350.700
25: 4505459.500 / 8370917.650
30: 2248146.500 / 4620182.050
35: 871437.125 / 2166675.375
40: 270400.000 / 939882.431
45: 83621.578 / 460804.159
50: 62388.398 / 293499.216
55: 56176.805 / 267741.523
60: 23103.893 / 290985.689
65: 28278.473 / 333823.448
70: 23484.395 / 358908.083
75: 28214.717 / 392569.703
80: 21769.113 / 411775.959
85: 26448.900 / 424190.222
90: 25231.080 / 447500.556
95: 17629.623 / 456324.847
100: 28603.090 / 460582.122
105: 12332.470 / 474435.084
110: 15192.391 / 533821.034
115: 13625.128 / 513735.875
120: 19666.725 / 536167.456
125: 18556.283 / 528399.425
130: 17675.799 / 560417.794
135: 13404.300 / 586590.463
140: 14688.618 / 588946.356
145: 17157.520 / 611277.578
150: 14889.264 / 614831.312
155: 23452.184 / 623831.419
160: 10126.900 / 642959.559
165: 17792.307 / 663449.206
170: 15377.741 / 708605.575
175: 12529.225 / 719839.800
180: 8913.057 / 730798.244
185: 13011.272 / 737462.831
190: 19295.400 / 778001.956
195: 11006.170 / 788797.306
200: 20612.020 / 791985.781
205: 11259.347 / 821910.206
210: 18533.629 / 884946.463
215: 14976.565 / 914829.831
220: 21719.162 / 931545.256
225: 14132.085 / 915815.150
230: 13870.415 / 942334.250
235: 16040.141 / 954070.294
240: 15644.234 / 980803.725
245: 13504.052 / 969433.519
250: 11687.331 / 1033187.963
255: 14642.302 / 1034556.188
260: 10285.203 / 1052061.694
265: 13225.854 / 1069987.669
270: 13656.006 / 1103618.281
275: 17226.656 / 1144877.250
280: 10938.690 / 1204123.106
285: 15114.189 / 1225875.288
290: 12473.958 / 1206755.837
295: 15824.935 / 1224949.131
300: 15023.126 / 1225073.387
305: 8686.107 / 1202209.988
310: 11335.568 / 1238634.837
315: 13464.034 / 1289303.369
320: 12765.536 / 1343493.988
325: 9573.641 / 1318440.156
330: 11467.188 / 1330573.006
335: 18512.385 / 1326899.163
340: 9480.616 / 1394629.938
345: 9306.218 / 1371930.294
350: 11376.373 / 1355255.663
355: 13060.541 / 1430234.375
360: 10119.390 / 1479812.044
365: 11497.752 / 1427995.025
370: 12331.620 / 1478526.575
375: 11578.179 / 1440672.512
380: 11200.567 / 1500483.125
385: 8426.743 / 1463967.306
390: 8397.515 / 1508900.975
395: 11139.476 / 1516547.212
400: 11441.350 / 1594118.325
{'cayley': [{'best': 98782.49765625, 'end': 745283.8375},
            {'best': 611296.053125, 'end': 2087311.7},
            {'best': 267741.52265625, 'end': 1594118.325}],
 'cayley_clusters': [{'best': 215800.32890625, 'end': 1317288.3375},
                     {'best': 634229.5375, 'end': 2232166.55},
                     {'best': 740418.871875, 'end': 2113584.7625}]}
Time: Fri Mar  1 14:53:15 GMT 2024
